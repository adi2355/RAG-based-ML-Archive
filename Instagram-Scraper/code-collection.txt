

================================================================================
File: app.py
================================================================================

"""
Web interface for the Instagram Knowledge Base
"""
import os
import re
import sqlite3
from functools import lru_cache
from flask import Flask, render_template, request, jsonify, g, send_from_directory, redirect, url_for

from config import (
    DB_PATH,
    WEB_PORT,
    DEBUG_MODE,
    DOWNLOAD_DIR,
    DATA_DIR
)

app = Flask(__name__)

def get_db():
    """Get database connection with row factory for easy access"""
    db = getattr(g, '_database', None)
    if db is None:
        db = g._database = sqlite3.connect(DB_PATH)
        db.row_factory = sqlite3.Row
    return db

@app.teardown_appcontext
def close_connection(exception):
    """Close database connection when app context ends"""
    db = getattr(g, '_database', None)
    if db is not None:
        db.close()

@app.route('/')
def index():
    """Home page with search interface"""
    return render_template('index.html')

@app.route('/search')
def search():
    """Search endpoint"""
    query = request.args.get('query', '')
    account_filter = request.args.get('account', '')
    tag_filter = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    # Calculate offset
    offset = (page - 1) * per_page
    
    db = get_db()
    cursor = db.cursor()
    
    # Get available accounts for filtering
    cursor.execute("SELECT DISTINCT account FROM videos ORDER BY account")
    accounts = [row['account'] for row in cursor.fetchall()]
    
    # Get popular tags for filtering
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 20
    ''')
    tags = [{'tag': row['tag'], 'count': row['count']} for row in cursor.fetchall()]
    
    results = []
    total_results = 0
    
    if query or account_filter or tag_filter:
        # Base query
        sql = '''
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, 
               v.timestamp, v.url, v.likes, v.comments
        FROM videos v
        '''
        
        params = []
        where_clauses = []
        
        # Add tag filter if specified
        if tag_filter:
            sql += "JOIN tags t ON v.id = t.video_id "
            where_clauses.append("t.tag = ?")
            params.append(tag_filter)
        
        # Add account filter if specified
        if account_filter:
            where_clauses.append("v.account = ?")
            params.append(account_filter)
        
        # Add search query if specified
        if query:
            sql += "JOIN videos_fts fts ON v.id = fts.docid "  # Changed from rowid to docid for FTS4
            where_clauses.append("videos_fts MATCH ?")
            params.append(query)
            
            # FTS4 doesn't support the snippet function, so we'll remove this line
            # and handle highlighting in application code later
        
        # Combine where clauses
        if where_clauses:
            sql += "WHERE " + " AND ".join(where_clauses)
        
        # Get total results count
        count_sql = f"SELECT COUNT(*) as count FROM ({sql})"
        cursor.execute(count_sql, params)
        total_results = cursor.fetchone()['count']
        
        # Add order and limit
        if query:
            # FTS4 doesn't have built-in rank function
            sql += " ORDER BY v.timestamp DESC"
        else:
            sql += " ORDER BY v.timestamp DESC"
            
        sql += " LIMIT ? OFFSET ?"
        params.extend([per_page, offset])
        
        # Execute query
        cursor.execute(sql, params)
        results = cursor.fetchall()
    
    # Format results for template
    formatted_results = []
    for row in results:
        # Get tags for this video
        cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (row['id'],))
        video_tags = [tag['tag'] for tag in cursor.fetchall()]
        
        # Format the date
        timestamp = row['timestamp'].split('T')[0] if row['timestamp'] else 'Unknown'
        
        # Get video path
        video_path = f"/video/{row['account']}/{row['shortcode']}"
        
        # Create our own snippet since snippet() isn't available in FTS4
        transcript = row['transcript'] or ""
        
        # Default snippet (first ~200 chars of transcript)
        if len(transcript) > 200:
            snippet = transcript[:200] + '...'
        else:
            snippet = transcript
        
        # If we have a search query, try to find a relevant part of the transcript
        if query and transcript:
            query_words = query.lower().split()
            transcript_lower = transcript.lower()
            
            # Find the first occurrence of any query word
            position = -1
            for word in query_words:
                pos = transcript_lower.find(word)
                if pos != -1:
                    position = pos
                    break
            
            # Extract a section around the match if found
            if position != -1:
                # Get 50 chars before and 150 after the match position
                start = max(0, position - 50)
                end = min(len(transcript), position + 150)
                
                # Get the relevant section
                context = transcript[start:end]
                
                # Add ellipsis if needed
                if start > 0:
                    context = '...' + context
                if end < len(transcript):
                    context += '...'
                
                snippet = context
        
        # Simple highlighting for matched terms
        if query and transcript:
            for word in query.lower().split():
                # Case-insensitive replace with HTML highlighting
                word_pattern = word.lower()
                start = 0
                while True:
                    start_pos = snippet.lower().find(word_pattern, start)
                    if start_pos == -1:
                        break
                    
                    end_pos = start_pos + len(word)
                    original_word = snippet[start_pos:end_pos]
                    snippet = snippet[:start_pos] + f'<mark>{original_word}</mark>' + snippet[end_pos:]
                    
                    # Move past this match
                    start = start_pos + len(f'<mark>{original_word}</mark>')
        
        formatted_results.append({
            'id': row['id'],
            'shortcode': row['shortcode'],
            'account': row['account'],
            'caption': row['caption'],
            'snippet': snippet,
            'timestamp': timestamp,
            'url': row['url'],
            'likes': row['likes'],
            'comments': row['comments'],
            'tags': video_tags,
            'video_path': video_path
        })
    
    # Calculate pagination info
    total_pages = (total_results + per_page - 1) // per_page if total_results > 0 else 1
    has_prev = page > 1
    has_next = page < total_pages
    
    return render_template(
        'search.html',
        query=query,
        account_filter=account_filter,
        tag_filter=tag_filter,
        results=formatted_results,
        accounts=accounts,
        tags=tags,
        total_results=total_results,
        page=page,
        per_page=per_page,
        total_pages=total_pages,
        has_prev=has_prev,
        has_next=has_next
    )

@app.route('/video/<account>/<shortcode>')
def video(account, shortcode):
    """Video detail page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get video details
    cursor.execute('''
    SELECT v.* FROM videos v
    WHERE v.account = ? AND v.shortcode = ?
    ''', (account, shortcode))
    video = cursor.fetchone()
    
    if not video:
        return "Video not found", 404
    
    # Get tags
    cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (video['id'],))
    tags = [tag['tag'] for tag in cursor.fetchall()]
    
    # Find video file
    video_filename = None
    account_dir = os.path.join(DOWNLOAD_DIR, account)
    if os.path.exists(account_dir):
        for filename in os.listdir(account_dir):
            if shortcode in filename and filename.endswith('.mp4'):
                video_filename = f"/media/{account}/{filename}"
                break
    
    return render_template(
        'video.html',
        video=video,
        tags=tags,
        video_filename=video_filename
    )

@app.route('/api/search')
def api_search():
    """API endpoint for search (for AJAX requests)"""
    query = request.args.get('query', '')
    account = request.args.get('account', '')
    tag = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    db = get_db()
    cursor = db.cursor()
    
    # Start building the query
    sql_select = """
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, v.summary,
               v.timestamp, v.url, v.likes, v.comments, v.word_count, v.duration_seconds
    """
    
    sql_from = " FROM videos v"
    sql_where = ""
    sql_order = ""
    params = []
    
    # Add search condition if query provided
    if query:
        sql_from += " JOIN videos_fts fts ON v.id = fts.docid"
        sql_where = " WHERE videos_fts MATCH ?"
        params.append(query)
        
        # Custom snippets for transcript and summary
        sql_select += """,
            (SELECT substr(v.transcript, 
                max(0, instr(lower(v.transcript), lower(?)) - 50), 
                150)) AS transcript_snippet,
            (SELECT substr(v.summary, 
                max(0, instr(lower(v.summary), lower(?)) - 25), 
                100)) AS summary_snippet
        """
        params.extend([query, query])  # Add query params for snippets
        
        # Enhanced ordering that prioritizes matches in summary, then caption
        sql_order = """
        ORDER BY
            CASE 
                WHEN fts.summary MATCH ? THEN 1
                WHEN fts.caption MATCH ? THEN 2
                WHEN fts.transcript MATCH ? THEN 3
                ELSE 4
            END
        """
        params.extend([query, query, query])
    
    # Add account filter if specified
    if account:
        if sql_where:
            sql_where += " AND v.account = ?"
        else:
            sql_where = " WHERE v.account = ?"
        params.append(account)
    
    # Add tag filter if specified
    if tag:
        sql_from += " JOIN tags t ON v.id = t.video_id"
        if sql_where:
            sql_where += " AND t.tag = ?"
        else:
            sql_where = " WHERE t.tag = ?"
        params.append(tag)
    
    # Count total results for pagination
    count_sql = f"SELECT COUNT(*) as total_count {sql_from}{sql_where}"
    cursor.execute(count_sql, params)
    total_count = cursor.fetchone()['total_count']
    
    # Calculate total pages
    total_pages = (total_count + per_page - 1) // per_page
    
    # Add default sorting if no query or specific ordering
    if not sql_order:
        sql_order = " ORDER BY v.timestamp DESC"
    
    # Add pagination
    sql_limit = " LIMIT ? OFFSET ?"
    
    # Complete SQL query
    full_sql = f"{sql_select}{sql_from}{sql_where}{sql_order}{sql_limit}"
    
    # Execute with pagination parameters
    cursor.execute(full_sql, params + [per_page, (page - 1) * per_page])
    results = cursor.fetchall()
    
    # Convert to list of dicts with highlighted snippets
    result_list = []
    for row in results:
        result_dict = {key: row[key] for key in row.keys()}
        
        # Add highlighted snippets if search was performed
        if query:
            # Highlight the query term in snippets
            if 'transcript_snippet' in result_dict and result_dict['transcript_snippet']:
                result_dict['transcript_snippet'] = highlight_term(
                    result_dict['transcript_snippet'], query
                )
            
            if 'summary_snippet' in result_dict and result_dict['summary_snippet']:
                result_dict['summary_snippet'] = highlight_term(
                    result_dict['summary_snippet'], query
                )
        
        result_list.append(result_dict)
    
    return jsonify({
        'results': result_list,
        'total': total_count,
        'page': page,
        'per_page': per_page,
        'total_pages': total_pages,
        'query': query,
        'account': account,
        'tag': tag
    })

def highlight_term(text, term):
    """Highlight search term in text using HTML"""
    if not text or not term:
        return text
    
    # Simple case-insensitive replace
    # For more complex highlighting, consider using regex
    term_lower = term.lower()
    text_lower = text.lower()
    
    result = ""
    last_pos = 0
    
    # Find all occurrences of the term
    pos = text_lower.find(term_lower)
    while pos != -1:
        # Add text before the term
        result += text[last_pos:pos]
        # Add the highlighted term
        result += f"<mark>{text[pos:pos+len(term)]}</mark>"
        # Move past this occurrence
        last_pos = pos + len(term)
        # Find next occurrence
        pos = text_lower.find(term_lower, last_pos)
    
    # Add any remaining text
    result += text[last_pos:]
    
    return result

@app.route('/media/<path:path>')
def media(path):
    """Serve media files"""
    return send_from_directory(DOWNLOAD_DIR, path)

# Routes for static templates
@app.route('/about')
def about():
    """About page"""
    return render_template('about.html')

@app.route('/stats')
def stats():
    """Statistics page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get general stats
    cursor.execute("SELECT COUNT(*) as total FROM videos")
    total_videos = cursor.fetchone()['total']
    
    # Get account stats
    cursor.execute('''
    SELECT account, COUNT(*) as count 
    FROM videos 
    GROUP BY account 
    ORDER BY count DESC
    ''')
    accounts = cursor.fetchall()
    
    # Get tag stats
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 50
    ''')
    tags = cursor.fetchall()
    
    # Get timeline stats
    cursor.execute('''
    SELECT substr(timestamp, 1, 7) as month, COUNT(*) as count 
    FROM videos 
    WHERE timestamp IS NOT NULL
    GROUP BY month 
    ORDER BY month
    ''')
    timeline = cursor.fetchall()
    
    return render_template(
        'stats.html',
        total_videos=total_videos,
        accounts=accounts,
        tags=tags,
        timeline=timeline
    )

# Caching utilities
@lru_cache(maxsize=100)
def get_video_by_shortcode(shortcode):
    """Get video details by shortcode with caching"""
    db = get_db()
    cursor = db.cursor()
    cursor.execute("SELECT * FROM videos WHERE shortcode = ?", [shortcode])
    return cursor.fetchone()

@lru_cache(maxsize=30)
def get_recent_videos(limit=10, account=None):
    """Get recent videos with caching"""
    db = get_db()
    cursor = db.cursor()
    
    if account:
        cursor.execute(
            "SELECT * FROM videos WHERE account = ? ORDER BY timestamp DESC LIMIT ?", 
            [account, limit]
        )
    else:
        cursor.execute(
            "SELECT * FROM videos ORDER BY timestamp DESC LIMIT ?", 
            [limit]
        )
    
    return cursor.fetchall()

@lru_cache(maxsize=20)
def get_video_statistics():
    """Get video statistics with caching"""
    db = get_db()
    cursor = db.cursor()
    
    # Get total videos
    cursor.execute("SELECT COUNT(*) as video_count FROM videos")
    total_videos = cursor.fetchone()['video_count']
    
    # Get videos per account
    cursor.execute(
        "SELECT account, COUNT(*) as count FROM videos GROUP BY account ORDER BY count DESC"
    )
    accounts = cursor.fetchall()
    
    # Get total duration (if available)
    cursor.execute(
        "SELECT SUM(duration_seconds) as total_duration FROM videos WHERE duration_seconds IS NOT NULL"
    )
    total_duration = cursor.fetchone()['total_duration'] or 0
    
    # Get total word count
    cursor.execute(
        "SELECT SUM(word_count) as total_words FROM videos WHERE word_count IS NOT NULL"
    )
    total_words = cursor.fetchone()['total_words'] or 0
    
    return {
        'total_videos': total_videos,
        'accounts': accounts,
        'total_duration_seconds': total_duration,
        'total_words': total_words
    }

# Clear caches when data changes
def clear_caches():
    """Clear all LRU caches"""
    get_video_by_shortcode.cache_clear()
    get_recent_videos.cache_clear()
    get_video_statistics.cache_clear()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=WEB_PORT, debug=DEBUG_MODE) 

================================================================================
File: arxiv_collector.py
================================================================================

"""
Module for collecting and processing AI/ML research papers from ArXiv
"""
import os
import json
import time
import logging
import sqlite3
import requests
import feedparser
from datetime import datetime, timedelta
import PyPDF2
import io
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'arxiv_collector.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('arxiv_collector')

def setup_directories():
    """Create necessary directories for storing paper data"""
    papers_dir = os.path.join(config.DATA_DIR, "papers")
    papers_pdf_dir = os.path.join(papers_dir, "pdfs")
    papers_text_dir = os.path.join(papers_dir, "text")
    
    os.makedirs(papers_dir, exist_ok=True)
    os.makedirs(papers_pdf_dir, exist_ok=True)
    os.makedirs(papers_text_dir, exist_ok=True)
    
    return papers_dir, papers_pdf_dir, papers_text_dir

def extract_text_from_pdf(pdf_path):
    """Extract text content from a PDF file"""
    try:
        text = ""
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            num_pages = len(reader.pages)
            
            # Extract text from each page
            for page_num in range(num_pages):
                page = reader.pages[page_num]
                text += page.extract_text() + "\n\n"
                
            logger.info(f"Extracted text from {pdf_path} ({num_pages} pages)")
        return text
    except Exception as e:
        logger.error(f"Error extracting text from PDF {pdf_path}: {str(e)}")
        return None

def parse_sections(text):
    """Attempt to parse sections from extracted PDF text"""
    sections = {
        "abstract": "",
        "introduction": "",
        "methodology": "",
        "results": "",
        "conclusion": "",
        "references": ""
    }
    
    if not text:
        return sections
        
    # Simple heuristic section detection
    lines = text.split('\n')
    current_section = "abstract"
    
    for line in lines:
        line = line.strip()
        line_lower = line.lower()
        
        # Check for section headers
        if "abstract" in line_lower and len(line) < 30:
            current_section = "abstract"
            continue
        elif any(x in line_lower for x in ["introduction", "background"]) and len(line) < 30:
            current_section = "introduction"
            continue
        elif any(x in line_lower for x in ["method", "approach", "model", "implementation"]) and len(line) < 30:
            current_section = "methodology"
            continue
        elif any(x in line_lower for x in ["result", "evaluation", "experiment", "performance"]) and len(line) < 30:
            current_section = "results"
            continue
        elif any(x in line_lower for x in ["conclusion", "discussion", "future work"]) and len(line) < 30:
            current_section = "conclusion"
            continue
        elif any(x in line_lower for x in ["reference", "bibliography"]) and len(line) < 30:
            current_section = "references"
            continue
            
        # Add text to current section
        if line and current_section in sections:
            sections[current_section] += line + "\n"
    
    # Clean up sections
    for section in sections:
        if len(sections[section]) > 100000:  # Limit section size
            sections[section] = sections[section][:100000] + "... [truncated]"
    
    return sections

def collect_papers(max_papers=None, force_update=False):
    """
    Collect papers from ArXiv based on configured topics
    
    Args:
        max_papers: Maximum number of papers to collect (None for no limit)
        force_update: Whether to force update of existing papers
        
    Returns:
        Number of new papers added
    """
    papers_dir, papers_pdf_dir, papers_text_dir = setup_directories()
    
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    
    # Get source type ID for 'research_paper'
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM source_types WHERE name = 'research_paper'")
    result = cursor.fetchone()
    if result:
        source_type_id = result[0]
    else:
        logger.error("Source type 'research_paper' not found in database")
        conn.close()
        return 0
    
    papers_processed = 0
    papers_added = 0
    paper_topics = config.RESEARCH_PAPER_CONFIG.get('topics', [
        "large language models",
        "diffusion models",
        "transformers", 
        "generative ai",
        "reinforcement learning",
        "computer vision"
    ])
    
    max_papers_per_topic = config.RESEARCH_PAPER_CONFIG.get('max_papers_per_topic', 5)
    max_age_days = config.RESEARCH_PAPER_CONFIG.get('max_age_days', 60)
    
    # Adjust max_papers_per_topic if max_papers is specified
    if max_papers and len(paper_topics) > 0:
        max_papers_per_topic = min(max_papers_per_topic, max_papers // len(paper_topics) + 1)
    
    # Process each configured topic
    for topic in paper_topics:
        logger.info(f"Collecting papers for topic: {topic}")
        
        # Construct ArXiv API query
        query = f'all:"{topic}" AND (cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV)'
        url = f"http://export.arxiv.org/api/query?search_query={query.replace(' ', '+')}&sortBy=submittedDate&sortOrder=descending&max_results={max_papers_per_topic}"
        
        try:
            # Get papers from ArXiv
            response = feedparser.parse(url)
            
            if not response.entries:
                logger.warning(f"No papers found for topic: {topic}")
                continue
                
            logger.info(f"Found {len(response.entries)} papers for topic: {topic}")
            
            # Process each paper
            for entry in response.entries:
                # Debug to see the structure of the entry
                entry_keys = entry.keys()
                logger.debug(f"Available keys in entry: {entry_keys}")
                
                try:
                    # Safe extraction of paper ID
                    paper_id = entry.id.split('/abs/')[-1] if hasattr(entry, 'id') else f"unknown_{time.time()}"
                    papers_processed += 1
                    
                    if max_papers and papers_added >= max_papers:
                        logger.info(f"Reached maximum paper limit: {max_papers}")
                        break
                    
                    # Check if paper already exists
                    cursor.execute(
                        "SELECT id, last_crawled FROM research_papers WHERE doi = ?", 
                        (paper_id,)
                    )
                    existing = cursor.fetchone()
                    
                    # Skip if paper exists and was recently updated (unless force_update is True)
                    if existing and not force_update:
                        if existing[1]:  # Check if last_crawled is not None
                            try:
                                last_updated = datetime.fromisoformat(existing[1].replace('Z', '+00:00'))
                                if (datetime.now() - last_updated).days < 30:
                                    logger.debug(f"Skipping recent paper: {paper_id}")
                                    continue
                            except (ValueError, AttributeError):
                                # If date parsing fails, process the paper anyway
                                pass
                    
                    # Safe extraction of publication date
                    published = datetime.now()  # Default to current time
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_date = entry.published_parsed
                            published = datetime(
                                year=published_date[0],
                                month=published_date[1],
                                day=published_date[2],
                                hour=published_date[3],
                                minute=published_date[4],
                                second=published_date[5]
                            )
                        except (IndexError, TypeError, ValueError) as e:
                            logger.warning(f"Error parsing published date for {paper_id}: {str(e)}")
                    
                    if not force_update and (datetime.now() - published).days > max_age_days:
                        logger.debug(f"Skipping old paper: {paper_id} ({published})")
                        continue
                    
                    # Safe extraction of paper details
                    title = entry.get('title', f"Unknown Paper {paper_id}")
                    
                    # Safe extraction of authors
                    authors = ""
                    if hasattr(entry, 'authors'):
                        try:
                            authors = ", ".join(author.get('name', '') for author in entry.authors)
                        except (AttributeError, TypeError) as e:
                            logger.warning(f"Error extracting authors for {paper_id}: {str(e)}")
                    
                    # Safe extraction of summary
                    summary = entry.get('summary', "No abstract available")
                    
                    # Get PDF link
                    pdf_link = None
                    if hasattr(entry, 'links'):
                        for link in entry.links:
                            if hasattr(link, 'rel') and hasattr(link, 'type'):
                                if link.rel == 'alternate' and link.type == 'application/pdf':
                                    pdf_link = link.href
                                    break
                            if hasattr(link, 'title') and link.title == 'pdf':
                                pdf_link = link.href
                                break
                    
                    # Extract article URL
                    article_url = None
                    if hasattr(entry, 'links'):
                        for link in entry.links:
                            if hasattr(link, 'rel') and hasattr(link, 'type'):
                                if link.rel == 'alternate' and link.type == 'text/html':
                                    article_url = link.href
                                    break
                    
                    if not article_url:
                        article_url = f"https://arxiv.org/abs/{paper_id}"
                    
                    # Extract PDF content if available
                    sections = {"abstract": summary}
                    pdf_path = None
                    extracted_text = None
                    
                    if pdf_link:
                        try:
                            # Save PDF locally
                            pdf_path = os.path.join(papers_pdf_dir, f"{paper_id.replace('/', '_')}.pdf")
                            if not os.path.exists(pdf_path) or force_update:
                                logger.info(f"Downloading PDF for {paper_id} from {pdf_link}")
                                
                                # Add a delay to be respectful of the API
                                time.sleep(2)
                                
                                pdf_response = requests.get(pdf_link, timeout=30)
                                with open(pdf_path, 'wb') as f:
                                    f.write(pdf_response.content)
                                logger.info(f"Downloaded PDF for {paper_id}")
                            
                            # Extract text from PDF
                            extracted_text = extract_text_from_pdf(pdf_path)
                            if extracted_text:
                                # Save extracted text
                                text_path = os.path.join(papers_text_dir, f"{paper_id.replace('/', '_')}.txt")
                                with open(text_path, 'w', encoding='utf-8') as f:
                                    f.write(extracted_text)
                                
                                # Parse sections
                                sections = parse_sections(extracted_text)
                                if not sections["abstract"].strip():
                                    sections["abstract"] = summary
                            
                        except Exception as e:
                            logger.error(f"Error processing PDF for {paper_id}: {str(e)}")
                    
                    # Calculate publication year
                    publication_year = published.year
                    
                    # Prepare data for database
                    if existing:
                        # Update existing paper record
                        cursor.execute("""
                            UPDATE research_papers
                            SET title = ?, authors = ?, abstract = ?, 
                                url = ?, pdf_path = ?, content = ?, last_crawled = ?
                            WHERE id = ?
                        """, (
                            title,
                            authors,
                            sections["abstract"],
                            article_url,
                            pdf_path if pdf_path else None,
                            extracted_text,
                            datetime.now().isoformat(),
                            existing[0]
                        ))
                        
                        # Update ai_content record
                        cursor.execute("""
                            UPDATE ai_content
                            SET title = ?, description = ?, content = ?,
                                url = ?, date_collected = ?, metadata = ?
                            WHERE source_type_id = ? AND source_id = ?
                        """, (
                            title,
                            sections["abstract"],
                            extracted_text if extracted_text else sections["abstract"],
                            article_url,
                            datetime.now().isoformat(),
                            json.dumps({
                                "authors": authors,
                                "published_date": published.isoformat(),
                                "year": publication_year,
                                "sections": sections
                            }),
                            source_type_id,
                            str(existing[0])
                        ))
                        
                        logger.info(f"Updated paper: {title}")
                    else:
                        # Insert into research_papers
                        cursor.execute("""
                            INSERT INTO research_papers (
                                title, authors, abstract, publication, year,
                                url, doi, pdf_path, content, last_crawled
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            title,
                            authors,
                            sections["abstract"],
                            "arXiv",  # Publication name
                            publication_year,
                            article_url,
                            paper_id,  # Using arXiv ID as DOI
                            pdf_path if pdf_path else None,
                            extracted_text,
                            datetime.now().isoformat()
                        ))
                        
                        # Get the inserted ID
                        paper_db_id = cursor.lastrowid
                        
                        # Insert into ai_content
                        cursor.execute("""
                            INSERT INTO ai_content (
                                source_type_id, source_id, title, description, content,
                                url, date_created, date_collected, metadata, is_indexed
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            source_type_id,
                            str(paper_db_id),
                            title,
                            sections["abstract"],
                            extracted_text if extracted_text else sections["abstract"],
                            article_url,
                            published.isoformat(),
                            datetime.now().isoformat(),
                            json.dumps({
                                "authors": authors,
                                "published_date": published.isoformat(),
                                "year": publication_year,
                                "sections": sections
                            }),
                            0  # Not indexed yet
                        ))
                        
                        papers_added += 1
                        logger.info(f"Added new paper: {title}")
                    
                    conn.commit()
                
                except Exception as e:
                    logger.error(f"Error processing paper entry: {str(e)}")
                
                # Add a delay to be respectful of the API
                time.sleep(3)
                
        except Exception as e:
            logger.error(f"Error collecting papers for topic {topic}: {str(e)}")
            continue
    
    conn.close()
    logger.info(f"Paper collection complete. Processed {papers_processed} papers, added {papers_added} new papers.")
    return papers_added

if __name__ == "__main__":
    collect_papers() 

================================================================================
File: chunking.py
================================================================================

"""
Text chunking module for breaking content into appropriate-sized pieces for embedding

This module provides functions to split content into overlapping chunks for better
context preservation and more effective embeddings for vector search.
"""
import logging
import re
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('chunking')

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """
    Split text into overlapping chunks for better context preservation
    
    Args:
        text: The text to split into chunks
        chunk_size: Maximum size in characters for each chunk
        overlap: Number of characters to overlap between chunks
        
    Returns:
        List of text chunks
    """
    if not text or len(text) <= chunk_size:
        return [text] if text else []
        
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        # Calculate end position with potential overlap
        end = min(start + chunk_size, text_length)
        
        # If we're not at the very end, try to find a good breaking point
        if end < text_length:
            # Look for natural text boundaries in order of preference
            # Check for double newlines (paragraph breaks)
            paragraph_break = text.rfind("\n\n", start, end)
            if paragraph_break != -1 and paragraph_break > start + chunk_size // 2:
                end = paragraph_break + 2
            else:
                # Check for single newlines
                newline = text.rfind("\n", start, end)
                if newline != -1 and newline > start + chunk_size // 2:
                    end = newline + 1
                else:
                    # Check for sentence boundaries (period followed by space)
                    sentence = text.rfind(". ", start, end)
                    if sentence != -1 and sentence > start + chunk_size // 2:
                        end = sentence + 2
                    else:
                        # Last resort: check for any space
                        space = text.rfind(" ", start, end)
                        if space != -1 and space > start + chunk_size // 2:
                            end = space + 1
        
        # Extract the chunk and add to results
        chunk = text[start:end].strip()
        if chunk:  # Only add non-empty chunks
            chunks.append(chunk)
        
        # Move start position for next chunk, ensuring overlap
        start = max(start + 1, end - overlap)
    
    logger.debug(f"Split text into {len(chunks)} chunks")
    return chunks

def chunk_with_metadata(content: Dict[str, Any], 
                        chunk_size: int = 1000, 
                        overlap: int = 200) -> List[Dict[str, Any]]:
    """
    Split content with metadata into chunks, preserving metadata in each chunk
    
    Args:
        content: Dictionary with text and metadata
        chunk_size: Maximum size in characters for each chunk
        overlap: Number of characters to overlap between chunks
        
    Returns:
        List of dictionaries, each with a text chunk and the original metadata
    """
    if not content or 'text' not in content:
        return []
    
    # Extract text and metadata
    text = content['text']
    metadata = {k: v for k, v in content.items() if k != 'text'}
    
    # Chunk the text
    text_chunks = chunk_text(text, chunk_size, overlap)
    
    # Create result with metadata preserved
    result = []
    for i, chunk in enumerate(text_chunks):
        chunk_data = metadata.copy()
        chunk_data['text'] = chunk
        chunk_data['chunk_index'] = i
        chunk_data['total_chunks'] = len(text_chunks)
        result.append(chunk_data)
    
    return result

def prepare_content_for_embedding(title: str, description: str, content: str) -> str:
    """
    Prepare content by combining title, description, and content with appropriate formatting
    
    Args:
        title: Content title
        description: Content description or summary
        content: Main content text
        
    Returns:
        Formatted text ready for chunking and embedding
    """
    parts = []
    
    if title:
        parts.append(f"Title: {title.strip()}")
    
    if description:
        parts.append(f"Description: {description.strip()}")
    
    if content:
        parts.append(f"Content: {content.strip()}")
    
    return "\n\n".join(parts)

if __name__ == "__main__":
    # Test the chunking functionality
    test_text = """
    Title: Understanding Vector Embeddings
    
    This is a test document that will be split into chunks. Vector embeddings are numerical representations 
    of concepts converted into a series of numbers so that computers can understand how semantically close 
    two concepts are to each other.
    
    The main idea behind vector embeddings is to represent data in a way that captures semantic similarity
    through spatial relationships in a high-dimensional vector space.
    
    When we perform operations like measuring the cosine similarity between two vectors, we're essentially
    quantifying how similar the underlying concepts are.
    
    This has applications in search, recommendations, classification, and many other machine learning tasks.
    """
    
    chunks = chunk_text(test_text, chunk_size=200, overlap=50)
    
    print(f"Split text into {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks):
        print(f"\nChunk {i+1}:")
        print("-" * 40)
        print(chunk)
        print("-" * 40) 

================================================================================
File: concept_extractor.py
================================================================================

"""
Concept extractor module for identifying and extracting AI concepts from content

This module uses anthropic.claude to analyze content from various sources
(research papers, GitHub repositories, Instagram videos) and extract
AI/ML concepts, creating a structured knowledge graph of concepts.
"""
import os
import json
import time
import logging
import sqlite3
from datetime import datetime
import anthropic
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'concept_extractor.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('concept_extractor')

# Set up the Anthropic Claude client
client = anthropic.Anthropic(api_key=config.ANTHROPIC_API_KEY)

def extract_concepts_from_text(text, content_type, title="", context=""):
    """
    Extract AI concepts from text content using Claude
    
    Args:
        text (str): The text content to analyze
        content_type (str): The type of content (research_paper, github, instagram)
        title (str): Title of the content
        context (str): Additional context or metadata
        
    Returns:
        dict: Dictionary of extracted concepts
    """
    if not text or len(text.strip()) < 50:
        logger.warning(f"Text too short for concept extraction: {text[:50]}...")
        return {"concepts": [], "relationships": []}
    
    # Truncate text if it's too long
    max_length = 20000  # Claude can handle larger texts, but we'll keep it reasonable
    if len(text) > max_length:
        logger.info(f"Truncating text from {len(text)} to {max_length} characters")
        text = text[:max_length] + "..."
    
    # Prepare prompt based on content type
    type_context = {
        "research_paper": "This is content from a research paper on AI/ML.",
        "github": "This is content from a GitHub repository related to AI/ML.",
        "instagram": "This is a transcript or summary from an Instagram video about AI/ML."
    }
    
    content_context = type_context.get(content_type, "This is AI/ML related content.")
    
    prompt = f"""
    {content_context}
    Title: {title}
    Context: {context}
    
    I need you to analyze the following content and extract key AI/ML concepts. For each concept:
    1. Provide a short description
    2. Identify related concepts
    3. Categorize it (e.g., model architecture, training technique, dataset, evaluation metric)
    4. Assess its importance in the content (high/medium/low)
    
    Your response should be a well-structured JSON with the following format:
    {{
        "concepts": [
            {{
                "name": "concept name",
                "description": "brief description",
                "category": "category",
                "importance": "high/medium/low",
                "related_concepts": ["related concept 1", "related concept 2"]
            }}
        ],
        "relationships": [
            {{
                "source": "concept1",
                "target": "concept2",
                "relationship_type": "uses/is_part_of/improves/etc"
            }}
        ]
    }}
    
    Focus on technical AI/ML concepts only. Identify between 5-15 concepts depending on content length and density.
    
    Content to analyze:
    {text}
    """
    
    try:
        # Call Claude API
        message = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=4000,
            temperature=0,
            system="You are an AI expert who specializes in extracting and organizing AI/ML concepts from technical content.",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # Extract JSON from response
        response_text = message.content[0].text
        
        # Try to parse the JSON
        try:
            # Find JSON in the response (might be wrapped in markdown code blocks)
            json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
            import re
            json_match = re.search(json_pattern, response_text)
            
            if json_match:
                json_str = json_match.group(1)
                concepts_data = json.loads(json_str)
            else:
                # Try parsing the whole response as JSON
                concepts_data = json.loads(response_text)
                
            # Validate the structure
            if not isinstance(concepts_data, dict) or "concepts" not in concepts_data:
                raise ValueError("Invalid JSON structure")
                
            return concepts_data
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse JSON from Claude: {str(e)}")
            logger.debug(f"Claude response: {response_text}")
            return {"concepts": [], "relationships": []}
    
    except Exception as e:
        logger.error(f"Error calling Claude API: {str(e)}")
        return {"concepts": [], "relationships": []}

def store_concepts(content_id, source_type_id, concepts_data):
    """
    Store extracted concepts in the database
    
    Args:
        content_id (int): ID of the content in ai_content table
        source_type_id (int): Source type ID
        concepts_data (dict): Dictionary containing concepts and relationships
        
    Returns:
        bool: Success or failure
    """
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Check if concepts table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='concepts'")
        has_concepts_table = cursor.fetchone() is not None
        
        # Create tables if they don't exist
        if not has_concepts_table:
            # Create concepts table
            cursor.execute("""
            CREATE TABLE concepts (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                category TEXT,
                UNIQUE(name)
            )
            """)
            
            # Create content_concepts table for many-to-many relationships
            cursor.execute("""
            CREATE TABLE content_concepts (
                id INTEGER PRIMARY KEY,
                content_id INTEGER,
                concept_id INTEGER,
                importance TEXT,
                metadata TEXT,
                UNIQUE(content_id, concept_id),
                FOREIGN KEY(content_id) REFERENCES ai_content(id),
                FOREIGN KEY(concept_id) REFERENCES concepts(id)
            )
            """)
            
            # Create concept relationships table
            cursor.execute("""
            CREATE TABLE concept_relationships (
                id INTEGER PRIMARY KEY,
                source_concept_id INTEGER,
                target_concept_id INTEGER,
                relationship_type TEXT,
                UNIQUE(source_concept_id, target_concept_id, relationship_type),
                FOREIGN KEY(source_concept_id) REFERENCES concepts(id),
                FOREIGN KEY(target_concept_id) REFERENCES concepts(id)
            )
            """)
            
            logger.info("Created concepts tables")
        
        # Store each concept
        concept_ids = {}
        for concept in concepts_data.get("concepts", []):
            name = concept.get("name", "").strip()
            if not name:
                continue
                
            # Check if concept already exists
            cursor.execute("SELECT id FROM concepts WHERE name = ?", (name,))
            result = cursor.fetchone()
            
            if result:
                concept_id = result[0]
            else:
                # Insert new concept
                cursor.execute("""
                INSERT INTO concepts (name, description, category) 
                VALUES (?, ?, ?)
                """, (
                    name,
                    concept.get("description", ""),
                    concept.get("category", "")
                ))
                concept_id = cursor.lastrowid
            
            concept_ids[name] = concept_id
            
            # Link concept to content
            try:
                cursor.execute("""
                INSERT INTO content_concepts (content_id, concept_id, importance, metadata)
                VALUES (?, ?, ?, ?)
                """, (
                    content_id,
                    concept_id,
                    concept.get("importance", "medium"),
                    json.dumps({
                        "related_concepts": concept.get("related_concepts", [])
                    })
                ))
            except sqlite3.IntegrityError:
                # Update existing link
                cursor.execute("""
                UPDATE content_concepts 
                SET importance = ?, metadata = ?
                WHERE content_id = ? AND concept_id = ?
                """, (
                    concept.get("importance", "medium"),
                    json.dumps({
                        "related_concepts": concept.get("related_concepts", [])
                    }),
                    content_id,
                    concept_id
                ))
        
        # Store relationships
        for relationship in concepts_data.get("relationships", []):
            source = relationship.get("source", "").strip()
            target = relationship.get("target", "").strip()
            rel_type = relationship.get("relationship_type", "related_to").strip()
            
            if not source or not target or source not in concept_ids or target not in concept_ids:
                continue
                
            source_id = concept_ids[source]
            target_id = concept_ids[target]
            
            try:
                cursor.execute("""
                INSERT INTO concept_relationships (source_concept_id, target_concept_id, relationship_type)
                VALUES (?, ?, ?)
                """, (source_id, target_id, rel_type))
            except sqlite3.IntegrityError:
                # Relationship already exists
                pass
        
        # Update ai_content to mark as processed for concepts
        cursor.execute("""
        UPDATE ai_content 
        SET metadata = json.set(metadata, '$.concepts_extracted', 1),
            date_indexed = ?
        WHERE id = ?
        """, (datetime.now().isoformat(), content_id))
        
        conn.commit()
        return True
        
    except Exception as e:
        logger.error(f"Error storing concepts: {str(e)}")
        conn.rollback()
        return False
        
    finally:
        conn.close()

def process_unprocessed_content(limit=5, source_type=None):
    """
    Process content that hasn't had concepts extracted yet
    
    Args:
        limit (int): Maximum number of items to process
        source_type (str, optional): If provided, only process this type of content
        
    Returns:
        int: Number of items processed
    """
    conn = sqlite3.connect(config.DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Get source type IDs
        source_type_map = {}
        cursor.execute("SELECT id, name FROM source_types")
        for row in cursor.fetchall():
            source_type_map[row[1]] = row[0]
        
        # Build query
        query = """
        SELECT c.id, c.source_type_id, c.title, c.content, c.description, c.metadata
        FROM ai_content c
        WHERE 
            (json_extract(c.metadata, '$.concepts_extracted') IS NULL OR 
             json_extract(c.metadata, '$.concepts_extracted') = 0)
            AND c.content IS NOT NULL AND length(c.content) > 100
        """
        
        params = []
        if source_type and source_type in source_type_map:
            query += " AND c.source_type_id = ?"
            params.append(source_type_map[source_type])
        
        query += " ORDER BY c.date_collected DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        items = cursor.fetchall()
        
        processed_count = 0
        for item in items:
            content_id, source_type_id, title, content, description, metadata_json = item
            
            # Determine source type name
            source_type_name = next((name for name, id_val in source_type_map.items() if id_val == source_type_id), "unknown")
            
            # Parse metadata
            try:
                metadata = json.loads(metadata_json) if metadata_json else {}
            except:
                metadata = {}
            
            # Different handling based on content type
            context = ""
            if source_type_name == "github":
                context = f"GitHub repository: {metadata.get('full_name', '')}"
                # Use description for context if content is README
                if description:
                    context += f"\nDescription: {description}"
                
            elif source_type_name == "research_paper":
                authors = metadata.get("authors", "")
                year = metadata.get("year", "")
                context = f"Authors: {authors}\nYear: {year}"
                
                # Use abstract for context if we're processing full text
                if description and len(content) > len(description)*3:
                    context += f"\nAbstract: {description[:500]}..."
            
            # Extract concepts
            logger.info(f"Extracting concepts from {source_type_name} content: {title}")
            concepts_data = extract_concepts_from_text(
                content,
                source_type_name,
                title=title,
                context=context
            )
            
            # Store concepts
            if concepts_data and concepts_data.get("concepts"):
                logger.info(f"Found {len(concepts_data['concepts'])} concepts in {title}")
                if store_concepts(content_id, source_type_id, concepts_data):
                    processed_count += 1
            else:
                logger.warning(f"No concepts found in {title}")
                # Mark as processed even if no concepts found
                cursor.execute("""
                UPDATE ai_content 
                SET metadata = json.set(metadata, '$.concepts_extracted', 1),
                    date_indexed = ?
                WHERE id = ?
                """, (datetime.now().isoformat(), content_id))
                conn.commit()
                processed_count += 1
                
            # Sleep to avoid rate limiting
            time.sleep(2)
        
        return processed_count
    
    except Exception as e:
        logger.error(f"Error processing content for concept extraction: {str(e)}")
        return 0
    
    finally:
        conn.close()

if __name__ == "__main__":
    # Process some content from each source type
    for source in ["research_paper", "github", "instagram"]:
        count = process_unprocessed_content(limit=3, source_type=source)
        logger.info(f"Processed {count} items from {source}") 

================================================================================
File: config.py
================================================================================

"""
Configuration file for Instagram Knowledge Base
"""
import os

# Content sources configuration
# Currently supports Instagram, with framework for adding more sources
CONTENT_SOURCES = {
    'instagram': {
        'enabled': True,
        'accounts': [
            'example_account1',
            'example_account2',
            'example_account3',
        ]
    },
    'github': {
        'enabled': True,
        'rate_limit': 5000,  # GitHub API has a rate limit of 5000 requests per hour for authenticated requests
        'max_repos_per_run': 10,
        'api_token': '',  # Add your GitHub API token here to increase rate limits
        'topics': [
            'machine-learning',
            'artificial-intelligence',
            'deep-learning',
            'data-science',
            'nlp',
            'computer-vision',
            'reinforcement-learning'
        ],
        'repo_stars_minimum': 1000,  # Minimum number of stars for a repository to be considered
        'cache_ttl_hours': 24  # How long to cache GitHub API results in hours
    },
    'research_papers': {
        'enabled': True,
        'max_papers_per_run': 10,
        'sources': ['arxiv']  # Currently only supports ArXiv
    }
}

# Instagram accounts list (for backward compatibility)
INSTAGRAM_ACCOUNTS = [account for account in CONTENT_SOURCES['instagram']['accounts']]

# Instagram credentials (only needed for private accounts)
# IMPORTANT: Add your Instagram credentials here to avoid 401 Unauthorized errors
# You can either set environment variables or directly add your credentials below:
# INSTAGRAM_USERNAME = "your_instagram_username"
# INSTAGRAM_PASSWORD = "your_instagram_password"
INSTAGRAM_USERNAME = os.getenv("INSTAGRAM_USERNAME", "")
INSTAGRAM_PASSWORD = os.getenv("INSTAGRAM_PASSWORD", "")

# Instagram credentials in a dictionary format
INSTAGRAM_CREDENTIALS = {
    'username': INSTAGRAM_USERNAME,
    'password': INSTAGRAM_PASSWORD
}

# Multiple Instagram accounts for rotation (to reduce rate limiting)
# If using account rotation, populate this list with your accounts
INSTAGRAM_ACCOUNT_ROTATION = {
    'enabled': False,  # Disabled due to 2FA requirements
    'accounts': [
        {'username': 'account1', 'password': 'password1', 'last_used': 0, 'failed_attempts': 0},
        {'username': 'account2', 'password': 'password2', 'last_used': 0, 'failed_attempts': 0}
    ],
    'failure_threshold': 3,  # Number of consecutive failures before an account is marked for cooldown
    'cooldown_hours': 24,    # Hours to wait before trying a failed account again
    'min_rotation_interval': 10  # Minimum time in minutes between account rotations
}

# Proxy configuration
# Add your proxy servers here to rotate IPs and reduce rate limiting
PROXY_SERVERS = [
    "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335",
    # Add more proxies as needed
]

# Configure if you want to use a specific country for proxies (US, UK, etc.)
PROXY_COUNTRY = "us"  # Change as needed, or set to None for random

# Proxy configuration in dict format
PROXY_CONFIG = {
    'enabled': True if PROXY_SERVERS else False,
    'type': 'rotating',  # 'rotating' or 'fixed'
    'test_url': 'https://geo.brdtest.com',
    'protocol': 'http',  # 'http', 'https', 'socks4', 'socks5'
    'host': 'brd.superproxy.io',
    'port': 33335,
    'username': 'brd-customer-hl_c7bff232-zone-residential_proxy1-country-us',
    'password': 'w46vs0z46xmc',
    'timeout': 10  # Seconds
}

# Claude API key for summarization
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# Directory settings
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "downloads")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
TRANSCRIPT_DIR = os.path.join(DATA_DIR, "transcripts")

# Whisper model size: tiny, base, small, medium, large
WHISPER_MODEL = "base"

# Database settings
DB_PATH = os.path.join(DATA_DIR, "knowledge_base.db")

# Web interface settings
WEB_PORT = 5001
DEBUG_MODE = True

# Rate limiting settings (to avoid IP blocks)
DOWNLOAD_DELAY = 10  # seconds between downloads (increased from 5 to reduce rate limiting)
MAX_DOWNLOADS_PER_RUN = 3
ACCOUNT_COOLDOWN_MINUTES = 60  # How long to wait before using an account again after a failure
PROXY_COOLDOWN_MINUTES = 30  # How long to wait before using a proxy again after a failure
RATE_LIMIT_WAIT = 3600  # Seconds to wait after hitting a rate limit

# Mistral transcription settings
MISTRAL_CONFIG = {
    'model_size': '7B',  # 7B, 16K
    'device': 'cuda',    # 'cuda', 'cpu'
    'max_duration': 600  # Maximum duration in seconds
}

# API Keys
API_KEYS = {
    'anthropic': 'your_anthropic_api_key',
    'openai': 'your_openai_api_key',
    'eleven_labs': 'your_eleven_labs_api_key'
}

# Web interface settings
WEB_CONFIG = {
    'host': '0.0.0.0',
    'port': 5000,
    'debug': True
}

# Rate limiting settings
RATE_LIMIT = {
    'enabled': True,
    'max_requests_per_minute': 20,
    'retry_after': 60  # seconds
}

# GitHub specific settings
GITHUB_CONFIG = {
    'target_repos': [
        # Foundational ML/DL libraries
        'tensorflow/tensorflow',
        'pytorch/pytorch',
        'scikit-learn/scikit-learn',
        'huggingface/transformers',
        'keras-team/keras',
        
        # LLM & Generative AI projects
        'openai/whisper',
        'facebookresearch/llama',
        'anthropics/claude-api',
        'google/gemma',
        'mistralai/mistral-src',
        
        # Training & Infrastructure
        'ray-project/ray',
        'microsoft/DeepSpeed',
        'google/jax',
        
        # Research implementations
        'facebookresearch/fairseq',
        'openai/CLIP',
        'LAION-AI/Open-Assistant',
        
        # Learning resources
        'datawhalechina/pumpkin-book',
        'afshinea/stanford-cs-229-machine-learning',
        'microsoft/ML-For-Beginners'
    ],
    'readme_max_length': 100000,  # Maximum length of README to store
    'metadata_fields': [
        'id', 'name', 'full_name', 'description', 'html_url', 
        'stargazers_count', 'watchers_count', 'forks_count', 
        'language', 'pushed_at', 'created_at', 'updated_at', 'topics'
    ],
    'update_frequency_days': 7,  # How often to update GitHub repository data
} 

# Research paper collection settings
RESEARCH_PAPER_CONFIG = {
    'topics': [
        "large language models",
        "diffusion models",
        "transformers",
        "generative ai",
        "ai alignment",
        "few-shot learning",
        "multimodal ai",
        "reinforcement learning",
        "computer vision",
        "natural language processing"
    ],
    'max_papers_per_topic': 5,
    'max_age_days': 60,  # Only collect papers published within this timeframe
    'update_frequency_days': 7,  # How often to update paper data
    'pdf_extract': True,  # Whether to extract and store full PDF text
    'download_directory': os.path.join(DATA_DIR, "papers", "pdfs"),
    'max_papers_per_run': 15  # Maximum number of papers to process in a single run
} 

================================================================================
File: create_db.sql
================================================================================

-- Content table stores all video data
CREATE TABLE IF NOT EXISTS videos (
    id INTEGER PRIMARY KEY,
    shortcode TEXT UNIQUE,
    account TEXT,
    filename TEXT,
    caption TEXT,
    transcript TEXT,
    summary TEXT,
    timestamp TEXT,
    download_date TEXT,
    url TEXT,
    likes INTEGER,
    comments INTEGER,
    word_count INTEGER,
    duration_seconds INTEGER,
    key_phrases TEXT
);

-- Tags table for improved filtering
CREATE TABLE IF NOT EXISTS tags (
    id INTEGER PRIMARY KEY,
    video_id INTEGER,
    tag TEXT,
    FOREIGN KEY (video_id) REFERENCES videos(id)
);

-- Improved indexing for better performance
CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

-- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
    shortcode,
    account,
    caption,
    transcript,
    summary,
    timestamp,
    content=videos,
    tokenize=porter
);

-- Create triggers to keep FTS table synchronized
CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END;

CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
END;

CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END; 

================================================================================
File: db_migration.py
================================================================================

"""
Database migration module for Instagram Knowledge Base
"""
import os
import logging
import sqlite3
from datetime import datetime
import json
import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('db_migration')

def migrate_database():
    """
    Perform database migration to support multiple content sources and vector embeddings
    
    Returns:
        bool: Success or failure
    """
    logger.info("Starting database migration")
    start_time = datetime.now()
    
    try:
        # Connect to the database
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Check for existing tables
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='videos'")
        has_videos_table = cursor.fetchone() is not None
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='source_types'")
        has_source_types_table = cursor.fetchone() is not None
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='github_repos'")
        has_github_repos_table = cursor.fetchone() is not None
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ai_content'")
        has_ai_content_table = cursor.fetchone() is not None
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='research_papers'")
        has_research_papers_table = cursor.fetchone() is not None
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='content_embeddings'")
        has_content_embeddings_table = cursor.fetchone() is not None
        
        # Create source_types table if it doesn't exist
        if not has_source_types_table:
            cursor.execute("""
            CREATE TABLE source_types (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                UNIQUE(name)
            )
            """)
            
            # Insert initial source types
            cursor.execute("INSERT INTO source_types (name, description) VALUES (?, ?)", 
                         ('instagram', 'Instagram videos and posts'))
            cursor.execute("INSERT INTO source_types (name, description) VALUES (?, ?)", 
                         ('github', 'GitHub repositories'))
            cursor.execute("INSERT INTO source_types (name, description) VALUES (?, ?)", 
                         ('research_paper', 'Scientific research papers'))
            
            logger.info("Created source_types table")
        
        # Create GitHub repositories table if it doesn't exist
        if not has_github_repos_table:
            cursor.execute("""
            CREATE TABLE github_repos (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                full_name TEXT NOT NULL,
                description TEXT,
                url TEXT,
                stars INTEGER,
                watchers INTEGER,
                forks INTEGER,
                language TEXT,
                last_push TEXT,
                created_at TEXT,
                updated_at TEXT,
                topics TEXT,
                readme TEXT,
                last_crawled TEXT,
                UNIQUE(full_name)
            )
            """)
            
            # Create index for searching
            cursor.execute("CREATE INDEX idx_github_repos_name ON github_repos(name)")
            cursor.execute("CREATE INDEX idx_github_repos_language ON github_repos(language)")
            
            logger.info("Created github_repos table")
        
        # Create research papers table if it doesn't exist
        if not has_research_papers_table:
            cursor.execute("""
            CREATE TABLE research_papers (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                authors TEXT,
                abstract TEXT,
                publication TEXT,
                year INTEGER,
                url TEXT,
                doi TEXT,
                pdf_path TEXT,
                content TEXT,
                last_crawled TEXT,
                UNIQUE(doi)
            )
            """)
            
            # Create indices
            cursor.execute("CREATE INDEX idx_research_papers_title ON research_papers(title)")
            cursor.execute("CREATE INDEX idx_research_papers_year ON research_papers(year)")
            
            logger.info("Created research_papers table")
        
        # Check if ai_content table exists but needs migration
        if has_ai_content_table:
            # Check for the required columns
            cursor.execute("PRAGMA table_info(ai_content)")
            columns = {column[1] for column in cursor.fetchall()}
            
            # If the table structure is significantly different, we might need to recreate it
            # But let's check if we just need to add a date_indexed column
            if "date_indexed" not in columns:
                try:
                    logger.info("Adding date_indexed column to ai_content table")
                    cursor.execute("ALTER TABLE ai_content ADD COLUMN date_indexed TEXT")
                    conn.commit()
                except sqlite3.OperationalError as e:
                    logger.warning(f"Could not add date_indexed column: {str(e)}")
            
            # Verify that required columns exist
            required_columns = {"source_id", "source_type_id"}
            missing_columns = required_columns - columns
            
            if missing_columns:
                logger.error(f"ai_content table is missing required columns: {missing_columns}, manual intervention needed")
                # We won't proceed with migration that could lose data
                return False
        else:
            # Create unified content table if it doesn't exist
            cursor.execute("""
            CREATE TABLE ai_content (
                id INTEGER PRIMARY KEY,
                source_type_id INTEGER NOT NULL,
                source_id TEXT NOT NULL,
                title TEXT,
                description TEXT,
                content TEXT,
                url TEXT,
                date_created TEXT,
                date_collected TEXT NOT NULL,
                date_indexed TEXT,
                metadata TEXT,
                is_indexed INTEGER DEFAULT 0,
                embedding_file TEXT,
                FOREIGN KEY(source_type_id) REFERENCES source_types(id),
                UNIQUE(source_type_id, source_id)
            )
            """)
            
            # Create indices
            cursor.execute("CREATE INDEX idx_ai_content_source ON ai_content(source_type_id, source_id)")
            cursor.execute("CREATE INDEX idx_ai_content_title ON ai_content(title)")
            cursor.execute("CREATE INDEX idx_ai_content_date ON ai_content(date_created)")
            
            logger.info("Created ai_content table")
            
            # Commit to ensure table is created before migration
            conn.commit()
            
            # Migrate existing Instagram videos if available
            if has_videos_table:
                try:
                    # Get source_type_id for Instagram
                    cursor.execute("SELECT id FROM source_types WHERE name = 'instagram'")
                    instagram_source_type_id = cursor.fetchone()
                    
                    if instagram_source_type_id is None:
                        # Insert instagram source type if not already present
                        cursor.execute("INSERT INTO source_types (name, description) VALUES (?, ?)", 
                                     ('instagram', 'Instagram videos and posts'))
                        instagram_source_type_id = cursor.lastrowid
                    else:
                        instagram_source_type_id = instagram_source_type_id[0]
                    
                    # Get columns in videos table to handle different schema versions
                    cursor.execute("PRAGMA table_info(videos)")
                    columns = [column[1] for column in cursor.fetchall()]
                    
                    # Build dynamic query based on available columns
                    select_columns = ["id", "shortcode"]
                    
                    # Add caption if it exists
                    if "caption" in columns:
                        select_columns.append("caption")
                    else:
                        select_columns.append("''")  # Empty string as placeholder
                    
                    # Add timestamp if it exists
                    if "timestamp" in columns:
                        select_columns.append("timestamp")
                    
                    # Add username if it exists, or try account
                    if "username" in columns:
                        select_columns.append("username")
                    elif "account" in columns:
                        select_columns.append("account")
                    
                    if "filepath" in columns:
                        select_columns.append("filepath")
                    
                    # Add where clause if downloaded column exists
                    where_clause = ""
                    if "downloaded" in columns:
                        where_clause = "WHERE downloaded = 1"
                    
                    # Create the query
                    query = f"SELECT {', '.join(select_columns)} FROM videos {where_clause}"
                    
                    # Execute the query
                    cursor.execute(query)
                    videos = cursor.fetchall()
                    
                    migrated_count = 0
                    for video in videos:
                        try:
                            # Extract data with flexible column positions
                            video_id = video[0]
                            shortcode = video[1]
                            caption = video[2] if len(video) > 2 else ""
                            
                            # Handle timestamp if available (index will depend on whether caption exists)
                            timestamp_idx = select_columns.index("timestamp") if "timestamp" in select_columns else -1
                            timestamp = video[timestamp_idx] if timestamp_idx >= 0 and timestamp_idx < len(video) else None
                            
                            # Handle username/account
                            username_idx = -1
                            if "username" in select_columns:
                                username_idx = select_columns.index("username")
                            elif "account" in select_columns:
                                username_idx = select_columns.index("account")
                                
                            username = video[username_idx] if username_idx >= 0 and username_idx < len(video) else "unknown"
                            
                            # Format date
                            date_created = datetime.now().isoformat()
                            if timestamp:
                                try:
                                    # Just use the timestamp as is, don't try to convert it
                                    date_created = timestamp
                                except:
                                    # Use current time if timestamp causes issues
                                    logger.warning(f"Could not use timestamp for video {video_id}, using current time")
                            
                            # Insert into ai_content table
                            cursor.execute("""
                            INSERT OR IGNORE INTO ai_content 
                            (source_type_id, source_id, title, description, url, date_created, date_collected, metadata)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                instagram_source_type_id,
                                str(video_id),
                                f"Instagram video from {username}",
                                caption,
                                f"https://www.instagram.com/p/{shortcode}/",
                                date_created,
                                datetime.now().isoformat(),
                                json.dumps({"shortcode": shortcode, "username": username})
                            ))
                            migrated_count += 1
                            
                            # Commit every 10 items to avoid holding locks too long
                            if migrated_count % 10 == 0:
                                conn.commit()
                                
                        except Exception as e:
                            logger.error(f"Error migrating video {video[0]}: {str(e)}")
                    
                    # Final commit
                    conn.commit()
                    logger.info(f"Migrated {migrated_count} Instagram videos to ai_content table")
                except Exception as e:
                    logger.error(f"Error migrating Instagram videos: {str(e)}")
        
        # Create content embeddings table if it doesn't exist
        if not has_content_embeddings_table:
            cursor.execute("""
            CREATE TABLE content_embeddings (
                id INTEGER PRIMARY KEY,
                content_id INTEGER NOT NULL,
                embedding_vector BLOB NOT NULL,
                embedding_model TEXT NOT NULL,
                chunk_index INTEGER DEFAULT 0,
                chunk_text TEXT,
                date_created TEXT NOT NULL,
                FOREIGN KEY(content_id) REFERENCES ai_content(id),
                UNIQUE(content_id, chunk_index)
            )
            """)
            
            # Create indices
            cursor.execute("CREATE INDEX idx_content_embeddings_content ON content_embeddings(content_id)")
            
            logger.info("Created content_embeddings table")
            
        # Create FTS virtual table for full-text search if it doesn't exist
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ai_content_fts'")
        has_fts_table = cursor.fetchone() is not None
        
        if not has_fts_table and has_ai_content_table:
            try:
                # Get column names from ai_content
                cursor.execute("PRAGMA table_info(ai_content)")
                columns = [column[1] for column in cursor.fetchall()]
                
                # Check required columns for FTS
                fts_columns = ["title", "description", "content"]
                available_fts_columns = [col for col in fts_columns if col in columns]
                
                if len(available_fts_columns) > 0:
                    # Create column definitions for FTS
                    content_link = 'content="ai_content"'
                    indexed_columns = ", ".join(available_fts_columns)
                    notindexed_columns = "notindexed=id, notindexed=source_type_id"
                    
                    fts_query = f"""
                    CREATE VIRTUAL TABLE ai_content_fts USING fts4(
                        {content_link},
                        {indexed_columns},
                        {notindexed_columns}
                    )
                    """
                    
                    cursor.execute(fts_query)
                    
                    # Populate FTS table with existing content
                    select_columns = ", ".join(available_fts_columns)
                    cursor.execute(f"""
                    INSERT INTO ai_content_fts(docid, {select_columns})
                    SELECT id, {select_columns} FROM ai_content
                    WHERE content IS NOT NULL
                    """)
                    
                    logger.info("Created ai_content_fts virtual table for full-text search")
                else:
                    logger.warning("Could not create FTS table: required columns not found in ai_content")
            except Exception as e:
                logger.warning(f"Could not create FTS table: {str(e)}")
        
        # Commit changes
        conn.commit()
        
        # Done
        elapsed_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"Database migration completed successfully in {elapsed_time:.2f} seconds")
        return True
        
    except Exception as e:
        logger.error(f"Database migration failed: {str(e)}")
        return False

if __name__ == "__main__":
    migrate_database() 

================================================================================
File: downloader.py
================================================================================

"""
Module for downloading Instagram content with proper rate limiting
"""
import os
import time
import json
import logging
import random
from datetime import datetime, timedelta
import instaloader
import sqlite3

# Add imports for proxy testing
import requests
from urllib.parse import urlparse
from urllib.parse import parse_qs
import socket
import json as json_lib
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from config import (
    INSTAGRAM_ACCOUNTS, 
    INSTAGRAM_USERNAME, 
    INSTAGRAM_PASSWORD,
    INSTAGRAM_ACCOUNT_ROTATION,
    PROXY_SERVERS,
    PROXY_COUNTRY,
    DOWNLOAD_DIR, 
    DOWNLOAD_DELAY, 
    MAX_DOWNLOADS_PER_RUN,
    DATA_DIR,
    ACCOUNT_COOLDOWN_MINUTES,
    PROXY_COOLDOWN_MINUTES,
    DB_PATH,
    INSTAGRAM_CREDENTIALS,
    PROXY_CONFIG,
    RATE_LIMIT_WAIT,
    CONTENT_SOURCES
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(DATA_DIR, 'logs', 'downloader.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('downloader')

# Account state tracking
def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(os.path.join(DATA_DIR, "state"), exist_ok=True)
    
    # Create account-specific directories
    for account in INSTAGRAM_ACCOUNTS:
        account_dir = os.path.join(DOWNLOAD_DIR, account["username"])
        os.makedirs(account_dir, exist_ok=True)

def get_random_delay():
    """Return a more human-like delay between actions"""
    # Base delay plus random variation to appear more human-like
    return DOWNLOAD_DELAY + random.uniform(-2, 5)

def get_next_account():
    """Get the next available account from rotation"""
    # If no accounts in rotation, use the default credentials
    if not INSTAGRAM_ACCOUNT_ROTATION:
        return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD
    
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Find the next available account
    for i in range(len(INSTAGRAM_ACCOUNT_ROTATION)):
        idx = (state["last_index"] + i + 1) % len(INSTAGRAM_ACCOUNT_ROTATION)
        account = INSTAGRAM_ACCOUNT_ROTATION[idx]
        username = account["username"]
        
        # Skip accounts that are in cooldown
        account_state = state["account_states"].get(username, {})
        next_available_str = account_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            logger.info(f"Using account {username} from rotation")
            return account["username"], account["password"]
    
    # If all accounts are in cooldown, fallback to default account
    logger.warning("All accounts in rotation are in cooldown, using default account")
    return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account as in cooldown after a failure"""
    if not username:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update account state
    if username not in state["account_states"]:
        state["account_states"][username] = {}
        
    state["account_states"][username]["next_available"] = next_available
    state["account_states"][username]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Account {username} marked for cooldown until {next_available}")

def get_proxy(country=None):
    """
    Get a proxy from the available pool
    
    Args:
        country: Optional two-letter country code (us, uk, etc.)
    """
    # If no proxies configured, return None
    if not PROXY_SERVERS:
        return None
    
    # Use country from config if not specified in function call
    if country is None and PROXY_COUNTRY:
        country = PROXY_COUNTRY
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    
    # Make sure the directory exists
    os.makedirs(os.path.dirname(state_file), exist_ok=True)
    
    # Load proxy state
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Find the next available proxy
    for i in range(len(PROXY_SERVERS)):
        idx = (state["last_index"] + i + 1) % len(PROXY_SERVERS)
        base_proxy = PROXY_SERVERS[idx]
        
        # If country is specified, modify the proxy URL
        proxy = base_proxy
        if country and "zone-residential" in base_proxy:
            # Extract components from the proxy URL
            parts = base_proxy.split('@')
            if len(parts) == 2:
                auth_part = parts[0]
                host_part = parts[1]
                
                # Check if country parameter is already in the auth part
                if "country-" in auth_part:
                    # Replace existing country
                    auth_parts = auth_part.split('-country-')
                    if len(auth_parts) == 2:
                        country_and_after = auth_parts[1].split(':', 1)
                        if len(country_and_after) == 2:
                            new_auth = f"{auth_parts[0]}-country-{country}:{country_and_after[1]}"
                            proxy = f"{new_auth}@{host_part}"
                else:
                    # Add country before the password
                    auth_parts = auth_part.split(':')
                    if len(auth_parts) >= 2:
                        password_idx = len(auth_parts) - 1
                        auth_parts[password_idx] = f"country-{country}:{auth_parts[password_idx].split(':')[-1]}"
                        proxy = f"{':'.join(auth_parts)}@{host_part}"
        
        # Skip proxies that are in cooldown
        proxy_state = state["proxy_states"].get(proxy, {})
        next_available_str = proxy_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            # Test the proxy before returning
            if test_proxy(proxy):
                logger.info(f"Using proxy: {proxy}")
                return proxy
            else:
                # Mark as in cooldown if test fails
                mark_proxy_cooldown(proxy, cooldown_minutes=30)
                logger.warning(f"Proxy test failed, marking for cooldown: {proxy}")
                continue
    
    # If all proxies are in cooldown, log warning and return None
    logger.warning("All proxies are in cooldown or not working, proceeding without proxy")
    return None

def test_proxy(proxy_url, test_url="https://www.instagram.com/favicon.ico", timeout=30):
    """Test if a proxy server is working correctly"""
    try:
        # Extract proxy username and password from URL
        parsed_url = urlparse(proxy_url)
        username = parsed_url.username or ""
        password = parsed_url.password or ""
        
        # Create proxy dictionary in the format required by requests
        scheme = parsed_url.scheme
        netloc = parsed_url.netloc
        if '@' in netloc:
            netloc = netloc.split('@')[1]  # Remove credentials from netloc
        
        proxies = {
            "http": f"{scheme}://{username}:{password}@{netloc}",
            "https": f"{scheme}://{username}:{password}@{netloc}"
        }
        
        # Test the proxy by making a request to the test URL
        response = requests.get(test_url, proxies=proxies, timeout=timeout, verify=False)
        
        if response.status_code == 200:
            logger.info(f"Proxy test successful: {netloc}")
            return True
        else:
            logger.warning(f"Proxy test failed with status code {response.status_code}: {netloc}")
            return False
            
    except Exception as e:
        logger.error(f"Error testing proxy: {str(e)}")
        return False

def mark_proxy_cooldown(proxy, cooldown_minutes=PROXY_COOLDOWN_MINUTES):
    """Mark a proxy as in cooldown after a failure"""
    if not proxy:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update proxy state
    if proxy not in state["proxy_states"]:
        state["proxy_states"][proxy] = {}
        
    state["proxy_states"][proxy]["next_available"] = next_available
    state["proxy_states"][proxy]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Proxy {proxy} marked for cooldown until {next_available}")

def login_with_session(L, username, password):
    """Login with proper session management and error handling"""
    session_file = os.path.join(DATA_DIR, "state", f"insta_session_{username}.txt")
    
    # Try to load existing session first
    if os.path.exists(session_file) and username:
        try:
            L.load_session_from_file(username, session_file)
            logger.info(f"Loaded existing session for {username}")
            
            # Test the session validity by trying a simple operation
            try:
                test_profile = instaloader.Profile.from_username(L.context, username)
                logger.info("Session validation successful")
                return True
            except Exception:
                logger.warning("Loaded session is invalid, will login again")
        except Exception as e:
            logger.warning(f"Could not load session: {str(e)}")
    
    # If we need to login again
    if not username or not password:
        logger.warning("No login credentials provided")
        return False
        
    try:
        # Add random delay before login to look more human-like
        delay = random.uniform(1, 3)
        logger.info(f"Waiting {delay:.1f}s before login attempt...")
        time.sleep(delay)
        
        L.login(username, password)
        
        # Save the session for future use
        L.save_session_to_file(session_file)
        logger.info(f"Login successful and session saved for {username}")
        return True
    except Exception as e:
        logger.error(f"Login failed for {username}: {str(e)}")
        # Mark account for cooldown after failure
        mark_account_cooldown(username)
        return False

def create_instaloader_instance(use_login=True, account=None, proxy=None):
    """Create an Instaloader instance with appropriate settings for our use case"""
    
    # Get a random user agent to appear more human-like
    user_agent = get_random_user_agent()
    
    # Create an Instaloader instance with our required settings
    loader = instaloader.Instaloader(
        download_videos=True,
        download_video_thumbnails=False,
        download_geotags=False,
        download_comments=False,
        save_metadata=True,
        compress_json=False,
        user_agent=user_agent,
        max_connection_attempts=3,
        sleep=True,  # Respect Instagram's rate limits
    )
    
    logger.info(f"Initialized Instaloader with user agent: {user_agent[:30]}...")
    
    # If a proxy is provided, set it on the session
    if proxy:
        try:
            # Extract proxy username and password from URL
            parsed_url = urlparse(proxy)
            username = parsed_url.username or ""
            password = parsed_url.password or ""
            
            # Create proxy string in the format required by requests
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            if '@' in netloc:
                netloc = netloc.split('@')[1]  # Remove credentials from netloc
            
            proxy_str = f"{scheme}://{username}:{password}@{netloc}"
            
            # Set the proxy on the session
            loader.context._session.proxies = {
                "http": proxy_str,
                "https": proxy_str
            }
            logger.info(f"Set proxy on Instaloader session: {netloc}")
        except Exception as e:
            logger.error(f"Error setting proxy on Instaloader session: {str(e)}")
    
    # Log in if requested and credentials are available
    if use_login:
        if account is None:
            # Use default credentials if no specific account is provided
            username = INSTAGRAM_USERNAME
            password = INSTAGRAM_PASSWORD
        else:
            # Use the provided account credentials
            username = account.get('username')
            password = account.get('password')
        
        if username and password:
            try:
                # Add a random delay before login to avoid detection
                delay = random.uniform(1, 3)
                logger.debug(f"Adding random delay of {delay:.2f}s before login attempt")
                time.sleep(delay)
                
                loader.login(username, password)
                logger.info(f"Logged in as {username}")
            except Exception as e:
                logger.error(f"Login failed for {username}: {str(e)}")
    
    return loader

def retry_with_backoff(func, max_retries=3, initial_delay=5):
    """Execute a function with exponential backoff retries"""
    retries = 0
    while retries <= max_retries:
        try:
            return func()
        except Exception as e:  # Use generic Exception instead of specific ones
            retries += 1
            if retries > max_retries:
                raise
            wait_time = initial_delay * (2 ** retries) + random.uniform(1, 5)
            logger.warning(f"Retry {retries}/{max_retries} after error: {str(e)}. Waiting {wait_time:.1f}s")
            time.sleep(wait_time)
    return None

def download_from_instagram(accounts=None):
    """
    Download content from Instagram accounts with proper rate limiting
    and proxy rotation
    """
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    downloaded_count = 0
    success_count = 0
    
    # Use the accounts from config if none are provided
    if accounts is None:
        accounts = INSTAGRAM_ACCOUNTS
    
    # Store failed accounts to retry later
    failed_accounts = []
    
    # Randomize the account order to distribute load
    accounts_to_process = list(accounts)
    random.shuffle(accounts_to_process)
    
    # Process each target account
    for account_idx, account_info in enumerate(accounts_to_process):
        # Extract the account name depending on the type
        if isinstance(account_info, dict):
            account_name = account_info.get("username")
        else:
            account_name = account_info
            
        if not account_name:
            logger.warning(f"Skipping invalid account info: {account_info}")
            continue
            
        # Check if we've reached the download limit
        if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
            logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
            break
            
        # Skip accounts that are not due for refresh
        if not is_account_due_for_refresh(account_name):
            logger.info(f"Skipping account {account_name} - not due for refresh")
            continue
            
        # Get a proxy if available
        proxy = get_proxy()
        if not proxy:
            logger.warning("No proxy available, proceeding without proxy")
        else:
            logger.info(f"Using proxy: {proxy}")
        
        # Log the attempt
        logger.info(f"Processing account {account_idx+1}/{len(accounts_to_process)}: {account_name}")
        
        # Add a delay before processing to avoid detection
        delay = random.uniform(2, 5)
        logger.debug(f"Adding random delay of {delay:.2f}s before processing account")
        time.sleep(delay)
        
        # Try up to 2 different methods to download content
        profile = None
        posts = None
        attempt_count = 0
        max_attempts = 2
        
        while attempt_count < max_attempts and posts is None:
            attempt_count += 1
            logger.info(f"Attempt {attempt_count}/{max_attempts} for account {account_name}")
            
            try:
                # Create Instaloader instance
                use_login = (attempt_count > 1)  # Try without login first, then with login if available
                L = create_instaloader_instance(use_login=use_login, proxy=proxy)
                
                # Try to get profile
                if profile is None:
                    profile = instaloader.Profile.from_username(L.context, account_name)
                    
                    # Check if the profile has posts
                    if not hasattr(profile, 'get_posts'):
                        logger.error(f"Profile {account_name} does not have get_posts method")
                        raise ValueError(f"Invalid profile structure for {account_name}")
                    
                    # Mark this account as processed now (whether it succeeds or fails)
                    mark_account_processed(account_name)
                    
                    # Handle private account
                    if profile.is_private and not use_login:
                        logger.warning(f"Account {account_name} is private - will retry with login if credentials available")
                        continue  # Skip to next attempt (which will use login)
                    
                    logger.info(f"Successfully retrieved profile for {account_name}")
                
                # Get the iterator for posts with proper error handling
                try:
                    # Add a random delay before fetching posts to appear more human-like
                    time.sleep(random.uniform(1, 3))
                    
                    # Get posts iterator
                    posts = profile.get_posts()
                    
                    # Try to access the first post to validate the iterator
                    next(posts)
                    # Reset the iterator
                    posts = profile.get_posts()
                    
                except StopIteration:
                    logger.warning(f"No posts found for account {account_name}")
                    posts = []  # Empty list to indicate success but no posts
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    logger.error(f"Error getting posts for {account_name} (attempt {attempt_count}): {str(e)}")
                    
                    # Check for specific errors that indicate we should try alternative approaches
                    if "401" in error_msg or "unauthorized" in error_msg:
                        if attempt_count < max_attempts:
                            logger.info(f"401 Unauthorized error - will retry with different approach")
                            # Mark previous proxy for cooldown and get a new one for next attempt
                            if proxy:
                                mark_proxy_cooldown(proxy)
                                proxy = get_proxy()
                            
                            posts = None  # Reset so we try again
                            time.sleep(5)  # Wait before retrying
                            continue
                    
                    # If we get here, we couldn't get posts after all retries
                    failed_accounts.append(account_name)
                    break
                
                # Process posts if we have them
                if posts is not None:
                    process_posts(L, profile, account_name, posts, 
                                 downloaded_count, success_count)
                    
            except Exception as e:
                logger.error(f"Error processing account {account_name} (attempt {attempt_count}): {str(e)}")
                
                if attempt_count < max_attempts:
                    # Mark previous proxy for cooldown and get a new one for next attempt
                    if proxy:
                        mark_proxy_cooldown(proxy)
                        proxy = get_proxy()
                    time.sleep(5)  # Wait before retrying
                else:
                    failed_accounts.append(account_name)
    
    # Log summary
    logger.info(f"Download session completed. Downloaded {success_count} videos from {len(accounts_to_process) - len(failed_accounts)}/{len(accounts_to_process)} accounts")
    
    if failed_accounts:
        logger.warning(f"Failed to process {len(failed_accounts)} accounts: {', '.join(str(a) for a in failed_accounts)}")
    
    return success_count, downloaded_count, failed_accounts

def process_posts(L, profile, account_name, posts, downloaded_count, success_count):
    """Process posts for an account"""
    # Process posts
    posts_processed = 0
    
    # Create account directory
    account_dir = os.path.join(DOWNLOAD_DIR, account_name)
    os.makedirs(account_dir, exist_ok=True)
    
    # Create a directory for metadata
    metadata_dir = os.path.join(account_dir, 'metadata')
    os.makedirs(metadata_dir, exist_ok=True)
    
    try:
        for post in posts:
            try:
                # Check if we've reached the maximum downloads limit
                if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
                    logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
                    break
                
                # Skip if it's not a video
                if not post.is_video:
                    logger.debug(f"Skipping non-video post from {account_name}: {post.shortcode}")
                    continue
                
                # Check if this video has already been downloaded
                video_filename = f"{post.date_utc.strftime('%Y-%m-%d_%H-%M-%S')}_{post.shortcode}.mp4"
                video_path = os.path.join(account_dir, video_filename)
                
                if os.path.exists(video_path):
                    logger.debug(f"Skipping already downloaded video: {video_filename}")
                    continue
                
                # Download the post
                logger.info(f"Downloading video post from {account_name}: {post.shortcode}")
                
                try:
                    # Download only the video
                    L.download_post(post, target=account_dir)
                    downloaded_count += 1
                    success_count += 1
                    
                    # Save post metadata to a separate JSON file
                    metadata = {
                        'shortcode': post.shortcode,
                        'date_utc': post.date_utc.strftime('%Y-%m-%d %H:%M:%S'),
                        'caption': post.caption if post.caption else '',
                        'likes': post.likes,
                        'comments': post.comments,
                        'url': f"https://www.instagram.com/p/{post.shortcode}/",
                        'account': account_name
                    }
                    
                    metadata_path = os.path.join(metadata_dir, f"{post.shortcode}.json")
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=4)
                    
                    # Respect Instagram's rate limits by adding a delay between downloads
                    time.sleep(DOWNLOAD_DELAY)
                    posts_processed += 1
                    
                except Exception as e:
                    logger.error(f"Error downloading post {post.shortcode} from {account_name}: {str(e)}")
                    # Continue with the next post despite this error
                    continue
            
            except Exception as post_error:
                logger.error(f"Error processing post from {account_name}: {str(post_error)}")
                # Continue with the next post
                continue
        
        logger.info(f"Processed {posts_processed} posts from {account_name}, downloaded {success_count} videos")
        return True, posts_processed
        
    except Exception as e:
        logger.error(f"Error processing posts for {account_name}: {str(e)}")
        return False, 0

def get_posts_alternative(profile, username):
    """Alternative approach to fetch posts when the standard iterator fails"""
    try:
        # Try direct URL construction approach
        base_url = f"https://www.instagram.com/{username}/"
        logger.info(f"Attempting alternative post fetching from {base_url}")
        
        # Return posts that we already have in the directory
        metadata_dir = os.path.join(DOWNLOAD_DIR, username, "metadata")
        existing_posts = []
        
        if os.path.exists(metadata_dir):
            for file in os.listdir(metadata_dir):
                if file.endswith('.json'):
                    try:
                        with open(os.path.join(metadata_dir, file), 'r') as f:
                            metadata = json.load(f)
                            if 'shortcode' in metadata:
                                existing_posts.append(metadata['shortcode'])
                    except Exception as e:
                        logger.error(f"Error reading metadata file {file}: {str(e)}")
        
        logger.info(f"Found {len(existing_posts)} existing posts metadata to process")
        return existing_posts
        
    except Exception as e:
        logger.error(f"Alternative post fetching failed: {str(e)}")
        return []

def get_random_user_agent():
    """Return a random user agent to appear more human-like"""
    user_agents = [
        # Desktop browsers
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        # Mobile browsers
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (Linux; Android 13; SM-S908B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.43 Mobile Safari/537.36"
    ]
    return random.choice(user_agents)

def schedule_refresh(username, backoff_minutes=60):
    """Schedule a refresh attempt with exponential backoff"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # Load or initialize state
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r') as f:
                state = json.load(f)
        except json.JSONDecodeError:
            state = {"accounts": {}}
    else:
        state = {"accounts": {}}
    
    # Get account state or initialize
    account_state = state["accounts"].get(username, {
        "last_attempt": None,
        "backoff_minutes": backoff_minutes,
        "consecutive_failures": 0
    })
    
    # Update for next attempt
    now = datetime.now().isoformat()
    account_state["last_attempt"] = now
    
    if account_state["consecutive_failures"] > 0:
        # Exponential backoff
        account_state["backoff_minutes"] *= 2
    
    # Schedule next attempt
    next_attempt = (datetime.now() + 
                   timedelta(minutes=account_state["backoff_minutes"]))
    account_state["next_attempt"] = next_attempt.isoformat()
    
    # Save state
    state["accounts"][username] = account_state
    with open(state_file, 'w') as f:
        json.dump(state, f)
    
    logger.info(f"Scheduled next refresh for {username} at {next_attempt.isoformat()}")
    return next_attempt.isoformat()

def should_refresh_account(username):
    """Check if an account is due for refresh based on backoff schedule"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # If no state file, always refresh
    if not os.path.exists(state_file):
        return True
        
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            
        # If account not in state, always refresh
        if username not in state.get("accounts", {}):
            return True
            
        account_state = state["accounts"][username]
        next_attempt_str = account_state.get("next_attempt")
        
        # If no next attempt scheduled, always refresh
        if not next_attempt_str:
            return True
            
        # Parse next attempt time
        next_attempt = datetime.fromisoformat(next_attempt_str)
        
        # Check if we're past the scheduled time
        return datetime.now() >= next_attempt
        
    except Exception as e:
        logger.error(f"Error checking refresh schedule: {str(e)}")
        return True  # Default to allowing refresh on error

def is_account_due_for_refresh(account_name):
    """Check if an account is due for refresh based on its last processing time"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    # Default: account is due for refresh
    if not os.path.exists(account_state_file):
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        return True
    
    try:
        # Load account states
        with open(account_state_file, 'r') as f:
            account_states = json.load(f)
        
        # Check if account exists in states
        if account_name not in account_states:
            return True
        
        account_state = account_states[account_name]
        last_processed = account_state.get('last_processed')
        
        # If no last processed time, account is due for refresh
        if not last_processed:
            return True
        
        # Check if account is in cooldown
        cooldown_until = account_state.get('cooldown_until')
        if cooldown_until:
            cooldown_time = datetime.fromisoformat(cooldown_until)
            if datetime.now() < cooldown_time:
                logger.info(f"Account {account_name} is in cooldown until {cooldown_until}")
                return False
        
        # Check if enough time has passed since last processing
        last_processed_time = datetime.fromisoformat(last_processed)
        refresh_interval = account_state.get('refresh_interval', 24)  # Default: 24 hours
        
        next_refresh_time = last_processed_time + timedelta(hours=refresh_interval)
        
        if datetime.now() < next_refresh_time:
            logger.info(f"Account {account_name} not due for refresh until {next_refresh_time.isoformat()}")
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Error checking refresh status for {account_name}: {str(e)}")
        # Default to allowing refresh on error
        return True

def mark_account_processed(account_name, success=True):
    """Mark an account as processed and update its refresh schedule"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if account_name not in account_states:
            account_states[account_name] = {}
        
        # Update account state
        account_states[account_name]['last_processed'] = datetime.now().isoformat()
        
        # Update success/failure count
        if success:
            account_states[account_name]['consecutive_failures'] = 0
            # Reset refresh interval to default on success
            account_states[account_name]['refresh_interval'] = 24  # Default: 24 hours
        else:
            # Increment failure count
            failures = account_states[account_name].get('consecutive_failures', 0) + 1
            account_states[account_name]['consecutive_failures'] = failures
            
            # Implement exponential backoff for failures
            backoff_hours = min(24 * (2 ** (failures - 1)), 168)  # Max 1 week
            account_states[account_name]['refresh_interval'] = backoff_hours
            
            # Set cooldown period for repeated failures
            if failures > 2:
                cooldown_minutes = ACCOUNT_COOLDOWN_MINUTES * (2 ** (failures - 3))  # Exponential cooldown
                cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
                account_states[account_name]['cooldown_until'] = cooldown_until
                logger.warning(f"Account {account_name} in cooldown until {cooldown_until} after {failures} failures")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {account_name} as processed: {str(e)}")

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account for cooldown after a failure"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if username not in account_states:
            account_states[username] = {}
        
        # Set cooldown period
        cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
        account_states[username]['cooldown_until'] = cooldown_until
        
        # Increment failure count
        failures = account_states[username].get('consecutive_failures', 0) + 1
        account_states[username]['consecutive_failures'] = failures
        
        logger.warning(f"Account {username} in cooldown until {cooldown_until}")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {username} for cooldown: {str(e)}")

if __name__ == "__main__":
    download_from_instagram() 

================================================================================
File: embeddings.py
================================================================================

"""
Embedding generation module for creating vector representations of content

This module provides functionality to generate and store vector embeddings
for content in the knowledge base using sentence-transformers.
"""
import os
import logging
import sqlite3
import pickle
import time
from datetime import datetime
import numpy as np
from typing import List, Dict, Any, Union, Optional, Tuple

# Import local modules
import config
from chunking import chunk_text, prepare_content_for_embedding

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('embeddings')

# Check if sentence-transformers is available
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    logger.warning("sentence-transformers not available. Using fallback embedding method.")
    SENTENCE_TRANSFORMERS_AVAILABLE = False

class EmbeddingGenerator:
    """Class for generating embeddings from text content"""
    
    def __init__(self, model_name: str = "multi-qa-mpnet-base-dot-v1"):
        """
        Initialize the embedding generator
        
        Args:
            model_name: Name of the sentence-transformers model to use
        """
        self.model_name = model_name
        self.embedding_size = 0
        
        # Initialize the model
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                logger.info(f"Loading sentence-transformers model: {model_name}")
                self.model = SentenceTransformer(model_name)
                self.embedding_size = self.model.get_sentence_embedding_dimension()
                logger.info(f"Model loaded. Embedding size: {self.embedding_size}")
            except Exception as e:
                logger.error(f"Error loading sentence-transformers model: {str(e)}")
                self.model = None
        else:
            logger.warning("Using fallback embedding model (simple TF-IDF)")
            self.model = None
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        Generate embedding for text using the loaded model
        
        Args:
            text: Text to generate embedding for
            
        Returns:
            Numpy array containing the embedding vector
        """
        if not text or len(text.strip()) == 0:
            logger.warning("Empty text provided for embedding generation")
            # Return a zero vector with correct dimensions
            if self.embedding_size > 0:
                return np.zeros(self.embedding_size)
            else:
                return np.zeros(768)  # Default fallback size
                
        if self.model is not None:
            # Use sentence-transformers
            try:
                embedding = self.model.encode(text, show_progress_bar=False)
                return embedding
            except Exception as e:
                logger.error(f"Error generating embedding with sentence-transformers: {str(e)}")
                return self._fallback_embedding(text)
        else:
            # Use fallback method
            return self._fallback_embedding(text)
    
    def _fallback_embedding(self, text: str) -> np.ndarray:
        """
        Fallback method for generating embeddings when sentence-transformers is not available
        This implements a simple TF-IDF based embedding
        
        Args:
            text: Text to generate embedding for
            
        Returns:
            Numpy array containing the embedding vector
        """
        # Simple fallback: convert text to bag of words and hash to fixed size
        from collections import Counter
        import hashlib
        
        # Default embedding size
        embedding_size = 768
        
        # Tokenize text (simple approach)
        tokens = text.lower().split()
        
        # Calculate term frequencies
        term_counts = Counter(tokens)
        
        # Generate a deterministic embedding based on term frequencies
        result = np.zeros(embedding_size)
        
        for term, count in term_counts.items():
            # Hash the term to get a position in the embedding
            term_hash = int(hashlib.md5(term.encode()).hexdigest(), 16)
            position = term_hash % embedding_size
            
            # Use term count as value, normalized by total tokens
            result[position] += count / len(tokens)
        
        # Normalize the vector
        norm = np.linalg.norm(result)
        if norm > 0:
            result = result / norm
            
        return result
    
    def process_content_item(self, content_id: int, force_update: bool = False, 
                            chunk_size: int = 1000, chunk_overlap: int = 200) -> bool:
        """
        Process a content item from the database, generating and storing embeddings
        
        Args:
            content_id: ID of the content item in the ai_content table
            force_update: Whether to update existing embeddings
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            
        Returns:
            Boolean indicating success
        """
        conn = None
        try:
            # Connect to database
            conn = sqlite3.connect(config.DB_PATH)
            cursor = conn.cursor()
            
            # Get content data
            cursor.execute("""
                SELECT title, description, content, source_type_id 
                FROM ai_content WHERE id = ?
            """, (content_id,))
            
            result = cursor.fetchone()
            if not result:
                logger.warning(f"Content with ID {content_id} not found")
                return False
            
            title, description, content, source_type_id = result
            
            # Get source type name
            cursor.execute("SELECT name FROM source_types WHERE id = ?", (source_type_id,))
            source_type_result = cursor.fetchone()
            source_type = source_type_result[0] if source_type_result else "unknown"
            
            # Skip if no meaningful content
            if not content or len(content.strip()) < 50:
                logger.warning(f"Content with ID {content_id} has insufficient text for embedding")
                return False
            
            # Check if we should update
            if not force_update:
                cursor.execute(
                    "SELECT COUNT(*) FROM content_embeddings WHERE content_id = ?",
                    (content_id,)
                )
                if cursor.fetchone()[0] > 0:
                    logger.debug(f"Embeddings already exist for content ID {content_id}")
                    return True
            
            # Prepare text for embedding
            full_text = prepare_content_for_embedding(title, description, content)
            
            # Generate chunks
            chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=chunk_overlap)
            if not chunks:
                logger.warning(f"Failed to generate chunks for content ID {content_id}")
                return False
            
            logger.info(f"Generated {len(chunks)} chunks for content ID {content_id}")
            
            # Process each chunk
            for i, chunk in enumerate(chunks):
                # Generate embedding
                start_time = time.time()
                embedding = self.generate_embedding(chunk)
                elapsed = time.time() - start_time
                
                logger.debug(f"Generated embedding for chunk {i+1}/{len(chunks)} in {elapsed:.2f}s")
                
                # Convert numpy array to binary for storage
                embedding_binary = pickle.dumps(embedding)
                
                # Store in database
                cursor.execute("""
                    INSERT OR REPLACE INTO content_embeddings 
                    (content_id, embedding_vector, embedding_model, chunk_index, chunk_text, date_created)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    content_id,
                    embedding_binary,
                    self.model_name,
                    i,
                    chunk,
                    datetime.now().isoformat()
                ))
            
            # Update ai_content to mark as processed
            cursor.execute("""
                UPDATE ai_content 
                SET metadata = json.set(COALESCE(metadata, '{}'), '$.embeddings_generated', 1),
                    date_indexed = ?
                WHERE id = ?
            """, (datetime.now().isoformat(), content_id))
            
            conn.commit()
            logger.info(f"Successfully processed content ID {content_id} with {len(chunks)} embeddings")
            return True
            
        except Exception as e:
            logger.error(f"Error processing content ID {content_id}: {str(e)}")
            if conn:
                conn.rollback()
            return False
            
        finally:
            if conn:
                conn.close()
    
    def process_batch(self, batch_size: int = 10, max_items: Optional[int] = None, 
                     source_type: Optional[str] = None) -> int:
        """
        Process a batch of content items from the database
        
        Args:
            batch_size: Number of items to process in a batch
            max_items: Maximum total number of items to process
            source_type: If provided, only process items of this source type
            
        Returns:
            Number of items successfully processed
        """
        conn = None
        try:
            # Connect to database
            conn = sqlite3.connect(config.DB_PATH)
            cursor = conn.cursor()
            
            # Build query to get unprocessed content
            query = """
                SELECT c.id FROM ai_content c
                WHERE c.content IS NOT NULL 
                AND LENGTH(c.content) > 100
                AND (
                    json_extract(c.metadata, '$.embeddings_generated') IS NULL 
                    OR json_extract(c.metadata, '$.embeddings_generated') = 0
                )
            """
            
            params = []
            
            # Add source type filter if provided
            if source_type:
                cursor.execute("SELECT id FROM source_types WHERE name = ?", (source_type,))
                source_type_id = cursor.fetchone()
                if source_type_id:
                    query += " AND c.source_type_id = ?"
                    params.append(source_type_id[0])
                else:
                    logger.warning(f"Source type '{source_type}' not found")
            
            # Add limit
            if max_items:
                query += " LIMIT ?"
                params.append(max_items)
            
            # Get content IDs
            cursor.execute(query, params)
            content_ids = [row[0] for row in cursor.fetchall()]
            
            logger.info(f"Found {len(content_ids)} content items to process")
            
            # Process in batches
            processed_count = 0
            for i in range(0, len(content_ids), batch_size):
                batch = content_ids[i:i+batch_size]
                logger.info(f"Processing batch {i//batch_size + 1}/{(len(content_ids)-1)//batch_size + 1}")
                
                for content_id in batch:
                    success = self.process_content_item(content_id)
                    if success:
                        processed_count += 1
                    
                    # Brief pause to avoid overwhelming resources
                    time.sleep(0.1)
                
                # Longer pause between batches
                if i + batch_size < len(content_ids):
                    logger.info(f"Batch complete. Pausing before next batch...")
                    time.sleep(1)
            
            return processed_count
            
        except Exception as e:
            logger.error(f"Error processing batch: {str(e)}")
            return 0
            
        finally:
            if conn:
                conn.close()

def main():
    """Main function for direct script execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate embeddings for content")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size for processing")
    parser.add_argument("--max-items", type=int, help="Maximum number of items to process")
    parser.add_argument("--source-type", help="Process only this source type (instagram, github, research_paper)")
    parser.add_argument("--model", default="multi-qa-mpnet-base-dot-v1", help="Name of the sentence-transformers model to use")
    parser.add_argument("--content-id", type=int, help="Process a specific content ID")
    parser.add_argument("--force-update", action="store_true", help="Force update of existing embeddings")
    
    args = parser.parse_args()
    
    # Create embedding generator
    generator = EmbeddingGenerator(model_name=args.model)
    
    if args.content_id:
        # Process specific content ID
        logger.info(f"Processing content ID: {args.content_id}")
        success = generator.process_content_item(args.content_id, force_update=args.force_update)
        if success:
            logger.info(f"Successfully processed content ID: {args.content_id}")
        else:
            logger.error(f"Failed to process content ID: {args.content_id}")
    else:
        # Process batch
        logger.info("Processing batch of content items")
        processed = generator.process_batch(
            batch_size=args.batch_size,
            max_items=args.max_items,
            source_type=args.source_type
        )
        logger.info(f"Successfully processed {processed} content items")

if __name__ == "__main__":
    main() 

================================================================================
File: generate_embeddings.py
================================================================================

#!/usr/bin/env python3
"""
Embedding Generation Script

This script generates vector embeddings for all content items in the database.
It processes content in batches, chunks the text, and stores the embeddings
in the content_embeddings table.
"""
import os
import sys
import logging
import sqlite3
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Tuple

# Add parent directory to path to ensure imports work
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import local modules
import config
from chunking import chunk_text, prepare_content_for_embedding
from embeddings import EmbeddingGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/embedding_generation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('generate_embeddings')

def ensure_embedding_table_exists(conn: sqlite3.Connection) -> None:
    """
    Ensure content_embeddings table exists in the database
    
    Args:
        conn: SQLite database connection
    """
    cursor = conn.cursor()
    
    # Check if content_embeddings table exists
    cursor.execute("""
        SELECT name FROM sqlite_master 
        WHERE type='table' AND name='content_embeddings'
    """)
    
    if not cursor.fetchone():
        logger.info("Creating content_embeddings table...")
        cursor.execute("""
            CREATE TABLE content_embeddings (
                id INTEGER PRIMARY KEY,
                content_id INTEGER NOT NULL,
                embedding_vector BLOB NOT NULL,
                embedding_model TEXT NOT NULL,
                chunk_index INTEGER DEFAULT 0,
                chunk_text TEXT,
                date_created TEXT NOT NULL,
                FOREIGN KEY(content_id) REFERENCES ai_content(id),
                UNIQUE(content_id, chunk_index)
            )
        """)
        
        # Create indices for faster lookup
        cursor.execute("""
            CREATE INDEX idx_content_embeddings_content_id
            ON content_embeddings(content_id)
        """)
        
        conn.commit()
        logger.info("content_embeddings table created")

def get_content_items(conn: sqlite3.Connection, 
                     source_type: Optional[str] = None,
                     limit: Optional[int] = None,
                     offset: int = 0,
                     skip_existing: bool = True) -> List[Dict[str, Any]]:
    """
    Get content items from the database
    
    Args:
        conn: SQLite database connection
        source_type: Optional filter by source type
        limit: Maximum number of items to return
        offset: Number of items to skip
        skip_existing: Skip items that already have embeddings
        
    Returns:
        List of content items
    """
    cursor = conn.cursor()
    
    # Build query
    query = """
        SELECT 
            c.id, c.source_type_id, c.source_id, c.title, c.description,
            c.content, c.date_created, c.metadata, st.name as source_type_name
        FROM ai_content c
        JOIN source_types st ON c.source_type_id = st.id
    """
    
    params = []
    
    # Add source type filter if provided
    if source_type:
        query += " WHERE st.name = ?"
        params.append(source_type)
    
    # Skip items that already have embeddings
    if skip_existing:
        if source_type:
            query += " AND NOT EXISTS"
        else:
            query += " WHERE NOT EXISTS"
            
        query += """
            (SELECT 1 FROM content_embeddings ce 
             WHERE ce.content_id = c.id)
        """
    
    # Add order by and limit/offset
    query += " ORDER BY c.id"
    
    if limit:
        query += " LIMIT ?"
        params.append(limit)
        
    if offset > 0:
        query += " OFFSET ?"
        params.append(offset)
    
    # Execute query
    cursor.execute(query, params)
    
    # Process results
    content_items = []
    for row in cursor.fetchall():
        (content_id, source_type_id, source_id, title, description, 
         content, date_created, metadata, source_type_name) = row
        
        content_items.append({
            'id': content_id,
            'source_type_id': source_type_id,
            'source_id': source_id,
            'title': title,
            'description': description,
            'content': content,
            'date_created': date_created,
            'metadata': metadata,
            'source_type_name': source_type_name
        })
    
    return content_items

def process_content_items(content_items: List[Dict[str, Any]], 
                         chunk_size: int = 500,
                         chunk_overlap: int = 100) -> Tuple[int, int, int]:
    """
    Process content items, chunk text, and generate embeddings
    
    Args:
        content_items: List of content items
        chunk_size: Size of each chunk in characters
        chunk_overlap: Overlap between chunks in characters
        
    Returns:
        Tuple of (total_items, successful_items, total_chunks)
    """
    conn = None
    try:
        conn = sqlite3.connect(config.DB_PATH)
        ensure_embedding_table_exists(conn)
        cursor = conn.cursor()
        
        # Create embedding generator
        embedding_generator = EmbeddingGenerator()
        model_name = embedding_generator.model_name
        
        # Track statistics
        total_items = len(content_items)
        successful_items = 0
        total_chunks = 0
        start_time = time.time()
        
        # Process each content item
        for i, item in enumerate(content_items):
            try:
                # Prepare text for chunking (combine title, description, content)
                text = prepare_content_for_embedding(
                    title=item.get('title', ''),
                    description=item.get('description', ''),
                    content=item.get('content', '')
                )
                
                # Skip if no text
                if not text.strip():
                    logger.warning(f"Skipping content {item['id']} - no text content")
                    continue
                
                # Chunk text
                text_chunks = chunk_text(text, chunk_size=chunk_size, overlap=chunk_overlap)
                
                # Process each chunk
                for chunk_idx, chunk_content in enumerate(text_chunks):
                    # Generate embedding
                    embedding = embedding_generator.generate_embedding(chunk_content)
                    
                    if embedding is not None:
                        # Serialize embedding (binary pickle)
                        import pickle
                        embedding_binary = pickle.dumps(embedding)
                        
                        # Store in database
                        cursor.execute("""
                            INSERT OR REPLACE INTO content_embeddings
                            (content_id, chunk_index, chunk_text, embedding_vector, embedding_model, date_created)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            item['id'],
                            chunk_idx,
                            chunk_content,
                            embedding_binary,
                            model_name,
                            datetime.now().isoformat()
                        ))
                        
                        total_chunks += 1
                
                # Commit after each item
                conn.commit()
                
                successful_items += 1
                
                # Log progress
                if (i + 1) % 10 == 0 or (i + 1) == total_items:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (i + 1)
                    logger.info(f"Processed {i+1}/{total_items} items, {total_chunks} chunks "
                                f"({successful_items} successful), "
                                f"avg {avg_time:.2f}s per item")
            
            except Exception as e:
                logger.error(f"Error processing content {item['id']}: {str(e)}")
                # Continue with next item
                continue
        
        return total_items, successful_items, total_chunks
        
    except Exception as e:
        logger.error(f"Error in process_content_items: {str(e)}")
        if conn:
            conn.rollback()
        return len(content_items), 0, 0
        
    finally:
        if conn:
            conn.close()

def main():
    """Main function for script execution"""
    parser = argparse.ArgumentParser(
        description="Generate embeddings for content in the database"
    )
    
    parser.add_argument(
        "--source-type", 
        help="Process only content of this source type (e.g., 'instagram', 'github', 'arxiv')"
    )
    
    parser.add_argument(
        "--limit", type=int, 
        help="Maximum number of items to process"
    )
    
    parser.add_argument(
        "--batch-size", type=int, default=100,
        help="Number of items to process in each batch"
    )
    
    parser.add_argument(
        "--chunk-size", type=int, default=500,
        help="Size of text chunks in characters"
    )
    
    parser.add_argument(
        "--chunk-overlap", type=int, default=100,
        help="Overlap between text chunks in characters"
    )
    
    parser.add_argument(
        "--force", action="store_true",
        help="Process items even if they already have embeddings"
    )
    
    parser.add_argument(
        "--stats", action="store_true",
        help="Show database statistics and exit"
    )
    
    args = parser.parse_args()
    
    # Create logs directory if it doesn't exist
    os.makedirs("logs", exist_ok=True)
    
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    
    if args.stats:
        # Show database statistics
        cursor = conn.cursor()
        
        # Get total content count
        cursor.execute("SELECT COUNT(*) FROM ai_content")
        total_content = cursor.fetchone()[0]
        
        # Get count by source type
        cursor.execute("""
            SELECT st.name, COUNT(*)
            FROM ai_content c
            JOIN source_types st ON c.source_type_id = st.id
            GROUP BY st.name
        """)
        
        counts_by_source = cursor.fetchall()
        
        # Get embedding statistics
        cursor.execute("""
            SELECT COUNT(*) FROM content_embeddings
        """)
        
        total_embeddings = cursor.fetchone()[0]
        
        cursor.execute("""
            SELECT COUNT(DISTINCT content_id) FROM content_embeddings
        """)
        
        content_with_embeddings = cursor.fetchone()[0]
        
        # Print statistics
        print("\nDatabase Statistics:")
        print("=" * 40)
        print(f"Total content items: {total_content}")
        print(f"Content items with embeddings: {content_with_embeddings}")
        print(f"Total embedding chunks: {total_embeddings}")
        print("\nContent by source type:")
        
        for source_name, count in counts_by_source:
            print(f"- {source_name}: {count}")
        
        # Calculate items without embeddings
        items_without_embeddings = total_content - content_with_embeddings
        print(f"\nContent items without embeddings: {items_without_embeddings}")
        
        conn.close()
        return
    
    # Start processing
    logger.info("Starting embedding generation...")
    logger.info(f"Source type: {args.source_type if args.source_type else 'all'}")
    logger.info(f"Limit: {args.limit if args.limit else 'none'}")
    logger.info(f"Batch size: {args.batch_size}")
    logger.info(f"Chunk size: {args.chunk_size}")
    logger.info(f"Chunk overlap: {args.chunk_overlap}")
    logger.info(f"Force reprocess: {args.force}")
    
    # Initialize counters
    total_processed = 0
    total_successful = 0
    total_chunks = 0
    
    # Process in batches
    offset = 0
    while True:
        # Get batch of content items
        conn = sqlite3.connect(config.DB_PATH)
        content_items = get_content_items(
            conn,
            source_type=args.source_type,
            limit=args.batch_size,
            offset=offset,
            skip_existing=not args.force
        )
        conn.close()
        
        # Exit if no more items
        if not content_items:
            break
        
        # Process batch
        logger.info(f"Processing batch of {len(content_items)} items (offset {offset})...")
        
        batch_total, batch_successful, batch_chunks = process_content_items(
            content_items,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap
        )
        
        # Update counters
        total_processed += batch_total
        total_successful += batch_successful
        total_chunks += batch_chunks
        
        # Update offset for next batch
        offset += len(content_items)
        
        # Exit if limit reached
        if args.limit and total_processed >= args.limit:
            break
    
    # Log final statistics
    logger.info("Embedding generation complete!")
    logger.info(f"Total items processed: {total_processed}")
    logger.info(f"Successfully processed: {total_successful}")
    logger.info(f"Total chunks generated: {total_chunks}")
    
if __name__ == "__main__":
    main() 

================================================================================
File: github_collector.py
================================================================================

"""
GitHub Repository Collector Module
Collects and processes AI/ML repositories from GitHub for the knowledge base
"""
import os
import json
import logging
import time
import base64
import sqlite3
import requests
from datetime import datetime, timedelta
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'github_collector.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('github_collector')

# List of repositories to collect (used as fallback if search doesn't yield enough results)
GITHUB_REPOS = [
    # Foundational ML/DL libraries
    'tensorflow/tensorflow',
    'pytorch/pytorch',
    'scikit-learn/scikit-learn',
    'huggingface/transformers',
    'keras-team/keras',
    
    # LLM & generative AI projects
    'openai/whisper',
    'facebookresearch/llama',
    'anthropics/claude-api',
    'google/gemma',
    'mistralai/mistral-src',
    
    # Training & infrastructure tools
    'ray-project/ray',
    'microsoft/DeepSpeed',
    'google/jax',
    
    # Research implementations
    'facebookresearch/fairseq',
    'openai/CLIP',
    'LAION-AI/Open-Assistant',
    
    # Learning resources
    'datawhalechina/pumpkin-book',
    'afshinea/stanford-cs-229-machine-learning',
    'microsoft/ML-For-Beginners'
]

def get_github_session():
    """Create a requests session with GitHub API token if available"""
    session = requests.Session()
    
    # Add GitHub API token if available
    github_token = config.GITHUB_CONFIG.get('api_token', '')
    if github_token:
        session.headers.update({
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        })
    else:
        session.headers.update({
            'Accept': 'application/vnd.github.v3+json'
        })
    
    # Set user agent
    session.headers.update({
        'User-Agent': 'AI-Knowledge-Base-Collector/1.0'
    })
    
    return session

def get_rate_limit_info(session):
    """Get the current GitHub API rate limit information"""
    try:
        response = session.get('https://api.github.com/rate_limit')
        if response.status_code == 200:
            data = response.json()
            core = data.get('resources', {}).get('core', {})
            remaining = core.get('remaining', 0)
            reset_timestamp = core.get('reset', 0)
            reset_time = datetime.fromtimestamp(reset_timestamp)
            
            logger.info(f"GitHub API rate limit: {remaining} requests remaining, resets at {reset_time}")
            return remaining, reset_time
        else:
            logger.warning(f"Failed to get rate limit info: {response.status_code}")
            return None, None
    except Exception as e:
        logger.error(f"Error getting rate limit info: {str(e)}")
        return None, None

def wait_for_rate_limit(session):
    """Check rate limit and wait if necessary"""
    remaining, reset_time = get_rate_limit_info(session)
    
    if remaining is None or reset_time is None:
        # If we can't get rate limit info, wait a conservative amount of time
        logger.warning("Could not get rate limit info, waiting 60 seconds")
        time.sleep(60)
        return
    
    if remaining <= 10:  # Keep a small buffer
        now = datetime.now()
        wait_seconds = (reset_time - now).total_seconds() + 5  # Add a small buffer
        
        if wait_seconds > 0:
            logger.warning(f"Rate limit almost reached. Waiting {wait_seconds:.1f} seconds until reset")
            time.sleep(wait_seconds)
        else:
            # If the reset time is in the past, wait a bit anyway
            logger.warning("Rate limit reset time is in the past, waiting 10 seconds as precaution")
            time.sleep(10)

def search_github_repos(session, topics, min_stars=1000, per_page=30, max_results=100):
    """Search for repositories based on topics and minimum stars"""
    repos = []
    topic_query = ' '.join([f'topic:{topic}' for topic in topics])
    query = f"{topic_query} stars:>={min_stars}"
    
    logger.info(f"Searching GitHub with query: {query}")
    
    page = 1
    while len(repos) < max_results:
        wait_for_rate_limit(session)
        
        try:
            response = session.get(
                'https://api.github.com/search/repositories',
                params={
                    'q': query,
                    'sort': 'stars',
                    'order': 'desc',
                    'per_page': per_page,
                    'page': page
                }
            )
            
            if response.status_code != 200:
                logger.error(f"GitHub search API returned error: {response.status_code} - {response.text}")
                break
                
            data = response.json()
            items = data.get('items', [])
            
            if not items:
                break
                
            repos.extend(items)
            total_count = data.get('total_count', 0)
            logger.info(f"Found {len(repos)}/{total_count} repositories (page {page})")
            
            if len(repos) >= max_results:
                logger.info(f"Reached maximum results limit of {max_results}")
                break
                
            # Check if we've reached the last page
            if len(items) < per_page:
                break
                
            page += 1
            
            # Add a small delay between requests
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"Error searching GitHub repositories: {str(e)}")
            break
    
    return repos[:max_results]

def get_repo_info(session, repo_full_name):
    """Get detailed information about a repository"""
    wait_for_rate_limit(session)
    
    try:
        logger.info(f"Getting info for repository: {repo_full_name}")
        response = session.get(f'https://api.github.com/repos/{repo_full_name}')
        
        if response.status_code != 200:
            logger.error(f"GitHub API returned error for {repo_full_name}: {response.status_code} - {response.text}")
            return None
            
        return response.json()
    except Exception as e:
        logger.error(f"Error getting repository info for {repo_full_name}: {str(e)}")
        return None

def get_repo_readme(session, repo_full_name):
    """Get repository README content"""
    wait_for_rate_limit(session)
    
    try:
        logger.info(f"Getting README for repository: {repo_full_name}")
        response = session.get(f'https://api.github.com/repos/{repo_full_name}/readme')
        
        if response.status_code != 200:
            logger.warning(f"Could not find README for {repo_full_name}: {response.status_code}")
            return None
            
        data = response.json()
        content = data.get('content', '')
        encoding = data.get('encoding', 'base64')
        
        if content and encoding == 'base64':
            readme_content = base64.b64decode(content).decode('utf-8', errors='replace')
            
            # Limit readme length if necessary
            max_length = config.GITHUB_CONFIG.get('readme_max_length', 100000)
            if len(readme_content) > max_length:
                readme_content = readme_content[:max_length] + "... [truncated]"
                
            return readme_content
        
        return None
    except Exception as e:
        logger.error(f"Error getting README for {repo_full_name}: {str(e)}")
        return None

def should_update_repo(conn, repo_full_name):
    """Check if a repository should be updated based on last crawl time"""
    cursor = conn.cursor()
    
    try:
        # Check if the repo exists in the database
        cursor.execute("SELECT last_crawled FROM github_repos WHERE full_name = ?", (repo_full_name,))
        result = cursor.fetchone()
        
        if not result:
            # Repository not in database, should add it
            return True
            
        last_crawled = result[0]
        if not last_crawled:
            return True
            
        # Convert string to datetime
        last_crawled_dt = datetime.fromisoformat(last_crawled.replace('Z', '+00:00'))
        
        # Check if it's time to update
        update_frequency_days = config.GITHUB_CONFIG.get('update_frequency_days', 7)
        update_threshold = datetime.now() - timedelta(days=update_frequency_days)
        
        return last_crawled_dt < update_threshold
        
    except Exception as e:
        logger.error(f"Error checking if repo {repo_full_name} should be updated: {str(e)}")
        return True

def store_repo_in_db(conn, repo_data, readme):
    """Store repository information in the database"""
    cursor = conn.cursor()
    
    try:
        # Extract fields to store
        repo_full_name = repo_data.get('full_name')
        
        # Check if repo exists in database
        cursor.execute("SELECT id FROM github_repos WHERE full_name = ?", (repo_full_name,))
        existing_repo = cursor.fetchone()
        
        # Prepare data for insertion/update
        repo_values = {
            'id': repo_data.get('id'),
            'name': repo_data.get('name'),
            'full_name': repo_full_name,
            'description': repo_data.get('description') or '',
            'url': repo_data.get('html_url'),
            'stars': repo_data.get('stargazers_count', 0),
            'watchers': repo_data.get('watchers_count', 0),
            'forks': repo_data.get('forks_count', 0),
            'language': repo_data.get('language') or '',
            'last_push': repo_data.get('pushed_at'),
            'created_at': repo_data.get('created_at'),
            'updated_at': repo_data.get('updated_at'),
            'topics': json.dumps(repo_data.get('topics', [])),
            'readme': readme or '',
            'last_crawled': datetime.now().isoformat()
        }
        
        if existing_repo:
            # Update existing repo
            placeholders = ', '.join([f"{key} = ?" for key in repo_values.keys()])
            query = f"UPDATE github_repos SET {placeholders} WHERE full_name = ?"
            values = list(repo_values.values()) + [repo_full_name]
            cursor.execute(query, values)
            logger.info(f"Updated repository {repo_full_name} in database")
            
            # Get the repo ID for ai_content relation
            repo_id = existing_repo[0]
        else:
            # Insert new repo
            placeholders = ', '.join(['?'] * len(repo_values))
            columns = ', '.join(repo_values.keys())
            query = f"INSERT INTO github_repos ({columns}) VALUES ({placeholders})"
            cursor.execute(query, list(repo_values.values()))
            logger.info(f"Added new repository {repo_full_name} to database")
            
            # Get the inserted repo ID
            repo_id = cursor.lastrowid
        
        # Add entry to ai_content table
        content_values = {
            'title': repo_data.get('name'),
            'description': repo_data.get('description') or '',
            'content': readme or '',
            'source_type_id': 2,  # 2 = GitHub
            'source_id': str(repo_id),
            'url': repo_data.get('html_url'),
            'date_created': repo_data.get('created_at'),
            'date_collected': datetime.now().isoformat(),
            'metadata': json.dumps({
                'stars': repo_data.get('stargazers_count', 0),
                'language': repo_data.get('language') or '',
                'topics': repo_data.get('topics', []),
                'forks': repo_data.get('forks_count', 0)
            })
        }
        
        # Check if content already exists
        cursor.execute(
            "SELECT id FROM ai_content WHERE source_type_id = 2 AND source_id = ?", 
            (str(repo_id),)
        )
        existing_content = cursor.fetchone()
        
        if existing_content:
            # Update existing content
            placeholders = ', '.join([f"{key} = ?" for key in content_values.keys()])
            query = f"UPDATE ai_content SET {placeholders} WHERE source_type_id = 2 AND source_id = ?"
            values = list(content_values.values()) + [str(repo_id)]
            cursor.execute(query, values)
        else:
            # Insert new content
            placeholders = ', '.join(['?'] * len(content_values))
            columns = ', '.join(content_values.keys())
            query = f"INSERT INTO ai_content ({columns}) VALUES ({placeholders})"
            cursor.execute(query, list(content_values.values()))
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        logger.error(f"Error storing repository {repo_data.get('full_name')} in database: {str(e)}")
        return False

def collect_github_repos(max_repos=None):
    """
    Main function to collect GitHub repositories
    Returns the number of successfully processed repositories
    """
    # Check if GitHub collection is enabled
    if not config.CONTENT_SOURCES.get('github', {}).get('enabled', False):
        logger.info("GitHub collection is disabled in configuration")
        return 0
    
    # Create necessary directories
    os.makedirs(config.DATA_DIR, exist_ok=True)
    os.makedirs(os.path.join(config.DATA_DIR, 'logs'), exist_ok=True)
    
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    
    # Initialize session
    session = get_github_session()
    
    # Set default max_repos if not specified
    if max_repos is None:
        max_repos = config.CONTENT_SOURCES.get('github', {}).get('max_repos_per_run', 10)
    
    success_count = 0
    
    try:
        # Check rate limits first
        remaining, reset_time = get_rate_limit_info(session)
        if remaining is not None and remaining < max_repos * 3:  # Each repo might need multiple API calls
            logger.warning(f"Only {remaining} API requests remaining before rate limit reset, which might not be enough")
        
        # Get repositories to process
        repos_to_process = []
        
        # First try to use the search API
        topics = config.CONTENT_SOURCES.get('github', {}).get('topics', ['machine-learning', 'deep-learning'])
        min_stars = config.CONTENT_SOURCES.get('github', {}).get('repo_stars_minimum', 1000)
        
        search_results = search_github_repos(
            session=session,
            topics=topics,
            min_stars=min_stars,
            max_results=max_repos
        )
        
        # Extract repo full names from search results
        for repo in search_results:
            if repo.get('full_name'):
                repos_to_process.append(repo.get('full_name'))
        
        # If we didn't get enough repos from search, add from the predefined list
        if len(repos_to_process) < max_repos:
            remaining_count = max_repos - len(repos_to_process)
            for repo in GITHUB_REPOS:
                if repo not in repos_to_process and len(repos_to_process) < max_repos:
                    repos_to_process.append(repo)
        
        # Process repositories
        logger.info(f"Processing {len(repos_to_process)} repositories")
        
        for i, repo_full_name in enumerate(repos_to_process):
            logger.info(f"Processing repository {i+1}/{len(repos_to_process)}: {repo_full_name}")
            
            # Check if we need to update this repository
            if not should_update_repo(conn, repo_full_name):
                logger.info(f"Skipping {repo_full_name} - recently updated")
                continue
            
            # Get repository information
            repo_data = get_repo_info(session, repo_full_name)
            if not repo_data:
                logger.warning(f"Could not get information for {repo_full_name}, skipping")
                continue
            
            # Get repository README
            readme = get_repo_readme(session, repo_full_name)
            
            # Store in database
            if store_repo_in_db(conn, repo_data, readme):
                success_count += 1
            
            # Add a delay between repositories
            time.sleep(2)
            
            # Check if we've reached the maximum
            if success_count >= max_repos:
                logger.info(f"Reached maximum repository limit of {max_repos}")
                break
        
        logger.info(f"GitHub collection completed. Successfully processed {success_count}/{len(repos_to_process)} repositories")
        
    except Exception as e:
        logger.error(f"Error in GitHub collection process: {str(e)}")
    
    finally:
        # Close database connection
        conn.close()
    
    return success_count

if __name__ == "__main__":
    collect_github_repos() 

================================================================================
File: hybrid_search.py
================================================================================

"""
Hybrid Search Module

This module combines vector search and keyword search to provide more accurate
and relevant search results. It implements adaptive weighting between the two search
techniques based on the query characteristics.
"""
import os
import logging
import sqlite3
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Union

# Import local modules
import config
from embeddings import EmbeddingGenerator
from vector_search import search_by_text, enrich_search_results

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('hybrid_search')

def keyword_search(query: str, top_k: int = 10, 
                  source_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Perform keyword search using SQLite FTS
    
    Args:
        query: The search query
        top_k: Maximum number of results to return
        source_type: Optional filter for source type
        
    Returns:
        List of search results
    """
    conn = None
    try:
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Check if FTS table exists
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='ai_content_fts'
        """)
        
        if not cursor.fetchone():
            logger.warning("FTS table does not exist, creating it...")
            # Create FTS table
            cursor.execute("""
                CREATE VIRTUAL TABLE IF NOT EXISTS ai_content_fts USING fts5(
                    title, description, content, 
                    content='ai_content', content_rowid='id'
                )
            """)
            
            # Populate FTS table
            cursor.execute("""
                INSERT INTO ai_content_fts(rowid, title, description, content)
                SELECT id, title, description, content FROM ai_content
            """)
            
            conn.commit()
        
        # Build search query
        search_terms = " OR ".join([f'"{term}"' for term in query.split()])
        fts_query = f"{search_terms}"
        
        sql_query = """
            SELECT 
                c.id, c.title, c.description, c.date_created, 
                c.source_type_id, st.name as source_type_name,
                snippet(ai_content_fts, 2, '<b>', '</b>', '...', 30) as snippet,
                rank
            FROM ai_content_fts
            JOIN ai_content c ON c.id = ai_content_fts.rowid
            JOIN source_types st ON c.source_type_id = st.id
        """
        
        params = []
        
        # Add source type filter if provided
        if source_type:
            cursor.execute("SELECT id FROM source_types WHERE name = ?", (source_type,))
            source_type_id = cursor.fetchone()
            if source_type_id:
                sql_query += " WHERE c.source_type_id = ?"
                params.append(source_type_id[0])
            else:
                logger.warning(f"Source type '{source_type}' not found")
        
        # Add search condition and order by rank
        sql_query += f""" 
            {'WHERE' if not source_type else 'AND'} ai_content_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        
        params.extend([fts_query, top_k])
        
        # Execute query
        cursor.execute(sql_query, params)
        
        # Process results
        results = []
        for row in cursor.fetchall():
            content_id, title, description, date_created, source_type_id, source_type_name, snippet, rank = row
            
            # Rank is between 0 (best) and 1 (worst), invert for consistency with vector search
            normalized_score = 1.0 - (float(rank) / 1000.0 if float(rank) > 0 else 0)
            
            # Add to results
            results.append({
                'content_id': content_id,
                'title': title,
                'description': description,
                'date_created': date_created,
                'source_type': source_type_name,
                'snippet': snippet,
                'score': normalized_score,
                'search_type': 'keyword'
            })
        
        return results
        
    except Exception as e:
        logger.error(f"Error in keyword search: {str(e)}")
        return []
        
    finally:
        if conn:
            conn.close()

def get_content_chunks(content_id: int, limit: int = 3) -> List[Dict[str, Any]]:
    """
    Get chunks for a content item
    
    Args:
        content_id: The content ID
        limit: Maximum number of chunks to return
        
    Returns:
        List of chunks with text and metadata
    """
    conn = None
    try:
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Get chunks for the content item
        cursor.execute("""
            SELECT 
                ce.chunk_index, ce.chunk_text,
                ac.title, ac.description, ac.source_type_id, st.name as source_type_name
            FROM content_embeddings ce
            JOIN ai_content ac ON ce.content_id = ac.id
            JOIN source_types st ON ac.source_type_id = st.id
            WHERE ce.content_id = ?
            ORDER BY ce.chunk_index
            LIMIT ?
        """, (content_id, limit))
        
        chunks = []
        for row in cursor.fetchall():
            chunk_index, chunk_text, title, description, source_type_id, source_type_name = row
            
            chunks.append({
                'content_id': content_id,
                'chunk_index': chunk_index,
                'chunk_text': chunk_text,
                'title': title,
                'description': description,
                'source_type': source_type_name
            })
        
        return chunks
        
    except Exception as e:
        logger.error(f"Error getting content chunks: {str(e)}")
        return []
        
    finally:
        if conn:
            conn.close()

def determine_weights(query: str) -> Tuple[float, float]:
    """
    Determine weights for vector and keyword search based on query characteristics
    
    Args:
        query: The search query
        
    Returns:
        Tuple of (vector_weight, keyword_weight)
    """
    # Default weights
    default_vector_weight = 0.7
    default_keyword_weight = 0.3
    
    # Check for code indicators
    code_patterns = [
        r'\bcode\b', r'\bfunction\b', r'\bclass\b', r'\bmethod\b',
        r'\bimport\b', r'\bdef\b', r'\bpython\b', r'\bjava\b', 
        r'\bjavascript\b', r'\bjs\b', r'\bc\+\+\b', r'\bsql\b'
    ]
    
    # Check for factual query indicators
    factual_patterns = [
        r'\bwho\b', r'\bwhat\b', r'\bwhen\b', r'\bwhere\b', 
        r'\bhow\b', r'\bwhy\b', r'\bdate\b', r'\byear\b',
        r'\bauthor\b', r'\bcreator\b', r'\bversions?\b'
    ]
    
    # Check for concept query indicators
    concept_patterns = [
        r'\bconcept\b', r'\btheory\b', r'\bframework\b', r'\barchitecture\b',
        r'\bparadigm\b', r'\bpattern\b', r'\bapproach\b', r'\bstrategy\b',
        r'\bexplain\b', r'\bdescribe\b', r'\bcompare\b', r'\banalyze\b'
    ]
    
    # Count matches for each category
    code_matches = sum(1 for pattern in code_patterns if re.search(pattern, query.lower()))
    factual_matches = sum(1 for pattern in factual_patterns if re.search(pattern, query.lower()))
    concept_matches = sum(1 for pattern in concept_patterns if re.search(pattern, query.lower()))
    
    # Adjust weights based on query type
    if code_matches > 0:
        # Code queries benefit from more keyword search
        vector_weight = 0.5
        keyword_weight = 0.5
    elif factual_matches > concept_matches:
        # Factual queries benefit from more keyword search
        vector_weight = 0.6
        keyword_weight = 0.4
    elif concept_matches > 0:
        # Concept queries benefit from more vector search
        vector_weight = 0.8
        keyword_weight = 0.2
    else:
        # Use default weights
        vector_weight = default_vector_weight
        keyword_weight = default_keyword_weight
    
    # Also consider query length and specificity
    query_words = query.split()
    
    if len(query_words) <= 2:
        # Short queries often benefit from more keyword search
        vector_weight = max(0.4, vector_weight - 0.1)
        keyword_weight = 1.0 - vector_weight
    elif len(query_words) >= 6:
        # Longer queries often benefit from more vector search
        vector_weight = min(0.9, vector_weight + 0.1)
        keyword_weight = 1.0 - vector_weight
    
    # Check for exact quotes which indicates keyword preference
    if '"' in query:
        vector_weight = max(0.3, vector_weight - 0.2)
        keyword_weight = 1.0 - vector_weight
    
    return vector_weight, keyword_weight

def hybrid_search(query: str, top_k: int = 5, 
                 source_type: Optional[str] = None,
                 vector_weight: Optional[float] = None,
                 keyword_weight: Optional[float] = None,
                 embedding_generator: Optional[EmbeddingGenerator] = None) -> List[Dict[str, Any]]:
    """
    Perform hybrid search combining vector and keyword search
    
    Args:
        query: The search query
        top_k: Maximum number of results to return
        source_type: Optional filter for source type
        vector_weight: Weight for vector search results (0-1)
        keyword_weight: Weight for keyword search results (0-1)
        embedding_generator: Optional pre-initialized embedding generator
        
    Returns:
        List of search results with combined scores
    """
    try:
        # Determine weights if not provided
        if vector_weight is None or keyword_weight is None:
            vector_weight, keyword_weight = determine_weights(query)
        
        logger.info(f"Hybrid search for '{query}' with weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
        
        # Double the top_k for individual searches to have more candidates to combine
        expanded_top_k = top_k * 2
        
        # Perform vector search
        vector_results = search_by_text(
            query_text=query,
            top_k=expanded_top_k,
            source_type=source_type,
            embedding_generator=embedding_generator
        )
        
        # Convert vector results format
        formatted_vector_results = []
        for result in vector_results:
            formatted_vector_results.append({
                'content_id': result['content_id'],
                'chunk_index': result.get('chunk_index', 0),
                'score': result['similarity'],
                'chunk_text': result.get('chunk_text', ''),
                'title': result.get('title', ''),
                'source_type': result.get('source_type', ''),
                'search_type': 'vector'
            })
        
        # Perform keyword search
        keyword_results = keyword_search(
            query=query,
            top_k=expanded_top_k,
            source_type=source_type
        )
        
        # Combine results
        all_results = {}
        
        # Add vector results
        for result in formatted_vector_results:
            content_id = result['content_id']
            # Use the content_id as the key to avoid duplicates
            if content_id not in all_results:
                all_results[content_id] = {
                    'content_id': content_id,
                    'vector_score': result['score'],
                    'keyword_score': 0.0,
                    'chunk_text': result.get('chunk_text', ''),
                    'title': result.get('title', ''),
                    'source_type': result.get('source_type', ''),
                    'has_vector_match': True,
                    'has_keyword_match': False,
                    'search_type': 'hybrid'
                }
            else:
                # Update if the new vector score is higher
                if result['score'] > all_results[content_id]['vector_score']:
                    all_results[content_id]['vector_score'] = result['score']
                    all_results[content_id]['chunk_text'] = result.get('chunk_text', '')
        
        # Add keyword results
        for result in keyword_results:
            content_id = result['content_id']
            if content_id not in all_results:
                # Get chunks for this content
                chunks = get_content_chunks(content_id, limit=1)
                chunk_text = chunks[0]['chunk_text'] if chunks else ''
                
                all_results[content_id] = {
                    'content_id': content_id,
                    'vector_score': 0.0,
                    'keyword_score': result['score'],
                    'chunk_text': chunk_text,
                    'title': result.get('title', ''),
                    'snippet': result.get('snippet', ''),
                    'source_type': result.get('source_type', ''),
                    'has_vector_match': False,
                    'has_keyword_match': True,
                    'search_type': 'hybrid'
                }
            else:
                # Update keyword score and flag
                all_results[content_id]['keyword_score'] = result['score']
                all_results[content_id]['has_keyword_match'] = True
                all_results[content_id]['snippet'] = result.get('snippet', '')
        
        # Calculate combined scores
        for content_id, result in all_results.items():
            result['combined_score'] = (
                vector_weight * result['vector_score'] + 
                keyword_weight * result['keyword_score']
            )
        
        # Convert to list and sort by combined score
        results_list = list(all_results.values())
        results_list.sort(key=lambda x: x['combined_score'], reverse=True)
        
        # Add query to results
        for result in results_list:
            result['query'] = query
        
        # Return top k results
        top_results = results_list[:top_k]
        
        # Enrich the top results with additional metadata
        return enrich_search_results(top_results)
        
    except Exception as e:
        logger.error(f"Error in hybrid search: {str(e)}")
        return []

def adjust_weights_from_feedback(query: str, result_id: int, 
                                feedback: str, 
                                weight_history: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Adjust search weights based on user feedback
    
    Args:
        query: The search query
        result_id: The ID of the result that received feedback
        feedback: The feedback ('relevant' or 'not_relevant')
        weight_history: Optional history of weight adjustments
        
    Returns:
        Updated weight history
    """
    # Initialize weight history if not provided
    if weight_history is None:
        weight_history = {
            'queries': {},
            'default_vector_weight': 0.7,
            'default_keyword_weight': 0.3,
        }
    
    # Normalize query for consistent keys
    normalized_query = query.lower().strip()
    query_type = classify_query_type(query)
    
    # Initialize query entry if it doesn't exist
    if normalized_query not in weight_history['queries']:
        weight_history['queries'][normalized_query] = {
            'vector_weight': weight_history.get('default_vector_weight', 0.7),
            'keyword_weight': weight_history.get('default_keyword_weight', 0.3),
            'feedback_count': 0,
            'query_type': query_type
        }
    
    # Get current weights
    current = weight_history['queries'][normalized_query]
    
    # Adjust weights based on feedback and result type
    conn = None
    try:
        # Connect to DB to get information about the result
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Check if the result exists in vector search
        cursor.execute("""
            SELECT COUNT(*) FROM content_embeddings
            WHERE content_id = ?
        """, (result_id,))
        
        has_vector_match = cursor.fetchone()[0] > 0
        
        # Check if result exists in keyword search (FTS)
        cursor.execute("""
            SELECT COUNT(*) FROM ai_content_fts
            WHERE rowid = ?
        """, (result_id,))
        
        has_keyword_match = cursor.fetchone()[0] > 0
        
        # Adjust weights
        adjustment = 0.05  # Small adjustment step
        
        if feedback == 'relevant':
            # Positive feedback - increase weight for the type that matched
            if has_vector_match and not has_keyword_match:
                # Only vector matched, increase vector weight
                current['vector_weight'] = min(0.95, current['vector_weight'] + adjustment)
                current['keyword_weight'] = 1.0 - current['vector_weight']
            elif has_keyword_match and not has_vector_match:
                # Only keyword matched, increase keyword weight
                current['keyword_weight'] = min(0.95, current['keyword_weight'] + adjustment)
                current['vector_weight'] = 1.0 - current['keyword_weight']
            # If both matched, no adjustment needed
        elif feedback == 'not_relevant':
            # Negative feedback - decrease weight for the type that matched
            if has_vector_match and not has_keyword_match:
                # Only vector matched, decrease vector weight
                current['vector_weight'] = max(0.05, current['vector_weight'] - adjustment)
                current['keyword_weight'] = 1.0 - current['vector_weight']
            elif has_keyword_match and not has_vector_match:
                # Only keyword matched, decrease keyword weight
                current['keyword_weight'] = max(0.05, current['keyword_weight'] - adjustment)
                current['vector_weight'] = 1.0 - current['keyword_weight']
            # If both matched, no adjustment needed
        
        # Increment feedback count
        current['feedback_count'] += 1
        
        # Update query type specific defaults
        if current['feedback_count'] >= 3:
            # Update default weights for this query type
            query_type_key = f"default_{query_type}_vector_weight"
            if query_type_key not in weight_history:
                weight_history[query_type_key] = weight_history['default_vector_weight']
            
            # Update with running average
            weight_history[query_type_key] = (
                weight_history[query_type_key] * 0.8 + 
                current['vector_weight'] * 0.2
            )
        
        return weight_history
        
    except Exception as e:
        logger.error(f"Error adjusting weights from feedback: {str(e)}")
        return weight_history
        
    finally:
        if conn:
            conn.close()

def classify_query_type(query: str) -> str:
    """
    Classify the query type based on its characteristics
    
    Args:
        query: The search query
        
    Returns:
        Query type classification
    """
    query_lower = query.lower()
    
    # Check for code queries
    code_patterns = [
        r'\bcode\b', r'\bfunction\b', r'\bclass\b', r'\bmethod\b',
        r'\bimport\b', r'\bdef\b', r'\bpython\b', r'\bjava\b', 
        r'\bjavascript\b', r'\bjs\b', r'\bc\+\+\b', r'\bsql\b'
    ]
    
    # Check for factual queries
    factual_patterns = [
        r'\bwho\b', r'\bwhat\b', r'\bwhen\b', r'\bwhere\b', 
        r'\bhow\b', r'\bwhy\b', r'\bdate\b', r'\byear\b',
        r'\bauthor\b', r'\bcreator\b', r'\bversions?\b'
    ]
    
    # Check for concept queries
    concept_patterns = [
        r'\bconcept\b', r'\btheory\b', r'\bframework\b', r'\barchitecture\b',
        r'\bparadigm\b', r'\bpattern\b', r'\bapproach\b', r'\bstrategy\b',
        r'\bexplain\b', r'\bdescribe\b', r'\bcompare\b', r'\banalyze\b'
    ]
    
    # Count matches for each category
    code_matches = sum(1 for pattern in code_patterns if re.search(pattern, query_lower))
    factual_matches = sum(1 for pattern in factual_patterns if re.search(pattern, query_lower))
    concept_matches = sum(1 for pattern in concept_patterns if re.search(pattern, query_lower))
    
    # Determine query type
    if code_matches > max(factual_matches, concept_matches):
        return 'code'
    elif factual_matches > concept_matches:
        return 'factual'
    elif concept_matches > 0:
        return 'concept'
    else:
        # Check for other characteristics
        if len(query.split()) >= 6:
            return 'long'
        elif '"' in query:
            return 'exact'
        else:
            return 'general'

def save_weights_history(weight_history: Dict[str, Any], 
                        filepath: str = 'data/search_weights.json') -> None:
    """
    Save weight history to a JSON file
    
    Args:
        weight_history: Weight history dictionary
        filepath: Path to save the JSON file
    """
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # Save to file
        with open(filepath, 'w') as f:
            json.dump(weight_history, f, indent=2)
            
        logger.info(f"Weight history saved to {filepath}")
            
    except Exception as e:
        logger.error(f"Error saving weight history: {str(e)}")

def load_weights_history(filepath: str = 'data/search_weights.json') -> Dict[str, Any]:
    """
    Load weight history from a JSON file
    
    Args:
        filepath: Path to the JSON file
        
    Returns:
        Weight history dictionary
    """
    default_history = {
        'queries': {},
        'default_vector_weight': 0.7,
        'default_keyword_weight': 0.3,
        'default_code_vector_weight': 0.5,
        'default_factual_vector_weight': 0.6,
        'default_concept_vector_weight': 0.8,
        'default_long_vector_weight': 0.75,
        'default_exact_vector_weight': 0.4,
        'default_general_vector_weight': 0.7,
        'last_updated': datetime.now().isoformat()
    }
    
    try:
        # Check if file exists
        if not os.path.exists(filepath):
            logger.info(f"Weight history file not found, using defaults")
            return default_history
        
        # Load from file
        with open(filepath, 'r') as f:
            history = json.load(f)
            
        logger.info(f"Weight history loaded from {filepath}")
        
        # Update last loaded time
        history['last_updated'] = datetime.now().isoformat()
        
        return history
            
    except Exception as e:
        logger.error(f"Error loading weight history: {str(e)}")
        return default_history

def main():
    """Main function for direct script execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Hybrid search for content")
    parser.add_argument("query", help="Query text")
    parser.add_argument("--top-k", type=int, default=5, help="Number of results to show")
    parser.add_argument("--source-type", help="Filter by source type")
    parser.add_argument("--vector-weight", type=float, help="Weight for vector search (0-1)")
    parser.add_argument("--keyword-weight", type=float, help="Weight for keyword search (0-1)")
    parser.add_argument("--adaptive", action="store_true", help="Use adaptive weights based on query")
    
    args = parser.parse_args()
    
    # Set weights
    vector_weight = args.vector_weight
    keyword_weight = args.keyword_weight
    
    # If adaptive or weights not provided, determine automatically
    if args.adaptive or (vector_weight is None and keyword_weight is None):
        # Load weight history
        weight_history = load_weights_history()
        
        # Get query type
        query_type = classify_query_type(args.query)
        
        # Use query-specific weights if available
        normalized_query = args.query.lower().strip()
        if normalized_query in weight_history['queries']:
            vector_weight = weight_history['queries'][normalized_query]['vector_weight']
            keyword_weight = weight_history['queries'][normalized_query]['keyword_weight']
            print(f"Using query-specific weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
        else:
            # Use query type defaults
            type_key = f"default_{query_type}_vector_weight"
            if type_key in weight_history:
                vector_weight = weight_history[type_key]
                keyword_weight = 1.0 - vector_weight
                print(f"Using {query_type} query type weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
            else:
                # Determine weights based on query
                vector_weight, keyword_weight = determine_weights(args.query)
                print(f"Using determined weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
    
    # Perform search
    results = hybrid_search(
        query=args.query,
        top_k=args.top_k,
        source_type=args.source_type,
        vector_weight=vector_weight,
        keyword_weight=keyword_weight
    )
    
    # Display results
    print(f"\nHybrid search results for: {args.query}")
    print(f"Weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
    print("=" * 80)
    
    for i, result in enumerate(results):
        print(f"\nResult {i+1} - Combined Score: {result.get('combined_score', 0):.4f}")
        print(f"Vector Score: {result.get('vector_score', 0):.4f}, Keyword Score: {result.get('keyword_score', 0):.4f}")
        print(f"Title: {result.get('title', 'Unknown')}")
        print(f"Source: {result.get('source_type', 'Unknown')}")
        print(f"Content ID: {result['content_id']}")
        print("-" * 40)
        
        # Show snippet or chunk text
        if 'snippet' in result and result['snippet']:
            print(f"Keyword Match: {result['snippet']}")
        
        if 'chunk_text' in result and result['chunk_text']:
            text = result['chunk_text']
            print(text[:300] + "..." if len(text) > 300 else text)
        
        print("-" * 40)
        
        # Display concepts if available
        if 'concepts' in result:
            print("Concepts:")
            for concept in result['concepts'][:5]:  # Show top 5 concepts
                print(f"- {concept['name']} ({concept['category']}, {concept['importance']})")
        
        print()
    
    return results

if __name__ == "__main__":
    main() 

================================================================================
File: indexer.py
================================================================================

"""
Module for indexing transcribed content into the knowledge base
"""
import os
import json
import glob
import sqlite3
import logging
from tqdm import tqdm

from config import (
    DB_PATH,
    TRANSCRIPT_DIR
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('indexer')

def setup_database():
    """Ensure the database is set up correctly"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Use the updated schema with summary field and FTS4
    cursor.executescript('''
    -- Content table stores all video data
    CREATE TABLE IF NOT EXISTS videos (
        id INTEGER PRIMARY KEY,
        shortcode TEXT UNIQUE,
        account TEXT,
        filename TEXT,
        caption TEXT,
        transcript TEXT,
        summary TEXT,
        timestamp TEXT,
        download_date TEXT,
        url TEXT,
        likes INTEGER,
        comments INTEGER,
        word_count INTEGER,
        duration_seconds INTEGER,
        key_phrases TEXT
    );
    
    -- Tags table for improved filtering
    CREATE TABLE IF NOT EXISTS tags (
        id INTEGER PRIMARY KEY,
        video_id INTEGER,
        tag TEXT,
        FOREIGN KEY (video_id) REFERENCES videos(id)
    );
    
    -- Improved indexing for better performance
    CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
    CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
    CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);
    
    -- Virtual FTS4 table for full-text search
    CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
        shortcode,
        account,
        caption,
        transcript,
        summary,
        timestamp,
        content=videos,
        tokenize=porter
    );
    
    -- Create triggers to keep FTS table synchronized
    CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    ''')
    
    conn.commit()
    conn.close()
    
    logger.info("Database setup complete")

def extract_tags_from_caption(caption):
    """Extract hashtags from captions"""
    if not caption:
        return []
    
    words = caption.split()
    tags = [word[1:] for word in words if word.startswith('#')]
    return tags

def calculate_word_count(text):
    """Calculate word count from text"""
    if not text:
        return 0
    return len(text.split())

def index_transcripts():
    """Index all transcripts into the database"""
    setup_database()
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Get all transcript files
    all_transcripts = []
    for root, _, _ in os.walk(TRANSCRIPT_DIR):
        transcripts = glob.glob(os.path.join(root, "*.json"))
        all_transcripts.extend(transcripts)
    
    logger.info(f"Found {len(all_transcripts)} transcripts to index")
    
    # Process each transcript
    new_count = 0
    updated_count = 0
    
    for transcript_path in tqdm(all_transcripts, desc="Indexing transcripts"):
        try:
            with open(transcript_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract relevant fields, using empty strings for missing data
            shortcode = data.get("shortcode", "")
            account = data.get("account", "")
            filename = data.get("filename", "")
            caption = data.get("caption", "")
            transcript_text = data.get("text", "")
            timestamp = data.get("timestamp", "")
            download_date = data.get("download_date", "")
            url = data.get("url", "")
            likes = data.get("likes", 0)
            comments = data.get("comments", 0)
            
            # Calculate word count
            word_count = calculate_word_count(transcript_text)
            
            # Duration (if available)
            duration_seconds = data.get("duration_seconds", None)
            
            # Initially no summary or key phrases
            summary = ""
            key_phrases = ""
            
            # Check if this video is already in the database
            cursor.execute("SELECT id, summary, word_count FROM videos WHERE shortcode = ?", (shortcode,))
            result = cursor.fetchone()
            
            if result:
                # Update existing record (preserve summary if it exists)
                video_id = result[0]
                existing_summary = result[1] or ""
                existing_word_count = result[2] or 0
                
                cursor.execute('''
                UPDATE videos SET
                    account = ?,
                    filename = ?,
                    caption = ?,
                    transcript = ?,
                    timestamp = ?,
                    download_date = ?,
                    url = ?,
                    likes = ?,
                    comments = ?,
                    word_count = ?,
                    duration_seconds = ?,
                    key_phrases = ?
                WHERE id = ?
                ''', (account, filename, caption, transcript_text, timestamp, 
                      download_date, url, likes, comments, word_count,
                      duration_seconds, key_phrases, video_id))
                updated_count += 1
                
                # Remove old tags
                cursor.execute("DELETE FROM tags WHERE video_id = ?", (video_id,))
                
            else:
                # Insert new record
                cursor.execute('''
                INSERT INTO videos (
                    shortcode, account, filename, caption, transcript, summary,
                    timestamp, download_date, url, likes, comments, 
                    word_count, duration_seconds, key_phrases
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (shortcode, account, filename, caption, transcript_text, summary,
                      timestamp, download_date, url, likes, comments, 
                      word_count, duration_seconds, key_phrases))
                video_id = cursor.lastrowid
                new_count += 1
            
            # Extract and save tags
            tags = extract_tags_from_caption(caption)
            for tag in tags:
                cursor.execute('''
                INSERT INTO tags (video_id, tag) VALUES (?, ?)
                ''', (video_id, tag))
            
            # Commit every 100 records
            if (new_count + updated_count) % 100 == 0:
                conn.commit()
                
        except Exception as e:
            logger.error(f"Error indexing {transcript_path}: {str(e)}")
    
    # Final commit
    conn.commit()
    conn.close()
    
    logger.info(f"Indexing complete. Added {new_count} new records, updated {updated_count} existing records.")

if __name__ == "__main__":
    index_transcripts() 

================================================================================
File: init_db.py
================================================================================

"""
Initialize the SQLite database with proper schema
"""
import os
import sqlite3
import logging

from config import DB_PATH, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('init_db.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('init_db')

def init_database():
    """Initialize the SQLite database with proper schema"""
    # Ensure data directory exists
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logger.info(f"Initializing database at {DB_PATH}")
    
    # Create or connect to database
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Read SQL schema from file
    schema_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "create_db.sql")
    
    try:
        with open(schema_path, 'r') as f:
            schema_sql = f.read()
        
        # Execute schema script
        cursor.executescript(schema_sql)
        logger.info("Successfully executed schema script")
        
    except FileNotFoundError:
        logger.warning(f"Schema file not found at {schema_path}. Using embedded schema.")
        
        # Execute embedded schema if file not found
        cursor.executescript('''
        -- Content table stores all video data
        CREATE TABLE IF NOT EXISTS videos (
            id INTEGER PRIMARY KEY,
            shortcode TEXT UNIQUE,
            account TEXT,
            filename TEXT,
            caption TEXT,
            transcript TEXT,
            summary TEXT,
            timestamp TEXT,
            download_date TEXT,
            url TEXT,
            likes INTEGER,
            comments INTEGER,
            word_count INTEGER,
            duration_seconds INTEGER,
            key_phrases TEXT
        );

        -- Tags table for improved filtering
        CREATE TABLE IF NOT EXISTS tags (
            id INTEGER PRIMARY KEY,
            video_id INTEGER,
            tag TEXT,
            FOREIGN KEY (video_id) REFERENCES videos(id)
        );

        -- Improved indexing for better performance
        CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
        CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
        CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

        -- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
        CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
            shortcode,
            account,
            caption,
            transcript,
            summary,
            timestamp,
            content=videos,
            tokenize=porter
        );

        -- Create triggers to keep FTS table synchronized
        CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;

        CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
        END;

        CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;
        ''')
        logger.info("Successfully executed embedded schema")
    
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        raise
    
    # Commit changes and close connection
    conn.commit()
    conn.close()
    
    logger.info(f"Database initialized at {DB_PATH}")
    logger.info("You can now run the pipeline to populate the database with your video data.")

if __name__ == "__main__":
    init_database() 

================================================================================
File: README_VECTOR_SEARCH.md
================================================================================

# Vector Search in Instagram Knowledge Base

This document explains how to use the vector search capabilities of the Instagram Knowledge Base system. Vector search provides semantic search capabilities, allowing users to find content based on meaning rather than just keywords.

## Installation

First, make sure you have all the necessary dependencies installed:

```bash
pip install -r requirements.txt
```

Some additional dependencies for vector search might require specific system packages. If you encounter any issues during installation, check the error messages for specific requirements.

## Database Migration

Before using vector search, make sure your database is migrated to the latest schema:

```bash
python run.py --migrate
```

This will create all necessary tables, including:
- `content_embeddings`: Stores vector embeddings for content chunks
- `ai_content`: Unified content storage across different source types
- `ai_content_fts`: Full-text search virtual table for keyword search

## Generating Embeddings

Before you can search, you need to generate embeddings for your content:

```bash
python run.py --generate-embeddings
```

This will generate vector embeddings for all content in your database. You can customize the process with these options:

- `--embeddings-source [research_paper|github|instagram]`: Only generate embeddings for a specific content type
- `--embeddings-limit N`: Process only N content items
- `--embeddings-batch N`: Process in batches of N items (default: 50)
- `--embeddings-chunk N`: Set chunk size in characters (default: 500)
- `--embeddings-overlap N`: Set chunk overlap in characters (default: 100)
- `--embeddings-force`: Regenerate embeddings for content that already has them

Example:
```bash
python run.py --generate-embeddings --embeddings-source research_paper --embeddings-limit 100 --embeddings-chunk 1000 --embeddings-overlap 200
```

## Performing Searches

### Vector Search

Vector search finds content semantically similar to the query:

```bash
python run.py --vector-search "deep learning techniques for image classification"
```

Options:
- `--search-top-k N`: Return top N results (default: 5)
- `--search-source [research_paper|github|instagram]`: Search only in a specific content type
- `--in-memory-index`: Use in-memory indexing for faster searches (uses more RAM)

### Hybrid Search

Hybrid search combines vector search with keyword search for better results:

```bash
python run.py --hybrid-search "deep learning techniques for image classification"
```

Options:
- `--search-top-k N`: Return top N results (default: 5)
- `--search-source [research_paper|github|instagram]`: Search only in a specific content type
- `--vector-weight N`: Weight for vector search component (0-1)
- `--keyword-weight N`: Weight for keyword search component (0-1)
- `--adaptive-weights`: Use adaptive weighting based on query type (default: true)

If you don't specify weights, the system will automatically determine appropriate weights based on the query characteristics.

## Query Types and Weight Adaptation

The system classifies queries into several types and applies different vector-to-keyword weights:

1. **Code Queries** (e.g., "python function for sorting arrays"): 0.5:0.5
2. **Factual Queries** (e.g., "when was transformer architecture introduced"): 0.6:0.4
3. **Concept Queries** (e.g., "explain attention mechanism in transformers"): 0.8:0.2
4. **Short Queries** (1-2 words): Reduce vector weight by 0.1
5. **Long Queries** (6+ words): Increase vector weight by 0.1
6. **Exact Match Queries** (with quotes): Reduce vector weight by 0.2

The system learns from user feedback to improve these weights over time.

## Advanced: Directly Using the Modules

For more advanced usage, you can import the modules directly in your code:

```python
from embeddings import EmbeddingGenerator
from vector_search import search_by_text
from hybrid_search import hybrid_search

# Generate embeddings for a query
embedding_generator = EmbeddingGenerator()
query_embedding = embedding_generator.generate_embedding("your query text")

# Perform vector search
vector_results = search_by_text(
    query_text="your query text",
    top_k=5,
    embedding_generator=embedding_generator
)

# Perform hybrid search
hybrid_results = hybrid_search(
    query="your query text",
    top_k=5,
    vector_weight=0.7,
    keyword_weight=0.3
)
```

## Troubleshooting

### Missing Sentence Transformers Model

If you see an error about missing models, the system will fall back to a simpler TF-IDF based embedding. For better results, make sure you have internet access during the first run so the model can be downloaded.

### Performance Issues

- For large databases, use `--in-memory-index` for faster searching
- Adjust chunk size and overlap based on your content type
- Use more specific queries for better results

### No Search Results

- Make sure you've generated embeddings first
- Check if you have content in your database
- Try using broader queries or different source types

## Further Reading

- Learn more about vector search in the code comments in `/home/adi235/MistralOCR/Instagram-Scraper/vector_search.py`
- Check hybrid search implementation in `/home/adi235/MistralOCR/Instagram-Scraper/hybrid_search.py`
- See the chunking strategy in `/home/adi235/MistralOCR/Instagram-Scraper/chunking.py` 

================================================================================
File: README.md
================================================================================

# Instagram-Scraper
Instagram scraper that i created to pass through instagram's strict rate limiting using cheeky tricks


================================================================================
File: run.py
================================================================================

"""
Main script to run the complete Instagram Knowledge Base system
"""
import os
import argparse
import logging
from time import time

# Import our modules
from config import DATA_DIR
import downloader
import transcriber
import indexer
import summarizer
from app import app

# Import the new modules
try:
    import db_migration
    import github_collector
    import arxiv_collector
    import concept_extractor
    import chunking
    import embeddings
    import generate_embeddings
    import vector_search
    import hybrid_search
    has_additional_modules = True
    has_vector_search = True
except ImportError as e:
    has_additional_modules = False
    has_vector_search = False
    import_error = str(e)
    missing_module = str(e).split("No module named ")[-1].strip("'")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('instagram_kb.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('main')

def setup():
    """Setup necessary directories"""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs('logs', exist_ok=True)

def run_downloader():
    """Run the Instagram downloader"""
    logger.info("Starting Instagram content download")
    start_time = time()
    downloader.download_from_instagram()
    logger.info(f"Download completed in {time() - start_time:.2f} seconds")

def run_transcriber():
    """Run the audio extraction and transcription"""
    logger.info("Starting audio extraction and transcription")
    start_time = time()
    transcriber.process_videos()
    logger.info(f"Transcription completed in {time() - start_time:.2f} seconds")

def run_summarizer():
    """Run the transcript summarization using Claude"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time()
    summarizer.summarize_transcripts()
    logger.info(f"Summarization completed in {time() - start_time:.2f} seconds")

def run_indexer():
    """Run the knowledge base indexer"""
    logger.info("Starting indexing of transcripts")
    start_time = time()
    indexer.index_transcripts()
    logger.info(f"Indexing completed in {time() - start_time:.2f} seconds")

def run_web_interface():
    """Run the web interface"""
    logger.info("Starting web interface")
    app.run(host='0.0.0.0', port=5000)

def run_db_migration():
    """Run database migration to support multiple content sources"""
    if not has_additional_modules:
        logger.error("Database migration module not available")
        return
    
    logger.info("Starting database migration")
    start_time = time()
    success = db_migration.migrate_database()
    
    if success:
        logger.info(f"Database migration completed in {time() - start_time:.2f} seconds")
    else:
        logger.error(f"Database migration failed after {time() - start_time:.2f} seconds")

def run_github_collector(max_repos=None):
    """Run GitHub repository collection"""
    if not has_additional_modules:
        logger.error("GitHub collector module not available")
        return
    
    logger.info("Starting GitHub repository collection")
    start_time = time()
    success_count = github_collector.collect_github_repos(max_repos=max_repos)
    logger.info(f"GitHub collection completed in {time() - start_time:.2f} seconds, processed {success_count} repositories")

def run_papers_collector(max_papers=None, force_update=False):
    """Run ArXiv research paper collection"""
    if not has_additional_modules:
        logger.error("ArXiv collector module not available")
        return
    
    logger.info("Starting ArXiv research paper collection")
    start_time = time()
    papers_added = arxiv_collector.collect_papers(max_papers=max_papers, force_update=force_update)
    logger.info(f"ArXiv collection completed in {time() - start_time:.2f} seconds, added {papers_added} new papers")

def run_concept_extractor(limit=None, source_type=None):
    """Run concept extraction on content"""
    if not has_additional_modules:
        logger.error("Concept extractor module not available")
        return
    
    logger.info("Starting concept extraction")
    start_time = time()
    
    if source_type:
        logger.info(f"Processing {limit or 'all'} items from source type: {source_type}")
        processed = concept_extractor.process_unprocessed_content(limit=limit or 5, source_type=source_type)
        logger.info(f"Processed {processed} items from {source_type}")
    else:
        # Process some content from each source type
        total_processed = 0
        for src_type in ["research_paper", "github", "instagram"]:
            logger.info(f"Processing source type: {src_type}")
            processed = concept_extractor.process_unprocessed_content(limit=limit or 3, source_type=src_type)
            logger.info(f"Processed {processed} items from {src_type}")
            total_processed += processed
        
        logger.info(f"Concept extraction completed in {time() - start_time:.2f} seconds, processed {total_processed} items")

def run_embedding_generation(source_type=None, limit=None, batch_size=50, 
                           chunk_size=500, chunk_overlap=100, force=False):
    """Generate embeddings for content"""
    if not has_vector_search:
        logger.error(f"Vector search modules not available: {import_error}")
        return
    
    logger.info("Starting embedding generation")
    start_time = time()
    
    args = []
    if source_type:
        args.extend(["--source-type", source_type])
    
    if limit:
        args.extend(["--limit", str(limit)])
    
    if batch_size:
        args.extend(["--batch-size", str(batch_size)])
    
    if chunk_size:
        args.extend(["--chunk-size", str(chunk_size)])
    
    if chunk_overlap:
        args.extend(["--chunk-overlap", str(chunk_overlap)])
    
    if force:
        args.append("--force")
    
    # Run embedding generator script
    import sys
    old_args = sys.argv
    sys.argv = ["generate_embeddings.py"] + args
    try:
        generate_embeddings.main()
    except Exception as e:
        logger.error(f"Error generating embeddings: {str(e)}")
    finally:
        sys.argv = old_args
    
    logger.info(f"Embedding generation completed in {time() - start_time:.2f} seconds")

def run_vector_search(query, top_k=5, source_type=None, in_memory_index=False):
    """Run vector search for a query"""
    if not has_vector_search:
        logger.error(f"Vector search modules not available: {import_error}")
        return []
    
    logger.info(f"Running vector search for: {query}")
    start_time = time()
    
    try:
        if in_memory_index:
            # Create in-memory index for faster search
            index = vector_search.create_memory_index()
            
            # Create embedding generator
            embedding_generator = embeddings.EmbeddingGenerator()
            
            # Generate query embedding
            query_embedding = embedding_generator.generate_embedding(query)
            
            # Search using in-memory index
            results = vector_search.search_memory_index(query_embedding, index, top_k=top_k)
            
            # Fetch chunk text and enrich results
            results = vector_search.enrich_search_results(results)
        else:
            # Use standard search
            results = vector_search.debug_search(query, top_k=top_k)
        
        logger.info(f"Vector search completed in {time() - start_time:.2f} seconds with {len(results)} results")
        
        # Print results
        print(f"\nVector search results for: {query}")
        print("=" * 80)
        
        for i, result in enumerate(results):
            print(f"\nResult {i+1} - Similarity: {result.get('similarity', 0):.4f}")
            print(f"Title: {result.get('title', 'Unknown')}")
            print(f"Source: {result.get('source_type', 'Unknown')}")
            print(f"Content ID: {result.get('content_id', 'Unknown')}")
            print("-" * 40)
            if 'chunk_text' in result:
                text = result['chunk_text']
                print(text[:300] + "..." if len(text) > 300 else text)
            print("-" * 40)
            
            if 'concepts' in result:
                print("Concepts:")
                for concept in result['concepts'][:5]:
                    print(f"- {concept['name']} ({concept['category']}, {concept['importance']})")
            print()
        
        return results
    
    except Exception as e:
        logger.error(f"Error performing vector search: {str(e)}")
        return []

def run_hybrid_search(query, top_k=5, source_type=None, vector_weight=None, 
                     keyword_weight=None, adaptive=True):
    """Run hybrid search for a query"""
    if not has_vector_search:
        logger.error(f"Vector search modules not available: {import_error}")
        return []
    
    logger.info(f"Running hybrid search for: {query}")
    start_time = time()
    
    try:
        # Load weight history if using adaptive weighting
        weight_history = None
        if adaptive and (vector_weight is None or keyword_weight is None):
            weight_history = hybrid_search.load_weights_history()
            
            # Get query type
            query_type = hybrid_search.classify_query_type(query)
            
            # Use query-specific weights if available
            normalized_query = query.lower().strip()
            if normalized_query in weight_history['queries']:
                vector_weight = weight_history['queries'][normalized_query]['vector_weight']
                keyword_weight = weight_history['queries'][normalized_query]['keyword_weight']
                logger.info(f"Using query-specific weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
            else:
                # Use query type defaults
                type_key = f"default_{query_type}_vector_weight"
                if type_key in weight_history:
                    vector_weight = weight_history[type_key]
                    keyword_weight = 1.0 - vector_weight
                    logger.info(f"Using {query_type} query type weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
        
        # Perform hybrid search
        results = hybrid_search.hybrid_search(
            query=query,
            top_k=top_k,
            source_type=source_type,
            vector_weight=vector_weight,
            keyword_weight=keyword_weight
        )
        
        logger.info(f"Hybrid search completed in {time() - start_time:.2f} seconds with {len(results)} results")
        
        # Get actual weights used for display
        if vector_weight is None or keyword_weight is None:
            vector_weight, keyword_weight = hybrid_search.determine_weights(query)
        
        # Print results
        print(f"\nHybrid search results for: {query}")
        print(f"Weights: vector={vector_weight:.2f}, keyword={keyword_weight:.2f}")
        print("=" * 80)
        
        for i, result in enumerate(results):
            print(f"\nResult {i+1} - Combined Score: {result.get('combined_score', 0):.4f}")
            print(f"Vector Score: {result.get('vector_score', 0):.4f}, Keyword Score: {result.get('keyword_score', 0):.4f}")
            print(f"Title: {result.get('title', 'Unknown')}")
            print(f"Source: {result.get('source_type', 'Unknown')}")
            print(f"Content ID: {result['content_id']}")
            print("-" * 40)
            
            if 'snippet' in result and result['snippet']:
                print(f"Keyword Match: {result['snippet']}")
            
            if 'chunk_text' in result and result['chunk_text']:
                text = result['chunk_text']
                print(text[:300] + "..." if len(text) > 300 else text)
            
            print("-" * 40)
            
            if 'concepts' in result:
                print("Concepts:")
                for concept in result['concepts'][:5]:
                    print(f"- {concept['name']} ({concept['category']}, {concept['importance']})")
            
            print()
        
        return results
    
    except Exception as e:
        logger.error(f"Error performing hybrid search: {str(e)}")
        return []

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Instagram Knowledge Base')
    parser.add_argument('--download', action='store_true', help='Run the downloader module')
    parser.add_argument('--transcribe', action='store_true', help='Run the transcription module')
    parser.add_argument('--summarize', action='store_true', help='Run the summarization module')
    parser.add_argument('--index', action='store_true', help='Run the indexer module')
    parser.add_argument('--web', action='store_true', help='Run the web interface')
    parser.add_argument('--all', action='store_true', help='Run the complete pipeline')
    
    # Add new arguments for additional modules
    if has_additional_modules:
        parser.add_argument('--migrate', action='store_true', help='Run database migration')
        parser.add_argument('--github', action='store_true', help='Run GitHub repository collection')
        parser.add_argument('--github-max', type=int, help='Maximum number of GitHub repositories to collect')
        parser.add_argument('--papers', action='store_true', help='Run ArXiv research paper collection')
        parser.add_argument('--papers-max', type=int, help='Maximum number of papers to collect')
        parser.add_argument('--force-update', action='store_true', help='Force update of existing content')
        parser.add_argument('--concepts', action='store_true', help='Run AI concept extraction')
        parser.add_argument('--concepts-limit', type=int, help='Maximum number of items to process for concept extraction')
        parser.add_argument('--concepts-source', choices=['research_paper', 'github', 'instagram'], 
                            help='Only extract concepts from this source type')
    
    # Add vector search related arguments
    if has_vector_search:
        parser.add_argument('--generate-embeddings', action='store_true', help='Generate vector embeddings for content')
        parser.add_argument('--embeddings-source', choices=['research_paper', 'github', 'instagram'],
                            help='Generate embeddings only for this source type')
        parser.add_argument('--embeddings-limit', type=int, help='Maximum number of items to process for embedding generation')
        parser.add_argument('--embeddings-batch', type=int, default=50, help='Batch size for embedding generation')
        parser.add_argument('--embeddings-chunk', type=int, default=500, help='Chunk size for embedding generation')
        parser.add_argument('--embeddings-overlap', type=int, default=100, help='Chunk overlap for embedding generation')
        parser.add_argument('--embeddings-force', action='store_true', help='Force regeneration of existing embeddings')
        
        parser.add_argument('--vector-search', help='Run vector search with the provided query')
        parser.add_argument('--hybrid-search', help='Run hybrid search with the provided query')
        parser.add_argument('--search-top-k', type=int, default=5, help='Number of search results to return')
        parser.add_argument('--search-source', choices=['research_paper', 'github', 'instagram'],
                            help='Search only in this source type')
        parser.add_argument('--vector-weight', type=float, help='Weight for vector search (0-1)')
        parser.add_argument('--keyword-weight', type=float, help='Weight for keyword search (0-1)')
        parser.add_argument('--adaptive-weights', action='store_true', help='Use adaptive weights based on query type')
        parser.add_argument('--in-memory-index', action='store_true', help='Use in-memory index for vector search (faster)')
    
    args = parser.parse_args()
    
    # Setup directories
    setup()
    
    # Run requested modules
    if args.all or args.download:
        run_downloader()
    
    if args.all or args.transcribe:
        run_transcriber()
    
    if args.all or args.summarize:
        run_summarizer()
    
    if args.all or args.index:
        run_indexer()
    
    # Run new modules if available
    if has_additional_modules:
        if args.all or args.migrate:
            run_db_migration()
        
        if args.all or args.github:
            max_repos = args.github_max if hasattr(args, 'github_max') else None
            run_github_collector(max_repos=max_repos)
            
        if args.all or args.papers:
            max_papers = args.papers_max if hasattr(args, 'papers_max') else None
            force_update = args.force_update if hasattr(args, 'force_update') else False
            run_papers_collector(max_papers=max_papers, force_update=force_update)
            
        if args.all or args.concepts:
            limit = args.concepts_limit if hasattr(args, 'concepts_limit') else None
            source_type = args.concepts_source if hasattr(args, 'concepts_source') else None
            run_concept_extractor(limit=limit, source_type=source_type)
    
    # Run vector search modules if available
    if has_vector_search:
        if args.generate_embeddings:
            source_type = args.embeddings_source if hasattr(args, 'embeddings_source') else None
            limit = args.embeddings_limit if hasattr(args, 'embeddings_limit') else None
            batch_size = args.embeddings_batch if hasattr(args, 'embeddings_batch') else 50
            chunk_size = args.embeddings_chunk if hasattr(args, 'embeddings_chunk') else 500
            chunk_overlap = args.embeddings_overlap if hasattr(args, 'embeddings_overlap') else 100
            force = args.embeddings_force if hasattr(args, 'embeddings_force') else False
            
            run_embedding_generation(
                source_type=source_type, 
                limit=limit,
                batch_size=batch_size,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                force=force
            )
        
        if args.vector_search:
            top_k = args.search_top_k if hasattr(args, 'search_top_k') else 5
            source_type = args.search_source if hasattr(args, 'search_source') else None
            in_memory_index = args.in_memory_index if hasattr(args, 'in_memory_index') else False
            
            run_vector_search(
                query=args.vector_search,
                top_k=top_k,
                source_type=source_type,
                in_memory_index=in_memory_index
            )
        
        if args.hybrid_search:
            top_k = args.search_top_k if hasattr(args, 'search_top_k') else 5
            source_type = args.search_source if hasattr(args, 'search_source') else None
            vector_weight = args.vector_weight if hasattr(args, 'vector_weight') else None
            keyword_weight = args.keyword_weight if hasattr(args, 'keyword_weight') else None
            adaptive = args.adaptive_weights if hasattr(args, 'adaptive_weights') else True
            
            run_hybrid_search(
                query=args.hybrid_search,
                top_k=top_k,
                source_type=source_type,
                vector_weight=vector_weight,
                keyword_weight=keyword_weight,
                adaptive=adaptive
            )
    
    if args.all or args.web:
        run_web_interface()
    
    # If no arguments provided, show help
    no_args = not (args.download or args.transcribe or args.summarize or 
                  args.index or args.web or args.all)
    
    # Check for new arguments if modules are available
    if has_additional_modules:
        no_args = no_args and not (args.migrate or args.github or args.papers or args.concepts)
    
    # Check for vector search arguments
    if has_vector_search:
        no_args = no_args and not (args.generate_embeddings or args.vector_search or args.hybrid_search)
    
    if no_args:
        parser.print_help()

if __name__ == "__main__":
    main() 

================================================================================
File: setup_db.py
================================================================================



================================================================================
File: summarizer.py
================================================================================

"""
Module for summarizing video transcripts using Claude API
"""
import os
import json
import time
import sqlite3
import logging
import re
from anthropic import Anthropic

from config import DB_PATH, TRANSCRIPT_DIR, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('summarizer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('summarizer')

class ClaudeSummarizer:
    def __init__(self, api_key=None):
        """Initialize the Claude summarizer with API key"""
        # Update to use the current API initialization pattern (v0.49.0)
        api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", "")
        self.client = Anthropic(api_key=api_key)
        
        self.cache_dir = os.path.join(DATA_DIR, "summaries_cache")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_file = os.path.join(self.cache_dir, "summary_cache.json")
        self._load_cache()
    
    def _load_cache(self):
        """Load summary cache from file"""
        try:
            with open(self.cache_file, 'r') as f:
                self.cache = json.load(f)
                logger.info(f"Loaded {len(self.cache)} cached summaries")
        except (FileNotFoundError, json.JSONDecodeError):
            self.cache = {}
            logger.info("Created new summary cache")
    
    def _save_cache(self):
        """Save summary cache to file"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
            logger.info(f"Saved {len(self.cache)} summaries to cache")
    
    def _create_enhanced_prompt(self, transcript, metadata=None):
        """Create an enhanced prompt for Claude with context about the video"""
        context = ""
        if metadata:
            context = f"Video by: {metadata.get('account', 'Unknown')}\n"
            if metadata.get('timestamp'):
                context += f"Posted: {metadata.get('timestamp', '').split('T')[0]}\n"
            if metadata.get('caption'):
                context += f"Caption: {metadata.get('caption', 'No caption')}\n"
        
        prompt = f"""Analyze this Instagram video transcript and provide structured information in your response:

{context}
TRANSCRIPT:
{transcript}

Please extract the following information:
1. Summary: Concise overview of the main points (80-100 words max)
2. Key Topics: 3-5 main topics covered
3. Entities Mentioned: People, products, services, or concepts mentioned
4. Tone & Style: Formal/informal, educational/conversational, etc.
5. Key Insights: 2-3 main takeaways or insights
6. Actionable Information: Any specific advice, steps, or actionable items
7. Content Type: What kind of content is this (tutorial, conversation, review, educational, etc.)

Provide your analysis in this format:

SUMMARY: [Your concise summary]

KEY TOPICS:
- [Topic 1]
- [Topic 2]
- [Topic 3]

ENTITIES MENTIONED:
- [Entity 1] (person/product/concept)
- [Entity 2] (person/product/concept)

TONE & STYLE: [Analysis of tone and presentation style]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]

ACTIONABLE INFORMATION:
- [Actionable item 1] 
- [Actionable item 2]

CONTENT TYPE: [content type classification]
"""
        return prompt
    
    def _extract_key_phrases(self, transcript, summary):
        """Extract key phrases and terms from transcript and summary"""
        combined_text = f"{summary} {transcript}"
        # Split into words, lowercase, and remove punctuation
        words = re.findall(r'\b\w+\b', combined_text.lower())
        
        # Count word frequencies (excluding common words)
        common_words = {'the', 'and', 'that', 'for', 'you', 'with', 'this', 'was', 'are', 'have', 
                       'its', 'they', 'from', 'but', 'not', 'what', 'all', 'were', 'when', 'your',
                       'can', 'said', 'there', 'use', 'been', 'has', 'would', 'each', 'which', 'she',
                       'how', 'their', 'will', 'other', 'about', 'out', 'many', 'then', 'them', 'these',
                       'some', 'her', 'him', 'into', 'more', 'could', 'know', 'like', 'just'}
        
        word_freq = {}
        for word in words:
            if word not in common_words and len(word) > 3:  # Exclude common words and short words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top N words by frequency
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # Extract 2-3 word phrases using regex patterns
        phrase_patterns = [
            r'\b\w+\s+\w+\b',           # 2-word phrases
            r'\b\w+\s+\w+\s+\w+\b'      # 3-word phrases
        ]
        
        phrases = []
        for pattern in phrase_patterns:
            phrases.extend(re.findall(pattern, combined_text.lower()))
        
        # Count phrase frequencies and exclude common phrases
        phrase_freq = {}
        for phrase in phrases:
            if not any(w in common_words for w in phrase.split() if len(w) <= 3):
                if len(phrase.split()) > 1:  # Ensure it's a real phrase
                    phrase_freq[phrase] = phrase_freq.get(phrase, 0) + 1
        
        # Get top N phrases by frequency
        top_phrases = sorted(phrase_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Combine top words and phrases
        key_phrases = [word for word, _ in top_words]
        key_phrases.extend([phrase for phrase, _ in top_phrases])
        
        # Remove duplicates and limit to 10 items
        unique_key_phrases = []
        for phrase in key_phrases:
            if not any(phrase in p for p in unique_key_phrases):
                unique_key_phrases.append(phrase)
                if len(unique_key_phrases) >= 10:
                    break
        
        return unique_key_phrases
    
    def _parse_structured_summary(self, claude_response):
        """Parse Claude's structured response into components"""
        result = {
            'summary': '',
            'key_topics': [],
            'entities': [],
            'tone': '',
            'key_insights': [],
            'actionable_items': [],
            'content_type': ''
        }
        
        # Extract the summary
        summary_match = re.search(r'SUMMARY:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if summary_match:
            result['summary'] = summary_match.group(1).strip()
        
        # Extract key topics
        topics_section = re.search(r'KEY TOPICS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if topics_section:
            topics = re.findall(r'-\s*(.*?)(?=\n|$)', topics_section.group(1))
            result['key_topics'] = [topic.strip() for topic in topics if topic.strip()]
        
        # Extract entities
        entities_section = re.search(r'ENTITIES MENTIONED:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if entities_section:
            entities = re.findall(r'-\s*(.*?)(?=\n|$)', entities_section.group(1))
            result['entities'] = [entity.strip() for entity in entities if entity.strip()]
        
        # Extract tone
        tone_match = re.search(r'TONE & STYLE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if tone_match:
            result['tone'] = tone_match.group(1).strip()
        
        # Extract insights
        insights_section = re.search(r'KEY INSIGHTS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if insights_section:
            insights = re.findall(r'-\s*(.*?)(?=\n|$)', insights_section.group(1))
            result['key_insights'] = [insight.strip() for insight in insights if insight.strip()]
        
        # Extract actionable items
        actionable_section = re.search(r'ACTIONABLE INFORMATION:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if actionable_section:
            actions = re.findall(r'-\s*(.*?)(?=\n|$)', actionable_section.group(1))
            result['actionable_items'] = [action.strip() for action in actions if action.strip()]
        
        # Extract content type
        content_match = re.search(r'CONTENT TYPE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if content_match:
            result['content_type'] = content_match.group(1).strip()
        
        return result
    
    def summarize(self, transcript, shortcode, metadata=None, max_retries=3):
        """Generate a summary using Claude API with retry logic"""
        # Check cache first
        if shortcode in self.cache:
            logger.info(f"Using cached summary for {shortcode}")
            return self.cache[shortcode]
        
        # If transcript is too short, use it as the summary
        if len(transcript.split()) < 30:
            logger.info(f"Transcript too short for {shortcode}, using as summary")
            
            # Even for short transcripts, provide some structure
            structured_response = {
                'summary': transcript,
                'key_topics': self._extract_key_phrases(transcript, transcript)[:3],
                'entities': [],
                'tone': 'Brief',
                'key_insights': [],
                'actionable_items': [],
                'content_type': 'Short clip'
            }
            
            summary_text = f"SUMMARY: {structured_response['summary']}\n\n"
            summary_text += "KEY TOPICS:\n" + "\n".join([f"- {topic}" for topic in structured_response['key_topics']])
            summary_text += "\n\nCONTENT TYPE: Short clip"
            
            self.cache[shortcode] = summary_text
            self._save_cache()
            return summary_text
        
        # Retry logic with exponential backoff
        retries = 0
        while retries <= max_retries:
            try:
                logger.info(f"Generating summary for {shortcode} (Attempt {retries+1}/{max_retries+1})")
                
                # Create enhanced prompt
                prompt = self._create_enhanced_prompt(transcript, metadata)
                
                # Use the current API pattern for Anthropic v0.49.0
                response = self.client.messages.create(
                    model="claude-3-haiku-20240307",  # More cost-effective model for batch processing
                    max_tokens=1024,
                    messages=[
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ]
                )
                
                # Get the text content from the response
                summary = response.content[0].text
                
                # Extract structured information from the response
                structured_data = self._parse_structured_summary(summary)
                
                # Extract key phrases if not already present in the response
                if not structured_data['key_topics'] or len(structured_data['key_topics']) < 3:
                    key_phrases = self._extract_key_phrases(transcript, structured_data['summary'])
                    structured_data['key_topics'] = key_phrases[:5]
                
                # Cache the result (store the full text response)
                self.cache[shortcode] = summary
                self._save_cache()
                
                logger.info(f"Successfully generated summary for {shortcode}")
                return summary
                
            except Exception as e:
                retries += 1
                logger.error(f"Error generating summary (Attempt {retries}/{max_retries+1}): {str(e)}")
                if retries <= max_retries:
                    wait_time = 2 ** retries + (retries * 2)  # Exponential backoff
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed to generate summary after {max_retries+1} attempts")
                    # Provide a basic structured response for failed summaries
                    basic_response = (
                        "SUMMARY: Summary generation failed due to API errors.\n\n"
                        "KEY TOPICS:\n- Unknown\n\n"
                        "CONTENT TYPE: Unknown"
                    )
                    return basic_response
        
        return "Summary generation failed"


def get_video_metadata(shortcode, conn):
    """Fetch metadata for a video from the database"""
    cursor = conn.cursor()
    cursor.execute(
        "SELECT account, timestamp, caption FROM videos WHERE shortcode = ?",
        (shortcode,)
    )
    result = cursor.fetchone()
    if result:
        return {
            'account': result['account'],
            'timestamp': result['timestamp'],
            'caption': result['caption']
        }
    return None


def analyze_content_quality(transcript):
    """Calculate metrics about content quality"""
    if not transcript:
        return {}
        
    # Calculate basic metrics
    words = transcript.split()
    word_count = len(words)
    
    # Estimate dialogue percentage (if there are colons, quotes, etc.)
    dialogue_markers = [":", "?", '"', "'", "says", "said", "asked"]
    dialogue_score = sum(transcript.count(marker) for marker in dialogue_markers) / max(1, len(transcript) / 100)
    
    # Calculate average word length (a proxy for complexity)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    
    # Estimate reading time (average person reads ~200-250 words per minute)
    reading_time_seconds = (word_count / 200) * 60
    
    return {
        "word_count": word_count,
        "dialogue_ratio": min(1.0, dialogue_score),
        "avg_word_length": avg_word_length,
        "estimated_read_time_seconds": reading_time_seconds
    }


def process_transcripts_with_claude(batch_size=10, delay_between_batches=5):
    """Process transcripts in batches using Claude API"""
    try:
        # Check for API key
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error("ANTHROPIC_API_KEY environment variable not set. Please set it before running the summarizer.")
            return
            
        summarizer = ClaudeSummarizer()
        
        # Get videos that need summarization
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Find videos with transcripts but no summaries
        cursor.execute(
            "SELECT id, shortcode, account, transcript FROM videos WHERE transcript IS NOT NULL AND transcript != '' AND (summary IS NULL OR summary = '')"
        )
        videos = cursor.fetchall()
        
        total_videos = len(videos)
        logger.info(f"Found {total_videos} videos that need summarization")
        
        if total_videos == 0:
            logger.info("No videos to summarize. Exiting.")
            conn.close()
            return
        
        # Process in batches
        for i in range(0, total_videos, batch_size):
            batch = videos[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_videos - 1) // batch_size + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} videos)")
            
            for video in batch:
                video_id = video['id']
                shortcode = video['shortcode']
                account = video['account']
                transcript = video['transcript']
                
                # Get full metadata for enhanced context
                metadata = get_video_metadata(shortcode, conn)
                
                # Calculate content quality metrics
                quality_metrics = analyze_content_quality(transcript)
                word_count = quality_metrics.get('word_count', 0)
                
                logger.info(f"Summarizing video {shortcode} ({word_count} words)")
                summary = summarizer.summarize(transcript, shortcode, metadata=metadata)
                
                # Extract key phrases for database storage
                structured_data = summarizer._parse_structured_summary(summary)
                key_phrases_json = json.dumps(structured_data.get('key_topics', []))
                
                # Update database with summary, word count and content metrics
                cursor.execute(
                    """UPDATE videos SET 
                       summary = ?, 
                       word_count = ?,
                       key_phrases = ?,
                       duration_seconds = ?
                       WHERE id = ?""",
                    (summary, word_count, key_phrases_json, 
                     quality_metrics.get('estimated_read_time_seconds', 0), video_id)
                )
                conn.commit()
                logger.info(f"Updated summary and metrics for video {shortcode}")
            
            # Respect API rate limits between batches
            processed_count = min(i + batch_size, total_videos)
            logger.info(f"Processed {processed_count}/{total_videos} videos")
            
            if i + batch_size < total_videos:
                logger.info(f"Waiting {delay_between_batches} seconds before next batch")
                time.sleep(delay_between_batches)
        
        conn.close()
        logger.info("Summarization process complete")
        
    except Exception as e:
        logger.error(f"Error in batch processing: {str(e)}")
        raise


def summarize_transcripts():
    """Main entry point for transcript summarization"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time.time()
    process_transcripts_with_claude()
    logger.info(f"Summarization completed in {time.time() - start_time:.2f} seconds")


if __name__ == "__main__":
    summarize_transcripts() 

================================================================================
File: test_db.py
================================================================================

import sqlite3

# Connect to the database
conn = sqlite3.connect('data/knowledge_base.db')
cursor = conn.cursor()

# Get the list of tables
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()
print("Tables in the database:")
for table in tables:
    print(f"- {table[0]}")

# Count records in videos table
cursor.execute("SELECT COUNT(*) FROM videos;")
count = cursor.fetchone()[0]
print(f"\nNumber of videos in database: {count}")

# Close the connection
conn.close()

print("\nDatabase access test completed successfully!") 

================================================================================
File: test_proxy.py
================================================================================

#!/usr/bin/env python
"""
Test script for Bright Data residential proxy
"""
import sys
import requests
import os
import json
from datetime import datetime
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def test_brightdata_proxy():
    """Test Bright Data residential proxy"""
    print("Testing Bright Data residential proxy...")
    
    # Proxy configuration
    proxy = "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335"
    
    # Setup proxies for requests
    proxies = {
        "http": proxy,
        "https": proxy
    }
    
    try:
        # First, test with the Bright Data test endpoint
        response = requests.get(
            "https://geo.brdtest.com/welcome.txt?product=resi&method=native",
            proxies=proxies,
            timeout=10,
            verify=False  # Disable SSL verification
        )
        
        if response.status_code == 200:
            print(" Bright Data test successful!")
            print(f"Response: {response.text.strip()}")
        else:
            print(f" Bright Data test failed with status code: {response.status_code}")
            print(f"Response: {response.text}")
        
        # Then test with Instagram
        print("\nTesting access to Instagram...")
        instagram_response = requests.get(
            "https://www.instagram.com/favicon.ico",
            proxies=proxies,
            timeout=10,
            verify=False,  # Disable SSL verification
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        
        if instagram_response.status_code == 200:
            print(" Instagram test successful!")
            print(f"Status code: {instagram_response.status_code}")
            print(f"Content length: {len(instagram_response.content)} bytes")
        else:
            print(f" Instagram test failed with status code: {instagram_response.status_code}")
            
        # Save test results
        results = {
            "timestamp": datetime.now().isoformat(),
            "brightdata_test": {
                "status_code": response.status_code,
                "success": response.status_code == 200,
                "response_text": response.text.strip()
            },
            "instagram_test": {
                "status_code": instagram_response.status_code,
                "success": instagram_response.status_code == 200,
                "content_length": len(instagram_response.content)
            }
        }
        
        # Create data directory if it doesn't exist
        os.makedirs(os.path.join("data", "logs"), exist_ok=True)
        
        # Save results to a file
        with open(os.path.join("data", "logs", "proxy_test_results.json"), "w") as f:
            json.dump(results, f, indent=2)
            
        print("\nTest results saved to data/logs/proxy_test_results.json")
        
    except Exception as e:
        print(f" Error testing proxy: {str(e)}")
        return False
    
    return True

if __name__ == "__main__":
    test_brightdata_proxy() 

================================================================================
File: test_vector_search.py
================================================================================

#!/usr/bin/env python3
"""
Test script for vector search functionality.

This script demonstrates and tests the different search capabilities of the system:
1. Vector search
2. Keyword search
3. Hybrid search with different weighting schemes
"""
import os
import sys
import argparse
import logging
import time
from typing import List, Dict, Any, Optional

# Import vector search modules
try:
    import embeddings
    import vector_search
    import hybrid_search
    import config
except ImportError as e:
    print(f"Error importing vector search modules: {str(e)}")
    print("Make sure you have installed all required dependencies from requirements.txt")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/vector_search_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('test_vector_search')

# Sample test queries for different content types
TEST_QUERIES = {
    "general": [
        "How do transformer models work?",
        "What are the recent advances in natural language processing?",
        "Explain self-attention mechanism in neural networks",
        "Methods for improving computer vision models",
        "Best practices for machine learning model deployment"
    ],
    "code": [
        "Python function to implement K-means clustering",
        "How to optimize TensorFlow models for production",
        "Example of using BERT for text classification in PyTorch",
        "Code for implementing a convolutional neural network",
        "SQL query optimization techniques"
    ],
    "factual": [
        "When was GPT-3 released?",
        "Who created the transformer architecture?",
        "What is the BLEU score used for?",
        "How many parameters are in GPT-4?",
        "Which institutions are leading AI research?"
    ]
}

def test_vector_search(queries: List[str], top_k: int = 5, 
                      source_type: Optional[str] = None,
                      in_memory_index: bool = False) -> List[Dict[str, Any]]:
    """Run vector search tests for the provided queries"""
    results = []
    
    logger.info(f"Testing vector search with {len(queries)} queries")
    embedding_generator = embeddings.EmbeddingGenerator()
    
    # Create in-memory index if requested
    index = None
    if in_memory_index:
        logger.info("Creating in-memory index...")
        index = vector_search.create_memory_index()
        logger.info(f"Created in-memory index with {len(index.get('vectors', []))} vectors")
    
    # Run tests for each query
    for i, query in enumerate(queries):
        logger.info(f"Query {i+1}/{len(queries)}: {query}")
        
        start_time = time.time()
        
        if in_memory_index and index and len(index.get('vectors', [])) > 0:
            # Generate query embedding
            query_embedding = embedding_generator.generate_embedding(query)
            
            # Search using in-memory index
            query_results = vector_search.search_memory_index(
                query_embedding, index, top_k=top_k
            )
            
            # Fetch chunk text and enrich results
            query_results = vector_search.enrich_search_results(query_results)
        else:
            # Use standard search
            query_results = vector_search.search_by_text(
                query_text=query,
                top_k=top_k,
                source_type=source_type,
                embedding_generator=embedding_generator
            )
        
        elapsed = time.time() - start_time
        
        # Add to results
        results.append({
            'query': query,
            'results': query_results,
            'count': len(query_results),
            'elapsed': elapsed
        })
        
        logger.info(f"Found {len(query_results)} results in {elapsed:.4f} seconds")
    
    return results

def test_keyword_search(queries: List[str], top_k: int = 5,
                       source_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """Run keyword search tests for the provided queries"""
    results = []
    
    logger.info(f"Testing keyword search with {len(queries)} queries")
    
    # Run tests for each query
    for i, query in enumerate(queries):
        logger.info(f"Query {i+1}/{len(queries)}: {query}")
        
        start_time = time.time()
        
        # Perform keyword search
        query_results = hybrid_search.keyword_search(
            query=query,
            top_k=top_k,
            source_type=source_type
        )
        
        elapsed = time.time() - start_time
        
        # Add to results
        results.append({
            'query': query,
            'results': query_results,
            'count': len(query_results),
            'elapsed': elapsed
        })
        
        logger.info(f"Found {len(query_results)} results in {elapsed:.4f} seconds")
    
    return results

def test_hybrid_search(queries: List[str], top_k: int = 5,
                      source_type: Optional[str] = None,
                      vector_weight: Optional[float] = None,
                      keyword_weight: Optional[float] = None,
                      adaptive: bool = True) -> List[Dict[str, Any]]:
    """Run hybrid search tests for the provided queries"""
    results = []
    
    logger.info(f"Testing hybrid search with {len(queries)} queries")
    embedding_generator = embeddings.EmbeddingGenerator()
    
    # Load weight history if using adaptive weighting
    weight_history = None
    if adaptive and (vector_weight is None or keyword_weight is None):
        weight_history = hybrid_search.load_weights_history()
    
    # Run tests for each query
    for i, query in enumerate(queries):
        logger.info(f"Query {i+1}/{len(queries)}: {query}")
        
        # Determine weights if adaptive
        query_vector_weight = vector_weight
        query_keyword_weight = keyword_weight
        
        if adaptive and (query_vector_weight is None or query_keyword_weight is None):
            if weight_history:
                # Get query type
                query_type = hybrid_search.classify_query_type(query)
                
                # Use query-specific weights if available
                normalized_query = query.lower().strip()
                if normalized_query in weight_history['queries']:
                    query_vector_weight = weight_history['queries'][normalized_query]['vector_weight']
                    query_keyword_weight = weight_history['queries'][normalized_query]['keyword_weight']
                    logger.info(f"Using query-specific weights: vector={query_vector_weight:.2f}, keyword={query_keyword_weight:.2f}")
                else:
                    # Use query type defaults
                    type_key = f"default_{query_type}_vector_weight"
                    if type_key in weight_history:
                        query_vector_weight = weight_history[type_key]
                        query_keyword_weight = 1.0 - query_vector_weight
                        logger.info(f"Using {query_type} query type weights: vector={query_vector_weight:.2f}, keyword={query_keyword_weight:.2f}")
                    else:
                        # Determine weights based on query
                        query_vector_weight, query_keyword_weight = hybrid_search.determine_weights(query)
                        logger.info(f"Using determined weights: vector={query_vector_weight:.2f}, keyword={query_keyword_weight:.2f}")
        
        start_time = time.time()
        
        # Perform hybrid search
        query_results = hybrid_search.hybrid_search(
            query=query,
            top_k=top_k,
            source_type=source_type,
            vector_weight=query_vector_weight,
            keyword_weight=query_keyword_weight,
            embedding_generator=embedding_generator
        )
        
        elapsed = time.time() - start_time
        
        # Add to results
        results.append({
            'query': query,
            'results': query_results,
            'count': len(query_results),
            'elapsed': elapsed,
            'vector_weight': query_vector_weight,
            'keyword_weight': query_keyword_weight
        })
        
        logger.info(f"Found {len(query_results)} results in {elapsed:.4f} seconds with weights: vector={query_vector_weight:.2f}, keyword={query_keyword_weight:.2f}")
    
    return results

def compare_search_methods(queries: List[str], top_k: int = 5,
                          source_type: Optional[str] = None) -> Dict[str, Any]:
    """Compare vector, keyword, and hybrid search for the provided queries"""
    logger.info(f"Comparing search methods for {len(queries)} queries")
    
    # Run each search method
    vector_results = test_vector_search(queries, top_k, source_type)
    keyword_results = test_keyword_search(queries, top_k, source_type)
    hybrid_results = test_hybrid_search(queries, top_k, source_type)
    
    # Compile comparison stats
    comparison = []
    for i, query in enumerate(queries):
        vector_count = vector_results[i]['count'] if i < len(vector_results) else 0
        vector_time = vector_results[i]['elapsed'] if i < len(vector_results) else 0
        
        keyword_count = keyword_results[i]['count'] if i < len(keyword_results) else 0
        keyword_time = keyword_results[i]['elapsed'] if i < len(keyword_results) else 0
        
        hybrid_count = hybrid_results[i]['count'] if i < len(hybrid_results) else 0
        hybrid_time = hybrid_results[i]['elapsed'] if i < len(hybrid_results) else 0
        hybrid_vector_weight = hybrid_results[i].get('vector_weight', 0) if i < len(hybrid_results) else 0
        hybrid_keyword_weight = hybrid_results[i].get('keyword_weight', 0) if i < len(hybrid_results) else 0
        
        # Add to comparison
        comparison.append({
            'query': query,
            'vector': {
                'count': vector_count,
                'time': vector_time
            },
            'keyword': {
                'count': keyword_count,
                'time': keyword_time
            },
            'hybrid': {
                'count': hybrid_count,
                'time': hybrid_time,
                'vector_weight': hybrid_vector_weight,
                'keyword_weight': hybrid_keyword_weight
            }
        })
    
    # Calculate averages
    avg_vector_count = sum(c['vector']['count'] for c in comparison) / len(comparison)
    avg_vector_time = sum(c['vector']['time'] for c in comparison) / len(comparison)
    
    avg_keyword_count = sum(c['keyword']['count'] for c in comparison) / len(comparison)
    avg_keyword_time = sum(c['keyword']['time'] for c in comparison) / len(comparison)
    
    avg_hybrid_count = sum(c['hybrid']['count'] for c in comparison) / len(comparison)
    avg_hybrid_time = sum(c['hybrid']['time'] for c in comparison) / len(comparison)
    
    return {
        'comparisons': comparison,
        'summary': {
            'vector': {
                'avg_count': avg_vector_count,
                'avg_time': avg_vector_time
            },
            'keyword': {
                'avg_count': avg_keyword_count,
                'avg_time': avg_keyword_time
            },
            'hybrid': {
                'avg_count': avg_hybrid_count,
                'avg_time': avg_hybrid_time
            }
        }
    }

def print_results(results: List[Dict[str, Any]], detailed: bool = False) -> None:
    """Print search results"""
    for i, result in enumerate(results):
        query = result['query']
        query_results = result['results']
        elapsed = result['elapsed']
        
        print(f"\nQuery {i+1}: {query}")
        print(f"Found {len(query_results)} results in {elapsed:.4f} seconds")
        
        if 'vector_weight' in result and 'keyword_weight' in result:
            print(f"Weights: vector={result['vector_weight']:.2f}, keyword={result['keyword_weight']:.2f}")
        
        if detailed and query_results:
            print("-" * 80)
            
            for j, res in enumerate(query_results):
                # Print common result details
                print(f"Result {j+1}")
                print(f"Content ID: {res.get('content_id', 'Unknown')}")
                print(f"Title: {res.get('title', 'Unknown')}")
                print(f"Source: {res.get('source_type', 'Unknown')}")
                
                # Print score details
                if 'similarity' in res:
                    print(f"Similarity: {res['similarity']:.4f}")
                if 'score' in res:
                    print(f"Score: {res['score']:.4f}")
                if 'combined_score' in res:
                    print(f"Combined Score: {res['combined_score']:.4f}")
                    print(f"Vector Score: {res.get('vector_score', 0):.4f}, Keyword Score: {res.get('keyword_score', 0):.4f}")
                
                # Print snippet or chunk text
                if 'snippet' in res and res['snippet']:
                    print(f"Keyword Match: {res['snippet']}")
                
                if 'chunk_text' in res and res['chunk_text']:
                    text = res['chunk_text']
                    print(f"Text: {text[:150]}..." if len(text) > 150 else f"Text: {text}")
                
                print("-" * 40)
        
        print("-" * 80)

def print_comparison(comparison: Dict[str, Any]) -> None:
    """Print comparison results between search methods"""
    comparisons = comparison['comparisons']
    summary = comparison['summary']
    
    print("\nSearch Method Comparison")
    print("=" * 80)
    
    # Print summary
    print("\nSummary:")
    print(f"Vector Search:  Avg Results: {summary['vector']['avg_count']:.2f}, Avg Time: {summary['vector']['avg_time']:.4f}s")
    print(f"Keyword Search: Avg Results: {summary['keyword']['avg_count']:.2f}, Avg Time: {summary['keyword']['avg_time']:.4f}s")
    print(f"Hybrid Search:  Avg Results: {summary['hybrid']['avg_count']:.2f}, Avg Time: {summary['hybrid']['avg_time']:.4f}s")
    
    # Print per-query comparison
    print("\nPer-Query Results:")
    for i, comp in enumerate(comparisons):
        query = comp['query']
        
        print(f"\nQuery {i+1}: {query}")
        print(f"Vector:  {comp['vector']['count']} results in {comp['vector']['time']:.4f}s")
        print(f"Keyword: {comp['keyword']['count']} results in {comp['keyword']['time']:.4f}s")
        print(f"Hybrid:  {comp['hybrid']['count']} results in {comp['hybrid']['time']:.4f}s (weights: {comp['hybrid']['vector_weight']:.2f}:{comp['hybrid']['keyword_weight']:.2f})")
    
    print("=" * 80)

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Test vector search functionality")
    parser.add_argument("--vector", action="store_true", help="Test vector search")
    parser.add_argument("--keyword", action="store_true", help="Test keyword search")
    parser.add_argument("--hybrid", action="store_true", help="Test hybrid search")
    parser.add_argument("--compare", action="store_true", help="Compare all search methods")
    parser.add_argument("--query-type", choices=["general", "code", "factual", "all"], default="all", 
                        help="Type of test queries to use")
    parser.add_argument("--custom-query", help="Run tests with a custom query")
    parser.add_argument("--top-k", type=int, default=5, help="Number of results to return")
    parser.add_argument("--source-type", choices=["research_paper", "github", "instagram"], 
                        help="Filter by source type")
    parser.add_argument("--vector-weight", type=float, help="Weight for vector search (0-1)")
    parser.add_argument("--keyword-weight", type=float, help="Weight for keyword search (0-1)")
    parser.add_argument("--adaptive", action="store_true", help="Use adaptive weighting based on query")
    parser.add_argument("--in-memory", action="store_true", help="Use in-memory indexing for vector search")
    parser.add_argument("--detailed", action="store_true", help="Show detailed results")
    
    args = parser.parse_args()
    
    # Create logs directory if it doesn't exist
    os.makedirs("logs", exist_ok=True)
    
    # Determine test queries
    test_queries = []
    
    if args.custom_query:
        test_queries = [args.custom_query]
    elif args.query_type == "all":
        for query_type in TEST_QUERIES:
            test_queries.extend(TEST_QUERIES[query_type])
    else:
        test_queries = TEST_QUERIES.get(args.query_type, [])
        if not test_queries:
            logger.error(f"No test queries available for type: {args.query_type}")
            return
    
    # Run tests
    if args.vector:
        logger.info("Running vector search tests")
        results = test_vector_search(
            queries=test_queries,
            top_k=args.top_k,
            source_type=args.source_type,
            in_memory_index=args.in_memory
        )
        
        print("\nVector Search Results:")
        print_results(results, detailed=args.detailed)
    
    if args.keyword:
        logger.info("Running keyword search tests")
        results = test_keyword_search(
            queries=test_queries,
            top_k=args.top_k,
            source_type=args.source_type
        )
        
        print("\nKeyword Search Results:")
        print_results(results, detailed=args.detailed)
    
    if args.hybrid:
        logger.info("Running hybrid search tests")
        results = test_hybrid_search(
            queries=test_queries,
            top_k=args.top_k,
            source_type=args.source_type,
            vector_weight=args.vector_weight,
            keyword_weight=args.keyword_weight,
            adaptive=args.adaptive
        )
        
        print("\nHybrid Search Results:")
        print_results(results, detailed=args.detailed)
    
    if args.compare:
        logger.info("Running comparison of search methods")
        comparison = compare_search_methods(
            queries=test_queries,
            top_k=args.top_k,
            source_type=args.source_type
        )
        
        print_comparison(comparison)
    
    # Show help if no test specified
    if not (args.vector or args.keyword or args.hybrid or args.compare):
        parser.print_help()

if __name__ == "__main__":
    main() 

================================================================================
File: transcriber.py
================================================================================

"""
Module for extracting audio from videos and transcribing with Whisper
"""
import os
import json
import glob
import subprocess
import logging
from tqdm import tqdm
import whisper

from config import (
    DOWNLOAD_DIR,
    AUDIO_DIR,
    TRANSCRIPT_DIR,
    WHISPER_MODEL
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('transcriber.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('transcriber')

def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(AUDIO_DIR, exist_ok=True)
    os.makedirs(TRANSCRIPT_DIR, exist_ok=True)
    
    # Create account-specific directories
    for account_dir in os.listdir(DOWNLOAD_DIR):
        if os.path.isdir(os.path.join(DOWNLOAD_DIR, account_dir)):
            os.makedirs(os.path.join(AUDIO_DIR, account_dir), exist_ok=True)
            os.makedirs(os.path.join(TRANSCRIPT_DIR, account_dir), exist_ok=True)

def extract_audio(video_path, audio_path):
    """Extract audio from video using FFmpeg"""
    try:
        cmd = f'ffmpeg -i "{video_path}" -vn -acodec pcm_s16le -ar 16000 -ac 1 "{audio_path}" -y'
        subprocess.call(cmd, shell=True)
        return True
    except Exception as e:
        logger.error(f"Error extracting audio from {video_path}: {str(e)}")
        return False

def process_videos():
    """Process all downloaded videos that haven't been transcribed yet"""
    setup_directories()
    
    # Load Whisper model
    logger.info(f"Loading Whisper model: {WHISPER_MODEL}")
    model = whisper.load_model(WHISPER_MODEL)
    logger.info("Model loaded successfully")
    
    # Get all video files
    all_videos = []
    for account_dir in os.listdir(DOWNLOAD_DIR):
        account_path = os.path.join(DOWNLOAD_DIR, account_dir)
        if os.path.isdir(account_path):
            videos = glob.glob(os.path.join(account_path, "*.mp4"))
            all_videos.extend(videos)
    
    logger.info(f"Found {len(all_videos)} total videos to process")
    
    # Process each video
    for video_path in tqdm(all_videos, desc="Processing videos"):
        # Extract account name and filename
        parts = video_path.split(os.sep)
        account = parts[-2]
        filename = os.path.basename(video_path)
        base_name = os.path.splitext(filename)[0]
        
        # Define paths
        audio_path = os.path.join(AUDIO_DIR, account, f"{base_name}.wav")
        transcript_path = os.path.join(TRANSCRIPT_DIR, account, f"{base_name}.json")
        
        # Skip if already transcribed
        if os.path.exists(transcript_path):
            continue
        
        # Extract audio if needed
        if not os.path.exists(audio_path):
            logger.info(f"Extracting audio from {filename}")
            os.makedirs(os.path.dirname(audio_path), exist_ok=True)
            if not extract_audio(video_path, audio_path):
                continue
        
        # Transcribe audio
        try:
            logger.info(f"Transcribing {filename}")
            result = model.transcribe(audio_path)
            
            # Get metadata if available
            metadata_path = os.path.join(DOWNLOAD_DIR, account, "metadata", f"{base_name}.json")
            metadata = {}
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            # Create transcript with metadata
            transcript_data = {
                "text": result["text"],
                "segments": result["segments"],
                "language": result["language"],
                "filename": filename,
                "account": account,
                **metadata
            }
            
            # Save transcript
            os.makedirs(os.path.dirname(transcript_path), exist_ok=True)
            with open(transcript_path, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=4)
            
            logger.info(f"Transcription complete for {filename}")
            
        except Exception as e:
            logger.error(f"Error transcribing {filename}: {str(e)}")
    
    logger.info("Transcription process completed")

if __name__ == "__main__":
    process_videos() 

================================================================================
File: vector_search.py
================================================================================

"""
Vector search module for semantic retrieval of content

This module provides functionality to search content using vector similarity,
enabling semantic search capabilities for the knowledge base.
"""
import os
import logging
import sqlite3
import pickle
import time
from datetime import datetime
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union

# Import local modules
import config
from embeddings import EmbeddingGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('vector_search')

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    Calculate cosine similarity between two vectors
    
    Args:
        vec1: First vector
        vec2: Second vector
        
    Returns:
        Cosine similarity (between -1 and 1)
    """
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
        
    return float(np.dot(vec1, vec2) / (norm1 * norm2))

def vector_search(query_embedding: np.ndarray, top_k: int = 5, 
                 source_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Search for similar content using vector embeddings
    
    Args:
        query_embedding: The query embedding vector
        top_k: Number of results to return
        source_type: Optional filter by source type
        
    Returns:
        List of search results with similarity scores and metadata
    """
    conn = None
    try:
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Build query to get all embeddings (potentially filtered by source type)
        query = """
            SELECT 
                ce.content_id, ce.chunk_index, ce.embedding_vector, ce.chunk_text,
                ac.title, ac.source_type_id
            FROM content_embeddings ce
            JOIN ai_content ac ON ce.content_id = ac.id
        """
        
        params = []
        
        # Add source type filter if provided
        if source_type:
            cursor.execute("SELECT id FROM source_types WHERE name = ?", (source_type,))
            source_type_id = cursor.fetchone()
            if source_type_id:
                query += " WHERE ac.source_type_id = ?"
                params.append(source_type_id[0])
            else:
                logger.warning(f"Source type '{source_type}' not found")
        
        # Execute query
        cursor.execute(query, params)
        
        # Calculate similarities and collect results
        results = []
        for row in cursor.fetchall():
            content_id, chunk_index, embedding_binary, chunk_text, title, source_type_id = row
            
            # Get source type name
            cursor.execute("SELECT name FROM source_types WHERE id = ?", (source_type_id,))
            source_type_result = cursor.fetchone()
            source_type_name = source_type_result[0] if source_type_result else "unknown"
            
            # Skip invalid embeddings
            if not embedding_binary:
                continue
                
            try:
                # Deserialize embedding
                embedding = pickle.loads(embedding_binary)
                
                # Calculate similarity
                similarity = cosine_similarity(query_embedding, embedding)
                
                # Add to results
                results.append({
                    'content_id': content_id,
                    'chunk_index': chunk_index,
                    'similarity': similarity,
                    'chunk_text': chunk_text,
                    'title': title,
                    'source_type': source_type_name
                })
            except Exception as e:
                logger.warning(f"Error processing embedding for content {content_id}, chunk {chunk_index}: {str(e)}")
                continue
        
        # Sort by similarity (descending)
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # Return top k results
        return results[:top_k]
        
    except Exception as e:
        logger.error(f"Error in vector search: {str(e)}")
        return []
        
    finally:
        if conn:
            conn.close()

def search_by_text(query_text: str, top_k: int = 5, source_type: Optional[str] = None,
                  embedding_generator: Optional[EmbeddingGenerator] = None) -> List[Dict[str, Any]]:
    """
    Search for content similar to the query text
    
    Args:
        query_text: The query text
        top_k: Number of results to return
        source_type: Optional filter by source type
        embedding_generator: Optional pre-initialized embedding generator
        
    Returns:
        List of search results with similarity scores and metadata
    """
    # Create or use provided embedding generator
    if embedding_generator is None:
        embedding_generator = EmbeddingGenerator()
    
    # Generate embedding for query
    query_embedding = embedding_generator.generate_embedding(query_text)
    
    # Perform vector search
    results = vector_search(query_embedding, top_k=top_k, source_type=source_type)
    
    # Add original query to results
    for result in results:
        result['query'] = query_text
    
    return results

def enrich_search_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Enrich search results with additional metadata from the database
    
    Args:
        results: List of search results
        
    Returns:
        Enriched search results
    """
    conn = None
    try:
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        for result in results:
            content_id = result['content_id']
            
            # Get content metadata
            cursor.execute("""
                SELECT 
                    title, description, date_created, url, metadata
                FROM ai_content 
                WHERE id = ?
            """, (content_id,))
            
            content_data = cursor.fetchone()
            if content_data:
                title, description, date_created, url, metadata_json = content_data
                
                # Add to result
                result['title'] = title or result.get('title', '')
                result['description'] = description
                result['date_created'] = date_created
                result['url'] = url
                
                # Parse and add metadata if available
                if metadata_json:
                    try:
                        import json
                        metadata = json.loads(metadata_json)
                        result['metadata'] = metadata
                    except:
                        # Ignore metadata parsing errors
                        pass
            
            # Get concepts associated with this content
            try:
                cursor.execute("""
                    SELECT c.name, c.category, cc.importance
                    FROM concepts c
                    JOIN content_concepts cc ON c.id = cc.concept_id
                    WHERE cc.content_id = ?
                    ORDER BY cc.importance DESC
                """, (content_id,))
                
                concepts = [{
                    'name': row[0],
                    'category': row[1],
                    'importance': row[2]
                } for row in cursor.fetchall()]
                
                if concepts:
                    result['concepts'] = concepts
            except:
                # Concepts table might not exist yet
                pass
        
        return results
        
    except Exception as e:
        logger.error(f"Error enriching search results: {str(e)}")
        return results
        
    finally:
        if conn:
            conn.close()

def create_memory_index() -> Dict[str, Any]:
    """
    Create an in-memory index of all embeddings for faster search
    
    Returns:
        Dictionary containing the in-memory index
    """
    logger.info("Creating in-memory embedding index...")
    conn = None
    try:
        start_time = time.time()
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        # Get all embeddings
        cursor.execute("""
            SELECT 
                ce.content_id, ce.chunk_index, ce.embedding_vector,
                ac.title, ac.source_type_id
            FROM content_embeddings ce
            JOIN ai_content ac ON ce.content_id = ac.id
        """)
        
        # Process embeddings
        content_ids = []
        chunk_indices = []
        vectors = []
        metadata = []
        
        for row in cursor.fetchall():
            content_id, chunk_index, embedding_binary, title, source_type_id = row
            
            # Skip invalid embeddings
            if not embedding_binary:
                continue
                
            try:
                # Deserialize embedding
                embedding = pickle.loads(embedding_binary)
                
                # Add to arrays
                content_ids.append(content_id)
                chunk_indices.append(chunk_index)
                vectors.append(embedding)
                metadata.append({
                    'content_id': content_id,
                    'chunk_index': chunk_index,
                    'title': title,
                    'source_type_id': source_type_id
                })
            except:
                # Skip problematic embeddings
                continue
        
        # Convert to numpy array for faster operations
        if vectors:
            vectors_array = np.array(vectors)
        else:
            vectors_array = np.array([])
        
        # Create index
        index = {
            'content_ids': content_ids,
            'chunk_indices': chunk_indices,
            'vectors': vectors_array,
            'metadata': metadata,
            'created_at': datetime.now().isoformat()
        }
        
        elapsed = time.time() - start_time
        logger.info(f"In-memory index created with {len(vectors)} embeddings in {elapsed:.2f}s")
        
        return index
        
    except Exception as e:
        logger.error(f"Error creating in-memory index: {str(e)}")
        return {
            'content_ids': [],
            'chunk_indices': [],
            'vectors': np.array([]),
            'metadata': [],
            'created_at': datetime.now().isoformat(),
            'error': str(e)
        }
        
    finally:
        if conn:
            conn.close()

def search_memory_index(query_embedding: np.ndarray, index: Dict[str, Any], 
                       top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Search the in-memory index for similar embeddings
    
    Args:
        query_embedding: The query embedding
        index: The in-memory index
        top_k: Number of results to return
        
    Returns:
        List of search results
    """
    try:
        vectors = index.get('vectors')
        if len(vectors) == 0:
            logger.warning("Empty in-memory index")
            return []
        
        # Calculate similarities in a vectorized way
        similarities = np.dot(vectors, query_embedding) / (
            np.linalg.norm(vectors, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Get top k indices
        if len(similarities) <= top_k:
            top_indices = np.argsort(similarities)[::-1]
        else:
            top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # Collect results
        results = []
        for idx in top_indices:
            results.append({
                'content_id': index['content_ids'][idx],
                'chunk_index': index['chunk_indices'][idx],
                'similarity': float(similarities[idx]),
                'metadata': index['metadata'][idx]
            })
        
        return results
        
    except Exception as e:
        logger.error(f"Error searching in-memory index: {str(e)}")
        return []

def debug_search(query_text: str, top_k: int = 5):
    """
    Debug function to test and display search results
    
    Args:
        query_text: Query text
        top_k: Number of results to show
    """
    # Create embedding generator
    embedding_generator = EmbeddingGenerator()
    
    # Search
    logger.info(f"Searching for: {query_text}")
    results = search_by_text(
        query_text, 
        top_k=top_k, 
        embedding_generator=embedding_generator
    )
    
    # Enrich results
    results = enrich_search_results(results)
    
    # Display results
    print(f"\nSearch results for: {query_text}")
    print("=" * 80)
    
    for i, result in enumerate(results):
        print(f"\nResult {i+1} - Similarity: {result['similarity']:.4f}")
        print(f"Title: {result.get('title', 'Unknown')}")
        print(f"Source: {result.get('source_type', 'Unknown')}")
        print(f"Content ID: {result['content_id']}, Chunk: {result['chunk_index']}")
        print("-" * 40)
        print(result.get('chunk_text', '')[:300] + "..." if len(result.get('chunk_text', '')) > 300 else result.get('chunk_text', ''))
        print("-" * 40)
        
        # Display concepts if available
        if 'concepts' in result:
            print("Concepts:")
            for concept in result['concepts'][:5]:  # Show top 5 concepts
                print(f"- {concept['name']} ({concept['category']}, {concept['importance']})")
        
        print()
    
    return results

def main():
    """Main function for direct script execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Vector search for content")
    parser.add_argument("query", help="Query text")
    parser.add_argument("--top-k", type=int, default=5, help="Number of results to show")
    parser.add_argument("--source-type", help="Filter by source type")
    parser.add_argument("--create-index", action="store_true", help="Create in-memory index before searching")
    
    args = parser.parse_args()
    
    if args.create_index:
        # Create and use in-memory index
        index = create_memory_index()
        
        # Create embedding generator
        embedding_generator = EmbeddingGenerator()
        
        # Generate query embedding
        query_embedding = embedding_generator.generate_embedding(args.query)
        
        # Search using in-memory index
        results = search_memory_index(query_embedding, index, top_k=args.top_k)
        
        # Fetch chunk text and enrich results
        conn = sqlite3.connect(config.DB_PATH)
        cursor = conn.cursor()
        
        for result in results:
            # Get chunk text
            cursor.execute(
                "SELECT chunk_text FROM content_embeddings WHERE content_id = ? AND chunk_index = ?",
                (result['content_id'], result['chunk_index'])
            )
            chunk_text = cursor.fetchone()
            if chunk_text:
                result['chunk_text'] = chunk_text[0]
        
        conn.close()
        
        # Enrich results
        results = enrich_search_results(results)
    else:
        # Use standard search
        results = debug_search(args.query, top_k=args.top_k)
    
if __name__ == "__main__":
    main() 