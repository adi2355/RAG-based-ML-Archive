Self-Taught Self-Correction for Small Language Models
Viktor Moskvoretskii1,2, Chris Biemann3, Irina Nikishina3
1Skoltech,2HSE University,3University of Hamburg
Correspondence: vvmoskvoretskii@gmail.com
Abstract
Although large language models (LLMs) have
achieved remarkable performance across var-
ious tasks, they remain prone to errors. A
key challenge is enabling them to self-correct.
While prior research has relied on external
tools or large proprietary models, this work
explores self-correction in small language mod-
els (SLMs) through iterative fine-tuning us-
ing solely self-generated data. We introduce
the Self-Taught Self-Correction (STaSC) algo-
rithm, which incorporates multiple algorith-
mic design choices. Experimental results on
a question-answering task demonstrate that
STaSC effectively learns self-correction, lead-
ing to significant performance improvements.
Our analysis further provides insights into the
mechanisms of self-correction and the impact
of different design choices on learning dynam-
ics and overall performance. To support future
research, we release our user-friendly codebase
and lightweight models.
1 Introduction
Recent advanced LLM employ complex reason-
ing (Guo et al., 2025) and meta-reasoning (Xiang
et al., 2025), expanding their capabilities. However,
even the most advanced models are prone to errors,
including hallucinations (Huang et al., 2025) and
logical inconsistencies (Ghosh et al., 2024), requir-
ing external verification or human intervention. To
address those problems, self-correction â€” the abil-
ity to revise their own outputs â€” has been evolved
(Madaan et al., 2023). The existing approaches
mostly use zero-shot prompting (Madaan et al.,
2023; Shinn et al., 2024), external evaluators for
correction or feedback (Zhang et al., 2024) or ap-
ply large proprietary models and focus specifically
on mathematical tasks (Kumar et al., 2024).
In this study, we focus on self-correction with-
out external information or evaluators, ensuring
inference efficiency while relying solely on themodelâ€™s intrinsic knowledge. We investigate the
self-correction capabilities of small language mod-
els (SLMs) by applying iterative fine-tuning on
self-generated data, allowing models to improve
their outputs without external supervision.
We introduce the Self-Taught Self-Correction
(STaSC) algorithm, which trains models to self-
correct using self-generated trajectories, adapting
the core idea of STaR (Zelikman et al., 2022).
STaSC extends and unifies approaches that itera-
tively train with self-generated trajectories, includ-
ing the Self-Correction (SC) algorithm (Welleck
et al., 2022), by incorporating flexible design
choices. Unlike prior methods, STaSC provides
control over initial answer exploration, correction
filtering, and iterative fine-tuning, encompassing
SC as a special case and demonstrating how dif-
ferent algorithmic choices impact self-correction
performance.
Our results on the Natural Questions dataset
(Kwiatkowski et al., 2019) show that SLMs can
learn to self-correct using self-synthesized data,
while also improving their initial answer quality
despite being trained solely for corrections. We
release easy-to-use and adaptable code for self-
correction algorithms at https://github.com/
VityaVitalich/STASC .
The contributions of the paper are as follows:
â€¢We propose the Self-Taught Self-Correction
(STaSC) algorithm, unifying and extending
existing self-correction methods trained with
self-generated trajectories.
â€¢We conduct extensive experiments on a purely
Natural Language Processing (NLP) task â€”
Question Answering â€” using an open-source
SLMs, demonstrating their ability to learn self-
correction with self-synthesized data.
â€¢We release open-source, easily adaptable code
for self-correction, along with efficient SLMs
1arXiv:2503.08681v1  [cs.CL]  11 Mar 2025

OUTPUT OUTPUTGenerator Generator Generator               Corrector CorrectorINPUT
OUTPUTINPUT
OUTPUTOUTPUT
OUTPUTSTaR Self-Correction
ðŸ”¥STaSC
ðŸ”¥INPUT
ðŸ”¥  / â„ ðŸ”¥OUTPUT
â„ Figure 1: Illustration of the self-improvement method STaR (left) (Zelikman et al., 2022), self-correction method
SC(center) (Welleck et al., 2022), and our method, STaSC (right). STaCS offers flexible control over initial answer
exploration, correction filtering, and iterative fine-tuning. It is inspired by STaR and effectively encompasses SC as
a special case. SC and STaSC allow several initial answers and corrections. The dotted line in the STaSC denotes
two possible setups: fine-tuning the model and generating from it at the next iteration (Evolving Fine-Tuning) and
keeping the Generator frozen and fine-tuning the Corrector model only (Fixed Fine-Tuning).
with fewer than 4B parameters, making self-
correction practical and accessible.
2 Self-Taught Self-Correction
In this section, we introduce the Self-Taught
Self-Correction (STaSC) algorithm, an adapta-
tion of STaR (Zelikman et al., 2022) for self-
correction through iterative fine-tuning on self-
synthesized data. STaSC unifies and extends
various self-correction approaches using self-
synthesized data, including the Self-Correction
(SC) algorithm (Welleck et al., 2022).
2.1 Foundations and Enhancements
Figure 1 presents three algorithms highlighting
their similarities and dissimilarities. The left part
describes the original STaR algorithm which fo-
cuses on generating reasoning paths, filtering cor-
rect ones, and fine-tuning them accordingly.
The central part of the Figure 1 demonstrates the
Self-Correction (SC) algorithm which fine-tunes
the corrector model on corrections leading to im-
provement, keeping the initial Generator model
fixed. The right part refers to STaSC , adapting the
idea of STaR of iteratively refining model outputs
by sampling an initial answer, generating correc-
tions, filtering correct trajectories, and fine-tuning
on the successful revisions. This process is re-
peated over multiple iterations, progressively en-
hancing the modelâ€™s accuracy.
The key extension of our algorithm over previ-ous baselines is the incorporation of flexible al-
gorithmic choices. Initial answers can be drawn
from either a frozen model or the previous iteration,
corrections can be filtered based on strict improve-
ments or by allowing non-worsening revisions, and
fine-tuning can be performed from either the initial
model or the latest iteration.
2.2 Formal Definition and Algorithmic
Choices
Below, we outline the Algorithm 1 steps and key
design choices, each of which influences the self-
correction process. For each choice, we define an
abbreviation that will be used to denote specific
algorithm configurations, such as STaSC FIF, where
subscripts specify the selected options.
2.2.1 Requirements and Notation
The algorithm begins with an initial language
model state, M0, and an initial dataset, D0, con-
sisting of input-output pairs (x, y). Additionally,
we define a number of improvement iterations N,
the number of sampled initial generations Ninit, the
number of sampled corrections Ncorr, and a reward
function r, which evaluates the quality of model-
generated outputs.
Step 1: Sampling Initial Answers In the first
step, we sample Ninitinitial answers Ë†y1for each
input xin the dataset D0. The primary design
choice here is the selection of the model Mused
for sampling:
2

Algorithm 1 Self-Taught Self-Correction (STaSC)
Require: Initial model M0, dataset D0, number of iterations N, initial samples Ninit, correction samples
Ncorr, reward function r
1:forn= 1toNdo
2: Step 1: Sample Initial Answers
3: Ë†Y1
i={Ë†y1
ij}Ninit
j=1âˆ¼M(xi),âˆ€xiâˆˆD0
4: Option 1: M=Mnâˆ’1(Evolving Initialization)
5: Option 2: M=M0(Fixed Initialization)
6: Step 2: Sample Corrections
7: Ë†Y2
i={Ë†y2
ijk}Ncorr
k=1âˆ¼Mnâˆ’1(xi,Ë†y1
ij),âˆ€Ë†y1
ijâˆˆË†Y1
i
8: Step 3: Filter Corrections
9: D+
n={(xi,Ë†y1
ij,Ë†y2
ijk)|r(Ë†y2
ijk)> r(Ë†y1
ij)}
10: D=
n={(xi,Ë†y1
ij,Ë†y2
ijk)|r(Ë†y2
ijk) =r(Ë†y1
ij)â‰¥t}
11: Option 1: Dn=D+
n (Improving Filter)
12: Option 2: Dn=D+
nâˆªD=
n (Non-Decreasing Filter)
13: Step 4: Fine-Tuning
14: Mn=train(M,{Ë†y2
ijk|(xi,Ë†y1
ij,Ë†y2
ijk)âˆˆDn})
15: Option 1: M=M0(Fixed Fine-Tuning)
16: Option 2: M=Mnâˆ’1(Evolvong Fine-Tuning)
17:end for
â€¢Fixed Initialization (STaSC F**)â€“ Initial an-
swers are sampled using the frozen model M0,
as in the SC algorithm (Welleck et al., 2022).
â€¢Evolving Initialization (STaSC E**)â€“ Initial
answers are sampled using the model from
the previous iteration Mnâˆ’1, as in the original
STaR (Zelikman et al., 2022).
Fixed Initialization ensures robustness to varia-
tions in self-improvement and dataset shifts, main-
taining consistency across iterations. In contrast,
Evolving Initialization allows for greater explo-
ration, potentially leading to more diverse refine-
ments and improved performance.
Step 2: Sample Corrections At the second step,
we sample Ncorrcorrections Ë†y2for each output Ë†y1
in dataset D0using the model from the last iteration
Mnâˆ’1.
Step 3: Filtering Corrections In this step, we fil-
ter the corrections using the reward function r(Ë†y2)
to construct the fine-tuning dataset Dn. The key
design choice here is the filtering criterion for se-
lecting corrections.
â€¢Improving Filter (STaSC *I*)â€“ Selects only
corrections that strictly improve the reward,
ensuring r(Ë†y2)> r(Ë†y1), as used in STaSC
and SC.â€¢Non-Decreasing Filter (STaSC *N*)â€“ Selects
corrections that either strictly improve the re-
ward or retain the initial answer if it was al-
ready correct, ensuring (r(Ë†y2) =r(Ë†y1))âˆ©
(r(Ë†y1)â‰¥t), as proposed in SCoRE (Kumar
et al., 2024). This allows the model to pre-
serve already correct answers while still refin-
ing incorrect ones.
The Improving Filter enforces strict improve-
ment for every input, ensuring that only progres-
sively better outputs are used for fine-tuning. In
contrast, the Non-Decreasing Filter permits stable
answers to be retained when already correct, allow-
ing for a more conservative refinement process.
Step 4: Fine-Tuning In this step, we fine-tune
the model on the dataset Dnobtained from Step
3 to produce the improved model Mn. The key
design choice here is the selection of the model
used for fine-tuning:
â€¢Fixed Fine-Tuning (STaSC **F) â€“ Fine-tunes
the initial model M0, as done in the original
STaR recipe, ensuring stability across itera-
tions and reducing the risk of accumulating
errors.
â€¢Evolving Fine-Tuning (STaSC **E) â€“ Fine-
tunes the model from the previous iteration
Mnâˆ’1, as in SC, allowing the model to pro-
gressively improve and adapt across iterations.
3

NinitNcorrQwen-2.5-1.5B Phi3-mini
max{r(Ë†Y1)}max{r(Ë†Y2)}max{r(Ë†Y1)}max{r(Ë†Y2)}
11 - - 0.320Â±0.005 0 .372Â±0.010
3 0.248Â±0.011 0 .208Â±0.011 0.326Â±0.009 0 .376Â±0.010
5 0.230Â±0.011 0 .228Â±0.021 0.352Â±0.016 0 .384Â±0.012
31 0.236Â±0.007 0 .232Â±0.018 0.334Â±0.011 0 .362Â±0.024
3 0.264Â±0.015 0 .238Â±0.018 0.342Â±0.010 0 .372Â±0.012
5 0.273Â±0.017 0 .236Â±0.019 0.332Â±0.007 0 .384Â±0.013
51 0.273Â±0.012 0 .250Â±0.024 0.334Â±0.008 0 .378Â±0.016
3 0.295Â±0.019 0 .244Â±0.023 0.336Â±0.021 0 .354Â±0.026
5 0.300Â±0.020 0 .248Â±0.029 0.350Â±0.012 0 .376Â±0.011
Table 1: Maximum reward rover iterations for initial answer r(Ë†Y1)and for correction r(Ë†Y2)for different number
of samples and initial generations. Bold corresponds to the best performance.
Fixed Fine-Tuning maintains a stable learning
process by always training from the original model,
preventing drift. In contrast, Evolving Fine-Tuning
enables iterative adaptation, potentially leading to
greater long-term improvements but also introduc-
ing the risk of compounding errors.
3 Experimental Setup
In this section, we describe the main experimental
procedure, including dataset selection, evaluation
metrics, and implementation details.
Dataset We evaluate our algorithm on
the QA task using the Natural Questions
dataset (Kwiatkowski et al., 2019), which consists
of factual simple questions. We use a subset of
500 questions per train and test split, following
previous studies (Trivedi et al., 2022; Jeong et al.,
2024; Moskvoretskii et al., 2025), to ensure
consistency and computational efficiency.
Evaluation We use In-accuracy as the primary
evaluation metric and reward function, which is
standard for this task (Trivedi et al., 2022; Jeong
et al., 2024). It assesses whether the generated
answer contains the reference answer, assigning
r(Ë†y) = 1 for correct responses and r(Ë†y) = 0 for
incorrect ones.
All metrics are reported on the test set, which re-
mains unseen during training. Additionally, we re-
portmax{r(Ë†Y1)}andmax{r(Ë†Y2)}, representing
the highest reward obtained for initial answers and
corrections across STaSC iterations, respectively.
To ensure a fair real-world setup, the reward is not
available to the model during inference. Unlikesome prior studies (Shinn et al., 2024), where in-
ference benefits from reward signals derived from
ground-truth labels, our approach aligns with a
fully unsupervised inference setting, ensuring a
more realistic evaluation.
Implementation Details We conduct experi-
ments using Qwen-2.5-1.5B (Qwen et al., 2025)
and Phi3-Mini (Abdin et al., 2024), employing de-
fault generation parameters. The default setup for
self-correction is 2-shot. STaSC fine-tuning is per-
formed for 1 epoch with a batch size of 8 and a
learning rate of 7Ã—10âˆ’6. Additional implementa-
tion details are provided in Appendix A.
4 Results & Discussion
In this section, we provide the results and discuss
them, inspecting the STaSC algorithm design.
4.1 Impact of NinitandNcorr
Firstly, we examine how the selection of parame-
tersNinitandNcorraffects algorithm performance,
exploring values of 1, 3, and 5 for both models. To
encourage exploration, we use STaSC EIF, where
initial answers are sampled from the previous itera-
tion, only improving corrections are retained, and
fine-tuning is performed from the base model to
ensure stability.
As shown in Table 1, a greedy approach for
Qwen-2.5-1.5B fails to ensure convergence, as no
improving corrections emerge in the first iteration.
In contrast, increased exploration significantly en-
hances performance, likely due to the weaker align-
ment of the initial model. Additionally, we observe
4

0 1 2 3 4 5 6 7 8 9 10
Iteration0.100.150.200.250.300.350.40Correction In-Accuracy
Phi3-mini
0 1 2 3 4 5 6 7 8 9 10
Iteration0.140.160.180.200.22
Qwen-2.5-1.5B
STaSCEIFSTaSCEIESTaSCENFSTaSCENEFigure 2: Correction In-accuracy for STaSC versions with Evolving Initialization for Phi3-mini and Qwen-2.5-1.5B.
0 1 2 3 4 5 6 7 8 9 10
Iteration0.300.310.320.330.340.350.360.37Correction In-Accuracy
Phi3-mini
0 1 2 3 4 5 6 7 8 9 10
Iteration0.160.170.180.190.200.21
Qwen-2.5-1.5B
STaSCFIESTaSCFIFSTaSCFNESTaSCFNF
Figure 3: Correction In-accuracy for STaSC versions with Fixed Initialization for Phi3-mini and Qwen-2.5-1.5B.
that exploring initial answers has a greater impact
than exploring corrections.
However, this trend does not hold for Phi3-Mini ,
where increasing the number of initial answers does
not improve results, but greater correction explo-
ration does. This discrepancy likely stems from
differences in model capabilities. Phi3-Mini, be-
ing inherently stronger, benefits more from refining
corrections, whereas Qwen-2.5-1.5B , with lower
initial competence, requires a broader search for
initial answers to gain useful knowledge early on.
For subsequent experiments, we adopt the most
stable and well-performing configurations:
â€¢Qwen-2.5-1.5B :(Ninit, Ncorr) = (5 ,5).
â€¢Phi3-Mini :(Ninit, Ncorr) = (1 ,5).
4.2 STaSC With Evolving Initialization
Figure 2 illustrates the dynamics of correction per-
formance for STaSC variants with Evolving Ini-
tialization. Below, we discuss the observed effects
on performance, highlighting key trends and their
implications.Effect of the Non-Decreasing Filter A key ob-
servation is that STaSC with the Non-Decreasing
Filter consistently degrades performance for Phi3-
Mini. This is likely due to the difficulty in stabiliz-
ing training when fewer corrections are retained, in-
creasing the risk of overfitting. Interestingly, Qwen
does not exhibit the same decline, possibly due
to a higher number of retained corrections, which
augments the data and stabilizes training.
Impact of Filtering Selectivity We further high-
light the need to properly filter corrections when us-
ing Evolving Fine-Tuning, as all Phi3 settings with
STaSC **Eexhibit a negative correlation between
the leniency of filtered trajectories and the gain in
correction performance ( r=âˆ’0.51, p < . 001).
The more corrections were provided to the Cor-
rector model, the worse it performed. This sug-
gests that insufficiently selective filtering intro-
duces noise, leading to overfitting and a decline
in performance improvement. Notably, no other
setting shows a significant correlation between the
number of filtered corrections and performance.
Evolving Fine-Tuning Trends Evolving Fine-
Tuning slightly improves correction performance
5

0 1 2 3 4 5 6 7 8 9 10
Iteration0.300.320.340.360.38Correction In-Accuracy
Phi3-mini
0 1 2 3 4 5 6 7 8 9 10
Iteration0.140.160.180.200.220.24
Qwen-2.5-1.5B
Algorithm Type
STaSCEIFSTaSCEIEStage Type
Correction InitialAlgorithm Type
STaSCEIFSTaSCEIEStage Type
Correction InitialFigure 4: Correction and Initial Answer In-accuracy for best STaSC versions for Phi3-mini and Qwen-2.5-1.5B.
Step 1 Step 3 Step 4 Qwen-2.5-1.5B Phi3-mini
Model Filter Model max{r(Ë†Y1)} max{r(Ë†Y2)} max{r(Ë†Y1)} max{r(Ë†Y2)}
M0ImprovingM0 0.212 0 .208Â±0.014 0.294 0 .354Â±0.010
Mnâˆ’1 0.212 0 .212Â±0.016 0.294 0 .374Â±0.009
Non-DecreasingM0 0.212 0 .198Â±0.012 0.294 0 .348Â±0.013
Mnâˆ’1 0.212 0 .206Â±0.014 0.294 0 .374Â±0.010
Mnâˆ’1ImprovingM00.244Â±0.011 0 .232Â±0.023 0.352Â±0.016 0.384Â±0.12
Mnâˆ’10.236Â±0.009 0 .230Â±0.024 0.392Â±0.024 0 .394Â±0.012
Non-DecreasingM0 0.240Â±0.009 0.222Â±0.023 0.316Â±0.056 0 .356Â±0.066
Mnâˆ’10.234Â±0.013 0 .228Â±0.022 0.294Â±0.062 0 .356Â±0.074
Table 2: Maximum reward rover iterations for initial answer r(Ë†Y1)and for correction r(Ë†Y2)for different settings
of STaSC Algorithm. Bold values correspond to the best performance, underlined represent second best.
for Phi3-Mini. For Qwen, performance increases
rapidly at first, then declines slightly before rising
again in later iterations. This suggests that accumu-
lated knowledge gains take effect only in the later
stages, once a sufficient number of corrections and
initial answers have been processed.
Key Takeaways Evolving Initialization is most
effective when combined with an Improving Fil-
ter and Evolving Fine-Tuning, as these either en-
hance correction performance or at least prevent
degradation. Additionally, filtering selectivity is
crucialâ€”overly lenient filters introduce noise and
cause overfitting.
4.3 STaSC With Fixed Initialization
Next, we analyze the trends and implications of
Fixed Initialization, which aligns with the algo-
rithmic choice used in SC. Figure 3 presents the
correction performance for STaSC variants under
this setting.
Effect of the Non-Decreasing Filter Unlike
Evolving Initialization, the Non-Decreasing Fil-ter does not lead to a general decline in correc-
tion performance for either Phi3 or Qwen, with
an exception of STaSC FNF,where performance de-
grades. For Phi3, correction performance remains
largely unchanged throughout most iterations, with
noticeable improvements emerging only in the final
stages. This delayed progress suggests that Evolv-
ing Fine-Tuning gradually accumulates knowledge,
but its effects become apparent only after suffi-
cient corrections and initial answers have been pro-
cessed. A similar trend was previously observed in
STaSC EIE, showing that iterative fine-tuning plays
a key role in long-term performance gains.
Importance of Evolving Fine-Tuning We find
that Evolving Fine-Tuning is crucial when using
Fixed Initialization, particularly for Phi3 and, to
a lesser extent, for Qwen. This is expected, as
Evolving Fine-Tuning serves as the only source of
exploration in this setting, driving the algorithm
forward. In contrast, with Fixed Fine-Tuning, we
observe a general decline in Phi3â€™s performance
and stagnation after the first iteration for Qwen.
This suggests that without sufficient exploration,
6

relying solely on corrections from previous steps is
insufficient for SLMs.
We also observe that when using both Fixed
Initialization and Fixed Fine-Tuning applying the
Non-Decreasing Filter further worsens the perfor-
mance, underscoring the importance of filtering se-
lectivity. In this setting, exploration is driven solely
by corrections, making the filtering process critical.
When corrections include too many noisy or unin-
formative trajectories due to improper filtering, the
model struggles to improve, leading to significant
performance degradation. This reinforces the need
for a more controlled correction selection process
to ensure meaningful updates during training.
Key Takeaways Fixed Initialization reduces ex-
ploration, making the model more tolerant to the
Non-Decreasing Filter and benefiting more from
Evolving Fine-Tuning. In contrast, Fixed Fine-
Tuning restricts exploration solely to corrections,
increasing reliance on selective filtering to main-
tain performance.
4.4 STaSC Impact on Initial Answers
In this section, we identify the best-performing
STaSC configurations and analyze their behavior in
terms of initial answers and correction dynamics.
Selection of High-Performing Variants Based
on our previous analysis and Table 2 we determine
that STaSC EIEand STaSC EIFexhibit the strongest
performance. These results highlight the crucial
role of Evolving Initialization and the Improving
Filter, while leaving the impact of Fine-Tuning
strategies open for further investigation.
Performance Comparison Figure 4 illustrates
the performance trends of initial answers and cor-
rections for STaSC EIEand STaSC EIF.
The effectiveness of Fine-Tuning strategies
varies between Phi3 and Qwen. For Phi3, Evolving
Fine-Tuning leads to a substantial increase in ini-
tial answer quality, surpassing Fixed Fine-Tuning,
while yielding moderate improvements in correc-
tions. In contrast, for Qwen, Fixed Fine-Tuning
results in superior performance for both initial an-
swers and corrections.
Effect of Correction-Only Training Interest-
ingly, despite the model being trained exclusively
on corrections, initial answer quality also improves.
Since gradient updates are applied only to correc-
tion tokens, this suggests that learning corrections
either enhances the modelâ€™s factual knowledge orimproves its internal reasoning ability at the initial
generation stage.
Alignment Between Initial Answers and Cor-
rections Evolving Fine-Tuning progressively re-
duces the gap between initial answers and correc-
tions, eventually leading to their alignment. For
Phi3, initial answers gradually improve until they
reach the quality of corrections. For Qwen, the
opposite trend is observedâ€”corrections improve
until they match the initial answers.
This suggests that Evolving Fine-Tuning either
helps internalize the correction process, leading to
higher-quality initial answers as seen in Phi3, or
stabilizes responses by preventing degradation, as
observed in Qwen.
5 Related Works
Self-correction is a relatively new yet actively grow-
ing research domain. A systematic survey defines
self-correction as a framework in which LLMs re-
fine their responses using LLMs during inference,
potentially incorporating external tools or knowl-
edge (Kamoi et al., 2024). A significant body of
work in this direction focuses on leveraging exter-
nal feedback from external knowledge and verifica-
tion tools (Jiang et al., 2023; Gou et al., 2024; Pan
et al., 2023; Xu et al., 2023), as these approaches
provide high-quality solution evaluation. However,
in real-world applications, such external resources
are often unavailable or computationally expensive.
Moreover, relying on external verification does not
pose a fundamentally challenging task for LLMs,
limiting their ability to develop intrinsic reasoning
and self-improvement capabilities.
An alternative approach is intrinsic self-
correction, where the model refines its own outputs
without relying on external critics. This can be
implemented in a zero-shot setting, using the same
model iteratively (Madaan et al., 2023), or through
external models trained on synthetic errors(Paul
et al., 2024) or self-generated corrections(Welleck
et al., 2022). While these approaches have shown
promise, they still rely on external critic models,
making them closer to verification-based genera-
tion rather than true self-correction.
The only work exploring self-correction in its
natural form is the SCoRE framework (Kumar
et al., 2024), which was the first to experiment with
intrinsic self-correction within the same model and
introduced a multi-turn RL training approach for
this purpose. However, SCoRE lacks a formalized
7

STaSCYou are a helpful reasoning assistant in general
domain question answering. Please reason
through the question step by step very shortly
before giving a final answer . Generate a short
chain-of-thought rationale very shortly , and then
provide the final answer in the format: "Final
answer: your answer"Step-by-step reasoning: The question refers to two dif ferent movies, one about pregnancy and
another about Tracey . 
The actress who played in both needs to be identified. 
Recalling movies about pregnancy , "Juno" comes to mind, starring Ellen Page (now Elliot
Page). 
For a movie about Tracey , "Boys Don't Cry" features Hilary Swank playing the role of
Tracey . 
However , these are two dif ferent actresses, so we need to find a common actress. After fact-
checking, the actress who played in both movies is likely to be Reese Witherspoon, who starred in
"Legally Blonde" (where her character is named Elle, not Tracey) and "Sweet Home Alabama"
(which involves pregnancy). 
Final Answer:  Reese W itherspoon
inputoutputPlease review the given answer for accuracy . If it is incorrect, identify the incorrect information
and provide the correct details. Ask and answer subquestions, and split the question into parts.
Disregard the information you already have.
system
promptsystem
prompt
input
Step-by-step reasoning: The initial analysis contains several inaccuracies regarding the
identification of an actress who starred in both a movie about pregnancy and another featuring a
character named Tracey . Let's address these inaccuracies and provide the correct information.  
Inaccuracies in the Initial Analysis:  
Elliot Page in "Juno": It's correctly noted that Elliot Page (formerly Ellen Page) starred in
"Juno," a film centered around teenage pregnancy . However , the analysis doesn't
connect Elliot Page to a film involving a character named Tracey . 
Hilary Swank in "Boys Don't Cry": Hilary Swank portrayed Brandon Teena in "Boys
Don't Cry ," not a character named Tracey . This film doesn't align with the criteria of
featuring a character named Tracey . 
Reese Witherspoon in "Legally Blonde" and "Sweet Home Alabama": Reese
Witherspoon's characters in these films are named Elle W oods and Melanie Smooter ,
respectively . Neither film prominently features a character named Tracey , nor are they
primarily about pregnancy . 
Correct Information:  
"Juno" (2007): Elliot Page plays the titular character , Juno MacGuf f, a teenager dealing
with an unplanned pregnancy . 
"The Tracey Fragments" (2007): Elliot Page portrays Tracey Berkowitz, a 15-year-old
navigating personal challenges.  
Final answer:  Elliot PageAnswer: Ellen (Elliot) Pagecorrect
answerQuestion: Who played in the movies about
pregnancy and Tracey? Reason step by step
very shortly , then conclude with the answer .
Step-by-step reasoning:Figure 5: Example of the STaSC pipeline.
theoretical foundation and a deeper investigation
of baseline algorithm adaptations. Additionally, it
is limited by the use of large proprietary models,
without open-sourcing the code or model weights,
making it difficult to build upon for future research.
6 Conclusion
In this study, we introduced the Self-Taught Self-
Correction (STaSC) algorithm, which incorporates
multiple algorithmic choices to enable genuine
intrinsic self-correction without relying on exter-
nal tools or large proprietary models. Inspired by
STaR (Zelikman et al., 2022), our approach trains
exclusively on self-generated data. Experiments on
a QA task with two small language models demon-
strate that SLMs can learn to self-correct using
their own generations and even improve initial an-
swers, despite being trained solely for corrections.
Furthermore, our analysis highlights key algorith-mic insights, emphasizing the importance of fil-
tering selectivity, initial answer exploration, and
the potential of iterative fine-tuning. To support
future research, we have open-sourced our code
and lightweight models.
7 Limitations
â€¢The selected SLMs, while effective, may have
certain capacity limitations that could influ-
ence the extent of the self-correction process.
â€¢Experiments were conducted with a single
run, which, while sufficient for initial insights,
may introduce some variability.
â€¢The evaluation focuses on a Question Answer-
ing (QA) task, leaving open the opportunity
to explore performance across other tasks and
domains.
8

â€¢The chosen hyperparameters, though reason-
able, may not fully optimize the modelâ€™s learn-
ing efficiency or overall performance.
â€¢A more detailed analysis of the types and pat-
terns of corrections could further enrich our
understanding of the self-correction mecha-
nism.
â€¢The reward function, while practical, may not
perfectly capture all nuances of desired behav-
ior, presenting room for refinement in future
work.
8 Ethical Considerations
Our work enables small language models to
self-correct using self-generated data. We em-
ploy advanced models like Qwen-2.5 and Phi-3,
pre-trained on diverse datasets, including user-
generated content. While efforts have been made
to remove harmful or biased data, some biases may
persist in outputs. This does not undermine our
methods, which are designed to self-correct factual
inaccuracies and are adaptable to other rigorously
debiased models. Beyond inherent bias challenges,
our work raises no additional ethical concerns.
References
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed
Awadallah, Ammar Ahmad Awan, Nguyen Bach,
Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat
Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck,
SÃ©bastien Bubeck, Martin Cai, Qin Cai, Vishrav
Chaudhary, Dong Chen, Dongdong Chen, Weizhu
Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng,
Parul Chopra, Xiyang Dai, Matthew Dixon, Ro-
nen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao,
Min Gao, Amit Garg, Allie Del Giorno, Abhishek
Goswami, Suriya Gunasekar, Emman Haider, Jun-
heng Hao, Russell J. Hewett, Wenxiang Hu, Jamie
Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi,
Xin Jin, Nikos Karampatziakis, Piero Kauffmann,
Mahoud Khademi, Dongwoo Kim, Young Jin Kim,
Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi
Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui
Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu,
Weishung Liu, Xiaodong Liu, Chong Luo, Piyush
Madan, Ali Mahmoudzadeh, David Majercak, Matt
Mazzola, Caio CÃ©sar Teodoro Mendes, Arindam Mi-
tra, Hardik Modi, Anh Nguyen, Brandon Norick,
Barun Patra, Daniel Perez-Becker, Thomas Portet,
Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang
Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy,
Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil
Salim, Michael Santacroce, Shital Shah, Ning Shang,
Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia
Song, Masahiro Tanaka, Andrea Tupini, PraneethaVaddamanu, Chunyu Wang, Guanhua Wang, Lijuan
Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel
Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia
Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu,
Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang,
Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu,
Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen
Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan
Zhang, and Xiren Zhou. 2024. Phi-3 technical report:
A highly capable language model locally on your
phone. Preprint , arXiv:2404.14219.
Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum
Arafat, and Arijit Khan. 2024. Logical consistency
of large language models in fact-checking. Preprint ,
arXiv:2412.16100.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
2024. Critic: Large language models can self-
correct with tool-interactive critiquing. Preprint ,
arXiv:2305.11738.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,
Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-
centivizing reasoning capability in llms via reinforce-
ment learning. arXiv preprint arXiv:2501.12948 .
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2025. A survey on hallucination in large lan-
guage models: Principles, taxonomy, challenges, and
open questions. ACM Transactions on Information
Systems , 43(2):1â€“55.
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju
Hwang, and Jong C Park. 2024. Adaptive-rag: Learn-
ing to adapt retrieval-augmented large language mod-
els through question complexity. arXiv preprint
arXiv:2403.14403 .
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active retrieval
augmented generation. Preprint , arXiv:2305.06983.
Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han,
and Rui Zhang. 2024. When can llms actually cor-
rect their own mistakes? a critical survey of self-
correction of llms. Transactions of the Association
for Computational Linguistics , 12:1417â€“1440.
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal,
Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,
Shariq Iqbal, Colton Bishop, Rebecca Roelofs,
et al. 2024. Training language models to self-
correct via reinforcement learning. arXiv preprint
arXiv:2409.12917 .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
9

Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics , 7:452â€“466.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: It-
erative refinement with self-feedback. Preprint ,
arXiv:2303.17651.
Viktor Moskvoretskii, Maria Lysyuk, Mikhail Sal-
nikov, Nikolay Ivanov, Sergey Pletenev, Daria Gal-
imzianova, Nikita Krayko, Vasily Konovalov, Irina
Nikishina, and Alexander Panchenko. 2025. Adap-
tive retrieval without self-knowledge? bringing uncer-
tainty back home. arXiv preprint arXiv:2501.12835 .
Liangming Pan, Alon Albalak, Xinyi Wang, and
William Yang Wang. 2023. Logic-lm: Empower-
ing large language models with symbolic solvers
for faithful logical reasoning. arXiv preprint
arXiv:2305.12295 .
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2024. REFINER: Reasoning feedback on
intermediate representations. In Proceedings of the
18th Conference of the European Chapter of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1100â€“1126, St. Julianâ€™s, Malta.
Association for Computational Linguistics.
Qwen, :, An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,
Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin,
Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,
Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,
Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,
Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji
Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang
Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang
Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru
Zhang, and Zihan Qiu. 2025. Qwen2.5 technical
report. Preprint , arXiv:2412.15115.
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2024. Re-
flexion: Language agents with verbal reinforcement
learning. Advances in Neural Information Process-
ing Systems , 36.
Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2022. Interleav-
ing retrieval with chain-of-thought reasoning for
knowledge-intensive multi-step questions. arXiv
preprint arXiv:2212.10509 .
Sean Welleck, Ximing Lu, Peter West, Faeze Brah-
man, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. 2022. Generating sequences by learning to
self-correct. arXiv preprint arXiv:2211.00053 .Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Al-
balak, Anikait Singh, Chase Blagden, Duy Phung,
Rafael Rafailov, Nathan Lile, Dakota Mahan, et al.
2025. Towards system 2 reasoning in llms: Learn-
ing how to think with meta chain-of-though. arXiv
preprint arXiv:2501.04682 .
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao
Song, Markus Freitag, William Yang Wang, and Lei
Li. 2023. Instructscore: Explainable text generation
evaluation with finegrained feedback. arXiv preprint
arXiv:2305.14282 .
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.
Goodman. 2022. Star: Bootstrapping reasoning with
reasoning. Preprint , arXiv:2203.14465.
Yunxiang Zhang, Muhammad Khalifa, Lajanugen Lo-
geswaran, Jaekyeom Kim, Moontae Lee, Honglak
Lee, and Lu Wang. 2024. Small language models
need strong verifiers to self-correct reasoning. arXiv
preprint arXiv:2404.17140 .
A Technical Details
We apply a weight decay of 0.1 and use the Adam
optimizer with default betas, along with a cosine
annealing scheduler. Training is performed using
Fully Sharded Data Parallel on two A100 GPUs,
with an estimated total compute of 80 GPU hours.
The dataset and Qwen come under Apache License,
Phi3-mini under MIT. We are open-sourcing all our
code under the Apache License.
Initial answer sampling prompt
You are a helpful reasoning assistant
in general domain question answering.
Please reason through the question step
by step very shortly before giving a final
answer.
Generate a short chain-of-thought ratio-
nale very shortly, and then provide the
final answer.
Step-by-step reasoning:
Final Answer:
Question: {question text}
Reason step by step very shortly, then
conclude with the answer.
Correction sampling prompt
You are a helpful reasoning assistant in
general domain question answering.Your
task is to correct the initial response if it
is incorrect.
10

Below is the question and the initial an-
swer. Generate a correction to the initial
answer if it is incorrect. Disregard the
information you already have, look for
other options. Do not use the information
that does not match your criteria.
Step-by-step reasoning:
Final Answer:
Question: Which Portuguese soccer
player has the most goals?
Step-by-step reasoning: Christiano
Ronaldo has scored 900 goals in a ca-
reer spanning 22 years. I cannot think of
anyone else scoring more than him.
Final Answer: Christiano Ronaldo
Step-by-step reasoning: Yes, Christiano
Ronaldo is the correct answer. Other de-
tails are unimportant.
Final Answer: Christiano Ronaldo
Question: Which actress played in both
movies: one about preg-cy and the other
about Tracey?
Step-by-step reasoning: 1. Identify
movies about preg-cy and about Tracey.
2. Movies about preg-cy include â€™Junoâ€™
and â€™Knocked Upâ€™. 3. Movie about
Tracey is likely â€™Tracy Chapmanâ€™ or a
documentary, but â€™Tracyâ€™ could be a ref-
erence to â€™Tracy Flickâ€™ in the movie
â€™Electionâ€™. 4. Considering well-known
movies with these themes, actress Reese
Witherspoon comes to mind as a possible
match.
Final Answer: Reese Witherspoon
Step-by-step reasoning: Letâ€™s break
down the answer. â€™Junoâ€™ is indeed a
movie about preg-cy, but it starred Ellen
Page (now Elliot Page), not Reese With-
erspoon. Reese Witherspoon did not star
in â€™Juno.â€™ Therefore, the association be-
tween Reese Witherspoon and â€™Junoâ€™ is
incorrect. Movies About Tracey is likely
â€™Tracey Fragments (2007)â€™, a movie in
which Ellen Page also starred as the lead
character, Tracey Berkowitz. Ellen Page
(Elliot Page) connects both movies.
Final answer: Ellen Page (now Elliot
Page)Question: {question text}
Initial Answer: {initial answer}
Write a correction if the initial answer is
incorrect.
11

